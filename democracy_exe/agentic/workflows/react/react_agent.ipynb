{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Load environment variables from .env file for API access\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-19 18:09:14.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpre_update\u001b[0m:\u001b[36m827\u001b[0m - \u001b[1mllm_model_name: None\u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpre_update\u001b[0m:\u001b[36m828\u001b[0m - \u001b[1mllm_embedding_model_name: None\u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpre_update\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1msetting default llm_model_name: gpt-4o-mini\u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpre_update\u001b[0m:\u001b[36m841\u001b[0m - \u001b[1msetting default llm_embedding_model_name: text-embedding-3-large\u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpost_root\u001b[0m:\u001b[36m856\u001b[0m - \u001b[1mbefore redis_path: \u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpost_root\u001b[0m:\u001b[36m857\u001b[0m - \u001b[1mbefore redis_pass: None\u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpost_root\u001b[0m:\u001b[36m858\u001b[0m - \u001b[1mbefore redis_user: None\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               +-----------+                                           \n",
      "                                               | __start__ |                                           \n",
      "                                               +-----------+                                           \n",
      "                                                      *                                                \n",
      "                                                      *                                                \n",
      "                                                      *                                                \n",
      "                                          +--------------------+                                       \n",
      "                                          | tasks_democracy_ai |                                       \n",
      "                                    ******+--------------------+........                               \n",
      "                             *******            **           ...        .......                        \n",
      "                      *******                 **                ...            ........                \n",
      "                  ****                       *                     ..                  ....            \n",
      "+---------------------+           +----------------+           +--------------+           +---------+  \n",
      "| update_instructions |           | update_profile |           | update_todos |           | __end__ |  \n",
      "+---------------------+           +----------------+           +--------------+           +---------+  \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"React Agent with Long-Term Memory.\n",
    "\n",
    "This module implements a React agent with long-term memory capabilities using LangChain and LangGraph.\n",
    "It manages user profiles, todo lists, and custom instructions through a state graph architecture.\n",
    "\"\"\"\n",
    "# pyright: reportUninitializedInstanceVariable=false\n",
    "# pyright: reportUndefinedVariable=false\n",
    "# pyright: reportAttributeAccessIssue=false\n",
    "# pyright: reportInvalidTypeForm=false\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "from datetime import UTC, datetime, timezone\n",
    "from typing import Any, Dict, List, Literal, Optional, Tuple, TypedDict, Union\n",
    "\n",
    "import configuration\n",
    "import langsmith\n",
    "import rich\n",
    "import tiktoken\n",
    "\n",
    "from _utils import get_message_text, load_chat_model\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, merge_message_runs\n",
    "from langchain_core.messages.utils import get_buffer_string\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableLambda\n",
    "from langchain_core.runnables.config import RunnableConfig, ensure_config, get_executor_for_config\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.tracers.schemas import Run\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from loguru import logger\n",
    "from pydantic import BaseModel, Field\n",
    "from settings import aiosettings\n",
    "from trustcall import create_extractor\n",
    "from trustcall._base import ExtractionOutputs, InputsLike\n",
    "\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    \"\"\"A single memory entry containing user-related information.\n",
    "\n",
    "    Attributes:\n",
    "        content (str): The main content of the memory (e.g., \"User expressed interest in learning about French\")\n",
    "    \"\"\"\n",
    "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    \"\"\"A collection of memories about the user.\n",
    "\n",
    "    Attributes:\n",
    "        memories (list[Memory]): A list of Memory objects containing user-related information\n",
    "    \"\"\"\n",
    "    memories: list[Memory] = Field(description=\"A list of memories about the user.\")\n",
    "\n",
    "\n",
    "## Utilities\n",
    "\n",
    "# SOURCE: https://github.com/langchain-ai/langchain-academy/blob/main/module-5/memory_agent.ipynb\n",
    "# Visibility into Trustcall updates\n",
    "# Trustcall creates and updates JSON schemas.\n",
    "# What if we want visibility into the specific changes made by Trustcall?\n",
    "# For example, we saw before that Trustcall has some of its own tools to:\n",
    "# Self-correct from validation failures -- see trace example here\n",
    "# Update existing documents -- see trace example here\n",
    "# Visibility into these tools can be useful for the agent we're going to build.\n",
    "# Below, we'll show how to do this!\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# We can add a listener to the Trustcall extractor.\n",
    "# This will pass runs from the extractor's execution to a class, Spy, that we will define.\n",
    "# Our Spy class will extract information about what tool calls were made by Trustcall.\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# Inspect the tool calls for Trustcall\n",
    "class Spy:\n",
    "    \"\"\"A class to monitor and collect tool calls made by the Trustcall extractor.\n",
    "\n",
    "    This class acts as a listener for the Trustcall extractor's execution runs,\n",
    "    collecting information about what tool calls were made during execution.\n",
    "\n",
    "    Attributes:\n",
    "        called_tools (list): A list to store tool calls made during execution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the Spy with an empty list of called tools.\"\"\"\n",
    "        self.called_tools: list = []\n",
    "\n",
    "    def __call__(self, run: Any) -> None:\n",
    "        \"\"\"Process a run and extract tool calls from chat model outputs.\n",
    "\n",
    "        Traverses the run tree and collects tool calls from chat model outputs.\n",
    "\n",
    "        Args:\n",
    "            run: The run object containing execution information.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Spy: {run}\")\n",
    "        logger.info(f\"Spy type: {type(run)}\")\n",
    "        q: list = [run]\n",
    "        while q:\n",
    "            r = q.pop()\n",
    "            if r.child_runs:\n",
    "                q.extend(r.child_runs)\n",
    "            if r.run_type == \"chat_model\":\n",
    "                self.called_tools.append(\n",
    "                    r.outputs[\"generation\"][0][0][\"message\"][\"kwargs\"][\"tool_calls\"]\n",
    "                )\n",
    "\n",
    "# Extract information from tool calls for both patches and new memories in Trustcall\n",
    "def extract_tool_info(tool_calls: list[list[dict[str, Any]]], schema_name: str = \"Memory\") -> str:\n",
    "    \"\"\"Extract information from tool calls for both patches and new memories.\n",
    "\n",
    "    This function processes tool calls to extract information about document updates\n",
    "    and new memory creation. It formats the extracted information into a human-readable\n",
    "    string.\n",
    "\n",
    "    Args:\n",
    "        tool_calls: List of tool call groups, where each group contains tool call\n",
    "            dictionaries with information about patches or new memory creation\n",
    "        schema_name: Name of the schema tool (e.g., \"Memory\", \"ToDo\", \"Profile\")\n",
    "\n",
    "    Returns:\n",
    "        A formatted string containing information about all document updates and\n",
    "        new memory creations\n",
    "    \"\"\"\n",
    "    # Initialize list of changes\n",
    "    changes: list[dict[str, Any]] = []\n",
    "\n",
    "    for call_group in tool_calls:\n",
    "        for call in call_group:\n",
    "            if call['name'] == 'PatchDoc':\n",
    "                changes.append({\n",
    "                    'type': 'update',\n",
    "                    'doc_id': call['args']['json_doc_id'],\n",
    "                    'planned_edits': call['args']['planned_edits'],\n",
    "                    'value': call['args']['patches'][0]['value']\n",
    "                })\n",
    "            elif call['name'] == schema_name:\n",
    "                changes.append({\n",
    "                    'type': 'new',\n",
    "                    'value': call['args']\n",
    "                })\n",
    "\n",
    "    # Format results as a single string\n",
    "    result_parts: list[str] = []\n",
    "    for change in changes:\n",
    "        if change['type'] == 'update':\n",
    "            result_parts.append(\n",
    "                f\"Document {change['doc_id']} updated:\\n\"\n",
    "                f\"Plan: {change['planned_edits']}\\n\"\n",
    "                f\"Added content: {change['value']}\"\n",
    "            )\n",
    "        else:\n",
    "            result_parts.append(\n",
    "                f\"New {schema_name} created:\\n\"\n",
    "                f\"Content: {change['value']}\"\n",
    "            )\n",
    "\n",
    "    return \"\\n\\n\".join(result_parts)\n",
    "\n",
    "## Schema definitions\n",
    "\n",
    "# Creating an agent\n",
    "# There are many different agent architectures to choose from.\n",
    "\n",
    "# Here, we'll implement something simple, a ReAct agent.\n",
    "\n",
    "# This agent will be a helpful companion for creating and managing a ToDo list.\n",
    "\n",
    "# This agent can make a decision to update three types of long-term memory:\n",
    "\n",
    "# (a) Create or update a user profile with general user information\n",
    "\n",
    "# (b) Add or update items in a ToDo list collection\n",
    "\n",
    "# (c) Update its own instructions on how to update items to the ToDo list\n",
    "\n",
    "# User profile schema\n",
    "class Profile(BaseModel):\n",
    "    \"\"\"This is the profile of the user you are chatting with\"\"\"\n",
    "    name: str | None = Field(description=\"The user's name\", default=None)\n",
    "    location: str | None = Field(description=\"The user's location\", default=None)\n",
    "    job: str | None = Field(description=\"The user's job\", default=None)\n",
    "    connections: list[str] = Field(\n",
    "        description=\"Personal connection of the user, such as family members, friends, or coworkers\",\n",
    "        default_factory=list\n",
    "    )\n",
    "    interests: list[str] = Field(\n",
    "        description=\"Interests that the user has\",\n",
    "        default_factory=list\n",
    "    )\n",
    "\n",
    "# ToDo schema\n",
    "class ToDo(BaseModel):\n",
    "    task: str = Field(description=\"The task to be completed.\")\n",
    "    time_to_complete: int | None = Field(description=\"Estimated time to complete the task (minutes).\")\n",
    "    deadline: datetime | None = Field(\n",
    "        description=\"When the task needs to be completed by (if applicable)\",\n",
    "        default=None\n",
    "    )\n",
    "    solutions: list[str] = Field(\n",
    "        description=\"List of specific, actionable solutions (e.g., specific ideas, service providers, or concrete options relevant to completing the task)\",\n",
    "        min_items=1,\n",
    "        default_factory=list\n",
    "    )\n",
    "    status: Literal[\"not started\", \"in progress\", \"done\", \"archived\"] = Field(\n",
    "        description=\"Current status of the task\",\n",
    "        default=\"not started\"\n",
    "    )\n",
    "\n",
    "## Initialize the model and tools\n",
    "\n",
    "# Update memory tool\n",
    "class UpdateMemory(TypedDict):\n",
    "    \"\"\"Decision on what memory type to update.\n",
    "\n",
    "    Attributes:\n",
    "        update_type (Literal['user', 'todo', 'instructions']): The type of memory to update\n",
    "    \"\"\"\n",
    "    update_type: Literal['user', 'todo', 'instructions']\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "model: BaseChatModel = init_chat_model(\"gpt-4o\", model_provider=aiosettings.llm_provider, temperature=0.0) # pyright: ignore[reportUndefinedVariable]\n",
    "# TODO: Use this to get embeddings\n",
    "# tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "## Create the Trustcall extractors for updating the user profile and ToDo list\n",
    "profile_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Profile],\n",
    "    tool_choice=\"Profile\",\n",
    ")\n",
    "\n",
    "## Prompts\n",
    "\n",
    "# Chatbot instruction for choosing what to update and what tools to call\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful chatbot.\n",
    "\n",
    "You are designed to be a companion to a user, helping them keep track of their ToDo list.\n",
    "\n",
    "You have a long term memory which keeps track of three things:\n",
    "1. The user's profile (general information about them)\n",
    "2. The user's ToDo list\n",
    "3. General instructions for updating the ToDo list\n",
    "\n",
    "Here is the current User Profile (may be empty if no information has been collected yet):\n",
    "<user_profile>\n",
    "{user_profile}\n",
    "</user_profile>\n",
    "\n",
    "Here is the current ToDo List (may be empty if no tasks have been added yet):\n",
    "<todo>\n",
    "{todo}\n",
    "</todo>\n",
    "\n",
    "Here are the current user-specified preferences for updating the ToDo list (may be empty if no preferences have been specified yet):\n",
    "<instructions>\n",
    "{instructions}\n",
    "</instructions>\n",
    "\n",
    "Here are your instructions for reasoning about the user's messages:\n",
    "\n",
    "1. Reason carefully about the user's messages as presented below.\n",
    "\n",
    "2. Decide whether any of the your long-term memory should be updated:\n",
    "- If personal information was provided about the user, update the user's profile by calling UpdateMemory tool with type `user`\n",
    "- If tasks are mentioned, update the ToDo list by calling UpdateMemory tool with type `todo`\n",
    "- If the user has specified preferences for how to update the ToDo list, update the instructions by calling UpdateMemory tool with type `instructions`\n",
    "\n",
    "3. Tell the user that you have updated your memory, if appropriate:\n",
    "- Do not tell the user you have updated the user's profile\n",
    "- Tell the user them when you update the todo list\n",
    "- Do not tell the user that you have updated instructions\n",
    "\n",
    "4. Err on the side of updating the todo list. No need to ask for explicit permission.\n",
    "\n",
    "5. Respond naturally to user user after a tool call was made to save memories, or if no tool call was made.\"\"\"\n",
    "\n",
    "# Trustcall instruction\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"Reflect on following interaction.\n",
    "\n",
    "Use the provided tools to retain any necessary memories about the user.\n",
    "\n",
    "Use parallel tool calling to handle updates and insertions simultaneously.\n",
    "\n",
    "System Time: {time}\"\"\"\n",
    "\n",
    "# Instructions for updating the ToDo list\n",
    "CREATE_INSTRUCTIONS = \"\"\"Reflect on the following interaction.\n",
    "\n",
    "Based on this interaction, update your instructions for how to update ToDo list items. Use any feedback from the user to update how they like to have items added, etc.\n",
    "\n",
    "Your current instructions are:\n",
    "\n",
    "<current_instructions>\n",
    "{current_instructions}\n",
    "</current_instructions>\"\"\"\n",
    "\n",
    "## Node definitions\n",
    "\n",
    "\n",
    "async def call_model(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> dict[str, list[BaseMessage]]:\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\n",
    "\n",
    "    This function retrieves user profile, todo list, and custom instructions from the store\n",
    "    and uses them to generate a personalized chatbot response.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Dict containing the list of messages with the chatbot's response\n",
    "        Format: {\"messages\": [response]}\n",
    "    \"\"\"\n",
    "    # Get the user ID from the config\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    user_id = configurable.user_id\n",
    "\n",
    "    # Retrieve profile memory from the store\n",
    "    namespace = (\"profile\", user_id)\n",
    "    # DISABLED: # memories = store.search(namespace)\n",
    "\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        user_profile = memories[0].value\n",
    "    else:\n",
    "        user_profile = None\n",
    "\n",
    "    # Retrieve people memory from the store\n",
    "    namespace = (\"todo\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    todo = \"\\n\".join(f\"{mem.value}\" for mem in memories)\n",
    "\n",
    "    # Retrieve custom instructions\n",
    "    namespace = (\"instructions\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        instructions = memories[0].value\n",
    "    else:\n",
    "        instructions = \"\"\n",
    "\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(user_profile=user_profile, todo=todo, instructions=instructions)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = await model.bind_tools([UpdateMemory], parallel_tool_calls=False).ainvoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "async def tasks_democracy_ai(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> dict[str, list[BaseMessage]]:\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\n",
    "\n",
    "    This function retrieves user profile, todo list, and custom instructions from the store\n",
    "    and uses them to generate a personalized chatbot response.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Dict containing the list of messages with the chatbot's response\n",
    "        Format: {\"messages\": [response]}\n",
    "    \"\"\"\n",
    "    # Get the user ID from the config\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    user_id = configurable.user_id\n",
    "\n",
    "    # Retrieve profile memory from the store\n",
    "    namespace = (\"profile\", user_id)\n",
    "    # DISABLED: # memories = store.search(namespace)\n",
    "\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        user_profile = memories[0].value\n",
    "    else:\n",
    "        user_profile = None\n",
    "\n",
    "    # Retrieve people memory from the store\n",
    "    namespace = (\"todo\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    todo = \"\\n\".join(f\"{mem.value}\" for mem in memories)\n",
    "\n",
    "    # Retrieve custom instructions\n",
    "    namespace = (\"instructions\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        instructions = memories[0].value\n",
    "    else:\n",
    "        instructions = \"\"\n",
    "\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(user_profile=user_profile, todo=todo, instructions=instructions)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = await model.bind_tools([UpdateMemory], parallel_tool_calls=False).ainvoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def update_profile(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> dict[str, list[dict[str, str]]]:\n",
    "    \"\"\"Reflect on the chat history and update the user profile in memory.\n",
    "\n",
    "    This function processes the chat history to extract and update user profile information\n",
    "    in the store using the Trustcall extractor.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Dict containing a tool message confirming the profile update\n",
    "        Format: {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"updated profile\",\n",
    "                \"tool_call_id\": str\n",
    "            }]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Get the user ID from the config\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    user_id = configurable.user_id\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace: tuple[str, str] = (\"profile\", user_id)\n",
    "\n",
    "    # Retrieve the most recent memories for context\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # Format the existing memories for the Trustcall extractor\n",
    "    tool_name: str = \"Profile\"\n",
    "    existing_memories: list[tuple[str, str, Any]] | None = (\n",
    "        [(existing_item.key, tool_name, existing_item.value)\n",
    "         for existing_item in existing_items]\n",
    "        if existing_items\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Merge the chat history and the instruction\n",
    "    trustcall_instruction_formatted: str = TRUSTCALL_INSTRUCTION.format(\n",
    "        time=datetime.now().isoformat()\n",
    "    )\n",
    "    updated_messages: list[BaseMessage] = list(\n",
    "        merge_message_runs(\n",
    "            messages=[SystemMessage(content=trustcall_instruction_formatted)] + state[\"messages\"][:-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Invoke the extractor\n",
    "    result = profile_extractor.invoke({\n",
    "        \"messages\": updated_messages,\n",
    "        \"existing\": existing_memories\n",
    "    })\n",
    "\n",
    "    # Save the memories from Trustcall to the store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"], strict=False):\n",
    "        store.put(\n",
    "            namespace,\n",
    "            rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "            r.model_dump(mode=\"json\"),\n",
    "        )\n",
    "\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "    # Return tool message with update verification\n",
    "    return {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"updated profile\",\n",
    "            \"tool_call_id\": tool_calls[0]['id']\n",
    "        }]\n",
    "    }\n",
    "\n",
    "def update_todos(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> dict[str, list[dict[str, str]]]:\n",
    "    \"\"\"Reflect on the chat history and update the todo list in memory.\n",
    "\n",
    "    This function processes the chat history to extract and update todo items\n",
    "    in the store using the Trustcall extractor. It also tracks changes made\n",
    "    using a Spy instance.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Dict containing a tool message with update details\n",
    "        Format: {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": str,  # Contains details of updates made\n",
    "                \"tool_call_id\": str\n",
    "            }]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Get the user ID from the config\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    user_id = configurable.user_id\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace: tuple[str, str] = (\"todo\", user_id)\n",
    "\n",
    "    # Retrieve the most recent memories for context\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # Format the existing memories for the Trustcall extractor\n",
    "    tool_name: str = \"ToDo\"\n",
    "    existing_memories: list[tuple[str, str, Any]] | None = (\n",
    "        [(existing_item.key, tool_name, existing_item.value)\n",
    "         for existing_item in existing_items]\n",
    "        if existing_items\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Merge the chat history and the instruction\n",
    "    trustcall_instruction_formatted: str = TRUSTCALL_INSTRUCTION.format(\n",
    "        time=datetime.now().isoformat()\n",
    "    )\n",
    "    updated_messages: list[BaseMessage] = list(\n",
    "        merge_message_runs(\n",
    "            messages=[SystemMessage(content=trustcall_instruction_formatted)] + state[\"messages\"][:-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Initialize the spy for visibility into the tool calls made by Trustcall\n",
    "    spy: Spy = Spy()\n",
    "\n",
    "    # Create the Trustcall extractor for updating the ToDo list\n",
    "    todo_extractor: Runnable[InputsLike, ExtractionOutputs] = create_extractor(\n",
    "        model,\n",
    "        tools=[ToDo],\n",
    "        tool_choice=tool_name,\n",
    "        enable_inserts=True\n",
    "    ).with_listeners(on_end=spy)\n",
    "\n",
    "    # Invoke the extractor\n",
    "    result = todo_extractor.invoke({\n",
    "        \"messages\": updated_messages,\n",
    "        \"existing\": existing_memories\n",
    "    })\n",
    "\n",
    "    # Save the memories from Trustcall to the store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"], strict=False):\n",
    "        store.put(\n",
    "            namespace,\n",
    "            rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "            r.model_dump(mode=\"json\"),\n",
    "        )\n",
    "\n",
    "    # Respond to the tool call made in tasks_democracy_ai, confirming the update\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "\n",
    "    # Extract the changes made by Trustcall and add to the ToolMessage returned to tasks_democracy_ai\n",
    "    todo_update_msg: str = extract_tool_info(spy.called_tools, tool_name)\n",
    "    return {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": todo_update_msg,\n",
    "            \"tool_call_id\": tool_calls[0]['id']\n",
    "        }]\n",
    "    }\n",
    "\n",
    "def update_instructions(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> dict[str, list[dict[str, str]]]:\n",
    "    \"\"\"Reflect on the chat history and update the instructions in memory.\n",
    "\n",
    "    This function processes the chat history to extract and update user-specified\n",
    "    preferences for managing the todo list. It stores these instructions for future\n",
    "    reference.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Dict containing a tool message confirming the instructions update\n",
    "        Format: {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"updated instructions\",\n",
    "                \"tool_call_id\": str\n",
    "            }]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Get the user ID from the config\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    user_id = configurable.user_id\n",
    "\n",
    "    namespace: tuple[str, str] = (\"instructions\", user_id)\n",
    "\n",
    "    existing_memory = store.get(namespace, \"user_instructions\")\n",
    "\n",
    "    # Format the memory in the system prompt\n",
    "    system_msg: str = CREATE_INSTRUCTIONS.format(\n",
    "        current_instructions=existing_memory.value if existing_memory else None\n",
    "    )\n",
    "    new_memory: BaseMessage = model.invoke(\n",
    "        [SystemMessage(content=system_msg)] +\n",
    "        state['messages'][:-1] +\n",
    "        [HumanMessage(content=\"Please update the instructions based on the conversation\")]\n",
    "    )\n",
    "\n",
    "    # Overwrite the existing memory in the store\n",
    "    key: str = \"user_instructions\"\n",
    "    store.put(namespace, key, {\"memory\": new_memory.content})\n",
    "\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "\n",
    "    # Return tool message with update verification\n",
    "    return {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"updated instructions\",\n",
    "            \"tool_call_id\": tool_calls[0]['id']\n",
    "        }]\n",
    "    }\n",
    "\n",
    "# Conditional edge\n",
    "def route_message(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> Literal[END, \"update_todos\", \"update_instructions\", \"update_profile\"]:\n",
    "    \"\"\"Route messages to appropriate memory update functions based on tool call type.\n",
    "\n",
    "    This function examines the latest message in the state and determines which memory\n",
    "    update function should handle it based on the tool call's update_type.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Literal indicating which node should process the message next:\n",
    "        - END: No tool calls present\n",
    "        - \"update_todos\": Route to todo list update\n",
    "        - \"update_instructions\": Route to instructions update\n",
    "        - \"update_profile\": Route to profile update\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the tool call's update_type is not recognized\n",
    "    \"\"\"\n",
    "    message = state['messages'][-1]\n",
    "    if len(message.tool_calls) == 0:\n",
    "        return END\n",
    "    else:\n",
    "        tool_call = message.tool_calls[0]\n",
    "        if tool_call['args']['update_type'] == \"user\":\n",
    "            return \"update_profile\"\n",
    "        elif tool_call['args']['update_type'] == \"todo\":\n",
    "            return \"update_todos\"\n",
    "        elif tool_call['args']['update_type'] == \"instructions\":\n",
    "            return \"update_instructions\"\n",
    "        else:\n",
    "            raise ValueError(\"Unknown update_type in tool call\")\n",
    "\n",
    "# SOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/create-react-agent-memory.ipynb\n",
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "\n",
    "# Create the graph + all nodes\n",
    "builder = StateGraph(MessagesState, config_schema=configuration.Configuration)\n",
    "\n",
    "# Define the flow of the memory extraction process\n",
    "builder.add_node(tasks_democracy_ai)\n",
    "builder.add_node(update_todos)\n",
    "builder.add_node(update_profile)\n",
    "builder.add_node(update_instructions)\n",
    "\n",
    "# Define the flow\n",
    "builder.add_edge(START, \"tasks_democracy_ai\")\n",
    "builder.add_conditional_edges(\"tasks_democracy_ai\", route_message)\n",
    "builder.add_edge(\"update_todos\", \"tasks_democracy_ai\")\n",
    "builder.add_edge(\"update_profile\", \"tasks_democracy_ai\")\n",
    "builder.add_edge(\"update_instructions\", \"tasks_democracy_ai\")\n",
    "\n",
    "# Compile the graph\n",
    "graph: CompiledStateGraph = builder.compile()\n",
    "\n",
    "print(graph.get_graph().print_ascii())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected dict, got [HumanMessage(content='I have 30 minutes, what tasks can I get done?', additional_kwargs={}, response_metadata={})]\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Chat with the chatbot\u001b[39;00m\n\u001b[1;32m      6\u001b[0m input_messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI have 30 minutes, what tasks can I get done?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m----> 8\u001b[0m \u001b[43mprint_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Run the graph\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     chunk[\"messages\"][-1].pretty_print()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# inputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print_stream(graph.stream(inputs, stream_mode=\"values\"))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 638\u001b[0m, in \u001b[0;36mprint_stream\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_stream\u001b[39m(stream):\n\u001b[0;32m--> 638\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1573\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1568\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1569\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1571\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1573\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1580\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:104\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    102\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, writer)\u001b[0m\n\u001b[1;32m     38\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:176\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m    175\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m--> 176\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    178\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/pregel/write.py:85\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     writes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     80\u001b[0m         ChannelWriteEntry(write\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;28minput\u001b[39m, write\u001b[38;5;241m.\u001b[39mskip_none, write\u001b[38;5;241m.\u001b[39mmapper)\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m write\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[1;32m     84\u001b[0m     ]\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_at_least_one_of\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/pregel/write.py:130\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[0;34m(config, writes, require_at_least_one_of)\u001b[0m\n\u001b[1;32m    127\u001b[0m entries \u001b[38;5;241m=\u001b[39m [write \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m writes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry)]\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# process entries into values\u001b[39;00m\n\u001b[1;32m    129\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 130\u001b[0m     \u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m write\u001b[38;5;241m.\u001b[39mmapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m entries\n\u001b[1;32m    132\u001b[0m ]\n\u001b[1;32m    133\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    134\u001b[0m     (write\u001b[38;5;241m.\u001b[39mchannel, val)\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val, write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, entries)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m write\u001b[38;5;241m.\u001b[39mskip_none \u001b[38;5;129;01mor\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m ]\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# filter out SKIP_WRITE values\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/graph/state.py:649\u001b[0m, in \u001b[0;36mCompiledStateGraph.attach_node.<locals>._get_state_key\u001b[0;34m(input, key)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    645\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[1;32m    646\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    647\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mINVALID_GRAPH_NODE_RETURN_VALUE,\n\u001b[1;32m    648\u001b[0m     )\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(msg)\n",
      "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got [HumanMessage(content='I have 30 minutes, what tasks can I get done?', additional_kwargs={}, response_metadata={})]\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"Lance\"}}\n",
    "\n",
    "# Chat with the chatbot\n",
    "input_messages = [HumanMessage(content=\"I have 30 minutes, what tasks can I get done?\")]\n",
    "\n",
    "print_stream(graph.stream(input_messages, stream_mode=\"values\"))\n",
    "# Run the graph\n",
    "# for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "#     chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "\n",
    "# inputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\n",
    "# print_stream(graph.stream(inputs, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
