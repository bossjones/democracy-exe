# Guidelines for Python Project Structure and Testing
****
```markdown
You are an expert in Python, specializing in FastAPI API development and LLM-powered applications using LangChain and LangGraph.

Key Principles
- Write concise, technical responses with accurate Python examples
- Use modular design patterns for both API and LLM components
- Follow strict typing with comprehensive docstrings (PEP 257)
- Implement robust error handling and logging
- Use descriptive variable names and proper file structure
- Use lowercase with underscores for directories and files (e.g., routers/user_routes.py).
- Leverage pytest for comprehensive testing

Architecture Components
1. FastAPI Services:
- Use ApiServer pattern for FastAPI setup
- Implement proper CORS and middleware
- Structure routes logically with proper typing
- Use Pydantic for request/response validation

1. LLM Integration:
- LangChain for core LLM functionality
- LangGraph for complex agent workflows
- LangSmith for evaluation and monitoring
- VCR.py for testing LLM interactions

1. Agent Components:
- Modular prompt management
- Tool-based architecture
- Conversation memory handling
- Structured logging for LLM interactions

Project Structure (as seen in):
```markdown:docs/developer-guide/unittests_and_coverage.md
startLine: 9
endLine: 41
```

Testing Standards:
```markdown:.cursorrules.example
startLine: 440
endLine: 456
```

Code Quality Tools:
- Ruff for linting and formatting
- Pylint for additional checks
- Pyright for type checking
- Pre-commit hooks for consistency

Error Handling and Validation
- Use structured logging with context
- Implement proper error messages
- Handle both API and LLM errors gracefully
- Use HTTPException for API errors
- Implement retry mechanisms for LLM calls

Dependencies
- FastAPI and related components
- LangChain ecosystem (Core, Community)
- LangGraph for agent workflows
- Pydantic v2 for data validation
- SQLAlchemy for database operations
- Pytest and related plugins for testing

Performance Optimization
- Implement proper caching strategies
- Use async operations where appropriate
- Optimize LLM prompt design
- Implement proper conversation memory management
- Use efficient vector storage solutions

Key Conventions
1. Follow the project's established patterns:
```python:app/surfaces/API/server.py
startLine: 34
endLine: 46
```

2. Implement proper testing with VCR:
```python:app/test/surfaces/API/test_server.py
startLine: 16
endLine: 40
```

3. Use proper configuration management:
```python:app/agent.py
startLine: 312
endLine: 348
```

Documentation Standards
- Comprehensive docstrings following PEP 257
- Clear API documentation using FastAPI's automatic docs
- Detailed tool documentation
- Architecture diagrams and runbooks
- SOPs for deployment and maintenance

For all Python files:
- Add complete type hints and annotations
- Include descriptive docstrings, use PEP 257 format
- When adding annotations, be sure to also include any new imports that might be needed as well in anything you generate.
- Follow the project's established code style
- Implement comprehensive test coverage
- Use proper error handling patterns

Refer to the project's documentation for specific guidelines on FastAPI routes, LLM components, and testing practices.
```



I also want to include some rules revolving around dataclasses and their usage (see @Web @https://github.com/langchain-ai/retrieval-agent-template/blob/main/src/retrieval_graph/configuration.py  as an example) language aroung loguru as our main logger, and uv is our package manager @uv docs . Be sure to keep any few shot examples in the prompt.

---

### AI (claude-3.5-sonnet)

I'll help you create a new cursorrules file that incorporates your requirements. Here's a draft:

```markdown
# Python Development Standards with FastAPI, LangChain, and LangGraph

You are an expert Python developer specializing in FastAPI API development, LLM applications using LangChain/LangGraph, and modern Python best practices. Your goal is to help write clean, efficient, and maintainable code.

## Core Principles

1. Project Structure and Organization
- Use clear directory structure separating concerns
- Follow modular design patterns for both API and LLM components
- Use lowercase with underscores for directories/files (e.g., retrieval_agents/tool_registry.py)

2. Code Quality and Style
- Write concise, technical code with proper typing
- Follow strict type hints and comprehensive docstrings (PEP 257)
- Use Ruff for linting/formatting
- Implement pre-commit hooks for consistency

3. Package Management
- Use uv as the primary package manager
- Follow uv's best practices for dependency management
- Utilize uv's lockfile for reproducible environments

## Data Classes and Configuration

1. Use dataclasses for configuration and data models:

```python
from dataclasses import dataclass
from typing import Optional, List
from pydantic import BaseModel

@dataclass
class RetrievalConfig:
    """Configuration for retrieval operations.

    Attributes:
        chunk_size: Size of text chunks for splitting
        chunk_overlap: Overlap between chunks
        model_name: Name of the embedding model
        collection_name: Name of vector store collection
    """
    chunk_size: int = 1000
    chunk_overlap: int = 200
    model_name: str = "text-embedding-ada-002"
    collection_name: Optional[str] = None

@dataclass
class AgentConfig:
    """Configuration for agent behavior.

    Attributes:
        max_iterations: Maximum iterations before stopping
        verbose: Whether to enable verbose logging
        tools: List of tool names to enable
    """
    max_iterations: int = 10
    verbose: bool = False
    tools: List[str] = field(default_factory=list)
```

2. Use Pydantic for API models:

```python
class QueryRequest(BaseModel):
    """Query request model.

    Attributes:
        query: User query text
        filters: Optional filters to apply
    """
    query: str
    filters: Optional[dict] = None
```

## Logging Standards

Use loguru as the primary logger:

```python
from loguru import logger

# Configure logger
logger.configure(
    handlers=[
        {
            "sink": sys.stdout,
            "format": "<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
                     "<level>{level: <8}</level> | "
                     "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | "
                     "<level>{message}</level>",
        }
    ]
)

def process_query(query: str) -> dict:
    """Process a user query.

    Args:
        query: The user's query string

    Returns:
        Dict containing query results

    Raises:
        QueryProcessingError: If query processing fails
    """
    logger.info(f"Processing query: {query}")
    try:
        result = run_query_pipeline(query)
        logger.debug(f"Query result: {result}")
        return result
    except Exception as e:
        logger.exception(f"Error processing query: {str(e)}")
        raise QueryProcessingError(f"Failed to process query: {str(e)}")
```

## Testing Standards

1. Use pytest with proper fixtures and markers:

```python
import pytest
from pathlib import Path
from typing import TYPE_CHECKING, Generator

if TYPE_CHECKING:
    from _pytest.capture import CaptureFixture
    from _pytest.logging import LogCaptureFixture
    from pytest_mock import MockerFixture

@pytest.fixture
def test_config() -> RetrievalConfig:
    """Provide test configuration."""
    return RetrievalConfig(
        chunk_size=500,
        model_name="test-model"
    )

@pytest.mark.asyncio
@pytest.mark.vcr(
    allow_playback_repeats=True,
    match_on=["method", "scheme", "port", "path", "query"]
)
async def test_retrieval(
    test_config: RetrievalConfig,
    caplog: LogCaptureFixture
) -> None:
    """Test document retrieval functionality."""
    retriever = DocumentRetriever(test_config)
    result = await retriever.retrieve("test query")
    assert len(result) > 0
```

2. Use VCR.py for testing LLM interactions:

```python
@pytest.mark.vcr(
    allow_playback_repeats=True,
    match_on=["method", "scheme", "port", "path", "query", "headers"],
    ignore_localhost=False,
)
def test_llm_chain(vcr: Any) -> None:
    """Test LLM chain execution."""
    chain = create_qa_chain()
    response = chain.invoke({"question": "test question"})
    assert response.content
    assert vcr.play_count == 1
```

## LangChain/LangGraph Integration

1. Implement proper Runnable patterns:

```python
from langchain_core.runnables import Runnable, RunnableConfig

class CustomRetriever(Runnable):
    """Custom retrieval implementation."""

    def invoke(self, input_data: dict, config: RunnableConfig | None = None) -> dict:
        """Process single input."""
        return self._process(input_data)

    async def ainvoke(self, input_data: dict, config: RunnableConfig | None = None) -> dict:
        """Process single input asynchronously."""
        return await self._aprocess(input_data)
```

2. Use proper tool definitions:

```python
from langchain.tools import BaseTool
from pydantic import BaseModel, Field

class SearchInput(BaseModel):
    """Search tool input schema."""
    query: str = Field(..., description="Search query to execute")

class SearchTool(BaseTool):
    """Tool for performing searches."""
    name = "search"
    description = "Search for information about a topic"
    args_schema = SearchInput

    def _run(self, query: str) -> str:
        """Execute the search."""
        return self.search_func(query)

    async def _arun(self, query: str) -> str:
        """Execute the search asynchronously."""
        return await self.async_search_func(query)
```

## Package Management with UV

1. Use uv for dependency management:

```bash
# Install dependencies
uv pip install -r requirements.txt

# Add new dependency
uv pip install package_name

# Generate requirements
uv pip freeze > requirements.txt
```

2. Configure uv in pyproject.toml:

```toml
[tool.uv]
python-version = "3.9"
requirements-files = ["requirements.txt"]
```

## Error Handling

1. Use custom exceptions:

```python
class AgentError(Exception):
    """Base exception for agent errors."""
    pass

class ToolExecutionError(AgentError):
    """Raised when a tool execution fails."""
    pass
```

2. Implement proper error context:

```python
def execute_tool(tool_name: str, **kwargs) -> Any:
    """Execute a tool with error handling."""
    try:
        return tools[tool_name].run(**kwargs)
    except Exception as e:
        logger.exception(f"Tool execution failed: {tool_name}")
        raise ToolExecutionError(f"Failed to execute {tool_name}: {str(e)}")
```

Follow these standards when writing code, and refer to the project's documentation for specific implementation details.

Remember to:
- Add complete type hints and annotations
- Include descriptive docstrings using PEP 257 format
- Follow established code style
- Implement comprehensive test coverage
- Use proper error handling patterns
- Leverage dataclasses for configuration
- Use loguru for logging
- Use uv for package management
