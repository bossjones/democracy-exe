"""
This type stub file was generated by pyright.
"""

from tokenizers import Tokenizer
from ..convert_slow_tokenizer import LlamaConverter, Qwen2Converter

"""
Integration with GGML / The file is copied and adapted from https://github.com/99991/pygguf
with extra methods beings exposed
"""
logger = ...
GGML_TYPES = ...
GGML_BLOCK_SIZES = ...
DATA_TYPES = ...
GGUF_TENSOR_MAPPING = ...
GGUF_CONFIG_MAPPING = ...
GGUF_TOKENIZER_MAPPING = ...
def dequantize_q4_k(data, n_bytes: int): # -> NDArray[floating[Any]]:
    ...

def dequantize_q4_0(data, n_bytes: int): # -> NDArray[floating[_32Bit]]:
    ...

def dequantize_q6_k(data, n_bytes: int): # -> NDArray[floating[Any]]:
    ...

def dequantize_q8_0(data, n_bytes: int): # -> NDArray[floating[Any]]:
    ...

def dequantize_q2_k(data, n_bytes: int): # -> NDArray[floating[Any]]:
    ...

def dequantize_q3_k(data, n_bytes: int): # -> NDArray[floating[Any]]:
    ...

def dequantize_q5_k(data, n_bytes: int): # -> NDArray[floating[Any]]:
    ...

def load_dequant_gguf_tensor(shape, ggml_type, data, n_bytes): # -> ndarray[Any, dtype[floating[_32Bit]]] | ndarray[Any, dtype[floating[Any]]]:
    ...

class GGUFTokenizerSkeleton:
    def __init__(self, dict_) -> None:
        ...
    


class GGUFLlamaConverter(LlamaConverter):
    def __init__(self, tokenizer_dict) -> None:
        ...
    
    def vocab(self, proto): # -> list[tuple[Any, Any]]:
        ...
    
    def merges(self, proto):
        ...
    
    def tokenizer(self, proto): # -> Tokenizer:
        ...
    
    def decoder(self, replacement, add_prefix_space): # -> Sequence:
        ...
    
    def converted(self): # -> Tokenizer:
        ...
    


class GGUFQwen2Converter(Qwen2Converter):
    def __init__(self, tokenizer_dict) -> None:
        ...
    
    def converted(self) -> Tokenizer:
        ...
    


GGUF_TO_FAST_CONVERTERS = ...
def convert_gguf_tokenizer(architecture, tokenizer_dict) -> Tokenizer:
    """
    Utilities to convert a slow tokenizer instance in a fast tokenizer instance.

    Args:
        architecture (`str`): The model architecture derived from gguf file.
        transformer_tokenizer ([`~tokenization_utils_base.PreTrainedTokenizer`]):
            Instance of a slow tokenizer to convert in the backend tokenizer for
            [`~tokenization_utils_base.PreTrainedTokenizerFast`].

    Return:
        A instance of [`~tokenizers.Tokenizer`] to be used as the backend tokenizer of a
        [`~tokenization_utils_base.PreTrainedTokenizerFast`]
    """
    ...

