
> [!NOTE]
> Documentation auto-generated by [ai-docs.](https://github.com/connor-john/ai-docs)

I'll create comprehensive documentation and testing examples for the `FakeTracer` module.

# FakeTracer Module Documentation

## Module Overview
The `FakeTracer` module provides a testing utility for LangChain applications that replaces dynamic UUIDs with deterministic ones for reliable snapshot testing. It extends the `BaseTracer` class to track and modify execution runs while maintaining consistent IDs across test runs.

### Key Features
- Deterministic UUID generation for reproducible tests
- Message ID replacement for LangChain message objects
- Hierarchical run tracking with parent-child relationships
- Support for LangChain's messaging and generation objects

### Dependencies
- langchain-core
- Python typing library
- UUID module

## Installation and Setup

```bash
pip install langchain-core
```

## Usage Guide

### Basic Usage

```python
from fake_tracer import FakeTracer

# Initialize the tracer
tracer = FakeTracer()

# Use with LangChain components
# The tracer will automatically capture and modify runs
```

### Example Usage Scenarios

```python
# Track a simple run
from langchain_core.tracers import Run

tracer = FakeTracer()
run = Run(
    id=UUID('random-uuid'),
    name="test_run",
    inputs={"prompt": "Hello"},
    outputs={"response": "Hi"}
)
tracer._persist_run(run)

# Access tracked runs
all_runs = tracer.runs
flattened_runs = tracer.flattened_runs()
run_ids = tracer.run_ids
```

## Testing Guide

### Test Setup

```python
# test_fake_tracer.py

from uuid import UUID
import pytest
from langchain_core.messages import HumanMessage
from langchain_core.tracers import Run
from fake_tracer import FakeTracer

@pytest.fixture
def tracer():
    """Create a fresh FakeTracer instance for each test."""
    return FakeTracer()

@pytest.fixture
def sample_run():
    """Create a sample run for testing."""
    return Run(
        id=UUID('12345678-1234-5678-1234-567812345678'),
        name="test_run",
        inputs={"prompt": "Hello"},
        outputs={"response": "Hi"}
    )
```

### Basic Functionality Tests

```python
def test_uuid_replacement(tracer):
    """Test that UUIDs are replaced consistently."""
    original_uuid = UUID('12345678-1234-5678-1234-567812345678')
    replaced_uuid = tracer._replace_uuid(original_uuid)

    assert replaced_uuid == UUID('00000000-0000-4000-8000-000000000000')
    # Second replacement should return the same UUID
    assert tracer._replace_uuid(original_uuid) == replaced_uuid

def test_message_id_replacement(tracer):
    """Test replacement of message IDs in various objects."""
    message = HumanMessage(content="Hello", id="original-id")
    replaced_message = tracer._replace_message_id(message)

    assert replaced_message.id != "original-id"
    assert UUID(replaced_message.id).version == 4

def test_run_persistence(tracer, sample_run):
    """Test that runs are properly persisted with replaced IDs."""
    tracer._persist_run(sample_run)

    assert len(tracer.runs) == 1
    persisted_run = tracer.runs[0]
    assert persisted_run.id != sample_run.id
    assert persisted_run.name == sample_run.name
```

### Hierarchical Run Tests

```python
def test_nested_runs(tracer):
    """Test handling of nested runs with parent-child relationships."""
    child_run = Run(
        id=UUID('22345678-1234-5678-1234-567812345678'),
        name="child_run"
    )
    parent_run = Run(
        id=UUID('12345678-1234-5678-1234-567812345678'),
        name="parent_run",
        child_runs=[child_run]
    )

    tracer._persist_run(parent_run)

    assert len(tracer.flattened_runs()) == 2
    assert tracer.runs[0].child_runs[0].name == "child_run"

def test_dotted_order_processing(tracer):
    """Test processing of dotted order in runs."""
    run = Run(
        id=UUID('12345678-1234-5678-1234-567812345678'),
        dotted_order="2023-01-01T00:00:00.000Z12345678-1234-5678-1234-567812345678"
    )

    tracer._persist_run(run)

    assert tracer.runs[0].dotted_order.endswith(
        str(UUID('00000000-0000-4000-8000-000000000000'))
    )
```

### Edge Cases and Error Handling

```python
def test_empty_run(tracer):
    """Test handling of runs with minimal data."""
    empty_run = Run(
        id=UUID('12345678-1234-5678-1234-567812345678'),
    )

    tracer._persist_run(empty_run)
    assert len(tracer.runs) == 1

def test_none_values(tracer):
    """Test handling of None values in runs."""
    run = Run(
        id=UUID('12345678-1234-5678-1234-567812345678'),
        trace_id=None,
        parent_run_id=None,
        dotted_order=None
    )

    tracer._persist_run(run)
    assert tracer.runs[0].trace_id is None
    assert tracer.runs[0].parent_run_id is None
```

## Testing Best Practices

1. Always use fixtures for common test data
2. Test both simple and complex hierarchical structures
3. Verify UUID consistency across multiple operations
4. Test all supported message and generation types
5. Include edge cases and None values
6. Verify proper cleanup of test resources

## Common Test Fixtures

```python
@pytest.fixture
def complex_run_hierarchy():
    """Create a complex run hierarchy for testing."""
    child_runs = [
        Run(
            id=UUID(f'12345678-1234-5678-1234-{i:012}'),
            name=f"child_{i}"
        )
        for i in range(3)
    ]

    return Run(
        id=UUID('12345678-1234-5678-1234-567812345678'),
        name="parent",
        child_runs=child_runs
    )
```

## Debugging and Troubleshooting

To debug tests:

```python
import pytest
import logging

logging.basicConfig(level=logging.DEBUG)

def test_with_debugging(tracer, caplog):
    """Test with debug logging enabled."""
    caplog.set_level(logging.DEBUG)
    # Your test code here
    assert "Expected log message" in caplog.text
```

Remember to:
- Use meaningful test names
- Document test purposes clearly
- Include proper type hints
- Test both success and edge cases
- Maintain test isolation
- Clean up any resources after tests

This documentation provides a solid foundation for working with and testing the FakeTracer module. Additional tests can be added based on specific use cases and requirements.
