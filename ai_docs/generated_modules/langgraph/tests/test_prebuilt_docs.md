
> [!NOTE]
> Documentation auto-generated by [ai-docs.](https://github.com/connor-john/ai-docs)

I'll create comprehensive documentation for this Python module, which appears to be focused on testing agent and tool functionalities, particularly around tool execution and state management.

## Module Overview

This module provides testing utilities for a graph-based agent system that handles tool execution, state management, and checkpointing. The core features include:

- Testing tool execution and validation
- Agent state management and modification
- Message handling and validation
- Error handling for tools
- Command injection and state updates
- Checkpoint management for agent state

Key dependencies:
- langchain_core
- pytest
- pydantic (v1 and v2 compatible)
- typing_extensions

## Installation and Setup

```bash
pip install langchain-core pytest pytest-mock pytest-asyncio pydantic typing-extensions
```

## Testing Guide

### 1. Basic Tool Execution Testing

```python
import pytest
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langchain_core.tools import BaseTool
from langgraph.prebuilt import ToolNode

def test_basic_tool_execution():
    """Test basic tool execution functionality."""
    @dec_tool
    def add(a: int, b: int) -> int:
        """Add two numbers."""
        return a + b

    # Create tool node
    node = ToolNode([add])

    # Create test input with tool call
    tool_call = {
        "name": "add",
        "args": {"a": 1, "b": 2},
        "id": "test_id"
    }
    input_message = AIMessage(content="", tool_calls=[tool_call])

    # Execute tool
    result = node.invoke({"messages": [input_message]})

    # Verify result
    tool_message = result["messages"][-1]
    assert tool_message.content == "3"
    assert tool_message.tool_call_id == "test_id"
```

### 2. Error Handling Testing

```python
@pytest.mark.asyncio
async def test_tool_error_handling():
    """Test tool error handling with different error types."""
    @dec_tool
    def failing_tool(x: int) -> str:
        """A tool that fails with different errors."""
        if x == 0:
            raise ValueError("Zero not allowed")
        if x < 0:
            raise ToolException("Negative not allowed")
        return str(x)

    # Test error handling configuration
    node = ToolNode(
        [failing_tool],
        handle_tool_errors="Custom error message"
    )

    # Test ValueError handling
    tool_call_value = {
        "name": "failing_tool",
        "args": {"x": 0},
        "id": "error_test"
    }

    result = await node.ainvoke({
        "messages": [AIMessage(content="", tool_calls=[tool_call_value])]
    })

    error_message = result["messages"][-1]
    assert error_message.status == "error"
    assert "Custom error message" in error_message.content
```

### 3. State Management Testing

```python
from typing import TypedDict, Annotated
from langgraph.prebuilt import create_react_agent

class TestState(TypedDict):
    messages: list
    context: dict

@pytest.mark.parametrize("checkpointer_name", ALL_CHECKPOINTERS_SYNC)
def test_state_management(request: pytest.FixtureRequest, checkpointer_name: str):
    """Test state management and checkpointing."""
    checkpointer = request.getfixturevalue(f"checkpointer_{checkpointer_name}")

    # Create test model
    model = FakeToolCallingModel()

    # Create agent with state
    agent = create_react_agent(
        model,
        [],
        checkpointer=checkpointer
    )

    # Test input state
    input_state = {
        "messages": [HumanMessage("test")],
        "context": {"user_id": "123"}
    }

    # Execute with state
    result = agent.invoke(input_state)

    # Verify state persistence
    if checkpointer:
        saved_state = checkpointer.get_tuple({"thread_id": "123"})
        assert saved_state is not None
        assert "messages" in saved_state.checkpoint
```

### 4. Async Tool Testing

```python
@pytest.mark.asyncio
async def test_async_tool_execution():
    """Test async tool execution."""
    @dec_tool
    async def async_tool(x: int) -> str:
        """Async tool example."""
        return f"Result: {x}"

    node = ToolNode([async_tool])

    tool_call = {
        "name": "async_tool",
        "args": {"x": 42},
        "id": "async_test"
    }

    result = await node.ainvoke({
        "messages": [AIMessage(content="", tool_calls=[tool_call])]
    })

    assert result["messages"][-1].content == "Result: 42"
```

### 5. Tool Validation Testing

```python
def test_tool_validation():
    """Test tool input validation."""
    class ToolSchema(BaseModel):
        value: int
        text: str

    @dec_tool(args_schema=ToolSchema)
    def validated_tool(value: int, text: str) -> str:
        return f"{value}-{text}"

    validation_node = ValidationNode([validated_tool])

    # Test valid input
    valid_call = {
        "name": "validated_tool",
        "args": {"value": 1, "text": "test"},
        "id": "valid_test"
    }

    result = validation_node.invoke({
        "messages": [AIMessage(content="", tool_calls=[valid_call])]
    })

    assert not result["messages"][-1].additional_kwargs.get("is_error")

    # Test invalid input
    invalid_call = {
        "name": "validated_tool",
        "args": {"value": "invalid", "text": 123},
        "id": "invalid_test"
    }

    result = validation_node.invoke({
        "messages": [AIMessage(content="", tool_calls=[invalid_call])]
    })

    assert result["messages"][-1].additional_kwargs.get("is_error")
```

## Test Fixtures

```python
@pytest.fixture
def fake_model():
    """Fixture providing a fake tool-calling model."""
    return FakeToolCallingModel(
        tool_calls=[[
            ToolCall(name="test_tool", args={"x": 1}, id="test_id")
        ]]
    )

@pytest.fixture
def memory_store():
    """Fixture providing an in-memory store."""
    store = InMemoryStore()
    store.put(("test",), "test_key", {"value": "test"})
    return store
```

## Best Practices

1. Always use type hints in test functions
2. Use parametrize for testing multiple scenarios
3. Test both sync and async versions of functionality
4. Include error handling tests
5. Use fixtures for common test setup
6. Test state management thoroughly
7. Validate tool inputs and outputs
8. Test checkpointing functionality
9. Include edge cases in tests
10. Test different message formats

## Error Handling Testing Patterns

```python
def test_error_handling_patterns():
    """Test different error handling patterns."""
    def handle_value_error(e: ValueError) -> str:
        return f"ValueError: {str(e)}"

    def handle_tool_error(e: ToolException) -> str:
        return f"ToolError: {str(e)}"

    # Test with specific error handlers
    node = ToolNode(
        [failing_tool],
        handle_tool_errors={
            ValueError: handle_value_error,
            ToolException: handle_tool_error
        }
    )
```

This documentation provides a foundation for testing the module's functionality. Each test case should be adapted based on specific requirements and use cases in your implementation.
