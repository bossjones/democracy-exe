
> [!NOTE]
> Documentation auto-generated by [ai-docs.](https://github.com/connor-john/ai-docs)

I'll create comprehensive documentation and testing examples for this module.

# Tool Execution Module Documentation

## Module Overview
This module provides functionality for executing tools in a structured way within a language model workflow. It consists of three main components:
- `ToolInvocationInterface`: Interface defining tool execution parameters
- `ToolInvocation`: Serializable class for tool execution requests
- `ToolExecutor`: Main executor class that handles tool invocations

Key Features:
- Synchronous and asynchronous tool execution
- Input validation and error handling
- Support for both BaseTool instances and callable functions
- Serializable tool invocation requests

Dependencies:
- langchain_core
- typing
- langgraph

## Installation and Setup

```bash
pip install langgraph langchain-core
```

## Usage Guide

### Basic Usage

```python
from langchain_core.tools import tool
from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation

# Define a tool
@tool
def search(query: str) -> str:
    """Search tool implementation."""
    return f"Search results for: {query}"

# Create executor
tools = [search]
executor = ToolExecutor(tools)

# Create and execute invocation
invocation = ToolInvocation(
    tool="search",
    tool_input="What is the capital of France?"
)
result = executor.invoke(invocation)
```

### Async Usage

```python
@tool
async def async_search(query: str) -> str:
    """Async search implementation."""
    return f"Async search results for: {query}"

# Execute asynchronously
result = await executor.ainvoke(invocation)
```

## Testing Guide

### Test Setup

```python
# test_tool_executor.py
from typing import Generator
import pytest
from langchain_core.tools import tool
from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation

@pytest.fixture
def sample_tools() -> Generator[list, None, None]:
    """Fixture providing test tools."""
    @tool
    def test_tool(input_str: str) -> str:
        """Test tool."""
        return f"Processed: {input_str}"

    yield [test_tool]

@pytest.fixture
def executor(sample_tools) -> ToolExecutor:
    """Fixture providing configured ToolExecutor."""
    return ToolExecutor(sample_tools)
```

### Basic Functionality Tests

```python
def test_tool_execution_success(
    executor: ToolExecutor,
    caplog: pytest.LogCaptureFixture
) -> None:
    """Test successful tool execution."""
    invocation = ToolInvocation(
        tool="test_tool",
        tool_input="test input"
    )
    result = executor.invoke(invocation)
    assert result == "Processed: test input"

def test_invalid_tool_handling(executor: ToolExecutor) -> None:
    """Test handling of invalid tool requests."""
    invocation = ToolInvocation(
        tool="nonexistent_tool",
        tool_input="test input"
    )
    result = executor.invoke(invocation)
    assert "is not a valid tool" in result
```

### Async Tests

```python
@pytest.mark.asyncio
async def test_async_tool_execution(
    executor: ToolExecutor,
    caplog: pytest.LogCaptureFixture
) -> None:
    """Test async tool execution."""
    @tool
    async def async_test_tool(input_str: str) -> str:
        return f"Async processed: {input_str}"

    executor = ToolExecutor([async_test_tool])
    invocation = ToolInvocation(
        tool="async_test_tool",
        tool_input="test input"
    )
    result = await executor.ainvoke(invocation)
    assert result == "Async processed: test input"
```

### Edge Case Tests

```python
def test_empty_tool_input(executor: ToolExecutor) -> None:
    """Test handling of empty tool input."""
    invocation = ToolInvocation(
        tool="test_tool",
        tool_input=""
    )
    result = executor.invoke(invocation)
    assert result == "Processed: "

@pytest.mark.parametrize("tool_input", [
    {"key": "value"},
    "simple string",
    "",
    None
])
def test_various_input_types(
    executor: ToolExecutor,
    tool_input: Union[str, dict, None]
) -> None:
    """Test handling of different input types."""
    invocation = ToolInvocation(
        tool="test_tool",
        tool_input=tool_input
    )
    result = executor.invoke(invocation)
    assert isinstance(result, str)
```

### Integration Tests

```python
from langchain_core.tools import BaseTool

def test_integration_with_base_tool() -> None:
    """Test integration with BaseTool implementation."""
    class CustomTool(BaseTool):
        name = "custom_tool"
        description = "Custom tool for testing"

        def _run(self, input_str: str) -> str:
            return f"Custom: {input_str}"

        async def _arun(self, input_str: str) -> str:
            return f"Async Custom: {input_str}"

    executor = ToolExecutor([CustomTool()])
    invocation = ToolInvocation(
        tool="custom_tool",
        tool_input="test"
    )
    result = executor.invoke(invocation)
    assert result == "Custom: test"
```

## Testing Best Practices

1. Use type hints consistently
2. Create reusable fixtures
3. Test both sync and async paths
4. Include error cases
5. Test with different input types
6. Verify deprecation warnings
7. Test integration with BaseTool
8. Use parametrized tests for input variations

## Error Handling Tests

```python
def test_tool_execution_error(executor: ToolExecutor) -> None:
    """Test handling of tool execution errors."""
    @tool
    def error_tool(_: str) -> str:
        raise ValueError("Tool error")

    executor = ToolExecutor([error_tool])
    invocation = ToolInvocation(
        tool="error_tool",
        tool_input="test"
    )

    with pytest.raises(ValueError, match="Tool error"):
        executor.invoke(invocation)
```

## Performance Testing

```python
import time

def test_tool_execution_performance(executor: ToolExecutor) -> None:
    """Test tool execution performance."""
    invocation = ToolInvocation(
        tool="test_tool",
        tool_input="test"
    )

    start_time = time.time()
    executor.invoke(invocation)
    execution_time = time.time() - start_time

    assert execution_time < 1.0  # Should complete within 1 second
```

This documentation and test suite provides a comprehensive foundation for understanding and validating the tool execution module. Remember to run tests with:

```bash
pytest test_tool_executor.py -v --cov
```

The tests cover core functionality, edge cases, error handling, and performance aspects of the module.
