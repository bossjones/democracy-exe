
> [!NOTE]
> Documentation auto-generated by [ai-docs.](https://github.com/connor-john/ai-docs)

I'll create comprehensive documentation for this module, which appears to be part of a graph processing and debugging system.

# Module Documentation: Graph Debug and Task Management

## Module Overview

This module provides debugging, task management, and checkpoint handling functionality for a graph processing system. It's primarily focused on:
- Task execution tracking and debugging
- Checkpoint management
- Debug output formatting
- Task state management and visualization

### Key Features
- Debug event streaming for tasks and checkpoints
- Pretty printing of task execution states
- Checkpoint management with metadata
- Task state mapping and transformation

### Dependencies
- langchain_core
- langgraph
- Python 3.7+
- typing-extensions (for TypedDict support)

## Installation and Setup

```bash
pip install langgraph langchain-core
```

## Usage Guide

### 1. Task Debugging

```python
from langgraph.pregel import PregelExecutableTask

# Create sample tasks
tasks = [
    PregelExecutableTask(
        id="task1",
        name="process_data",
        input={"data": "sample"},
        triggers=["event1"],
        config={}
    )
]

# Generate debug output for tasks
debug_outputs = list(map_debug_tasks(step=1, tasks=tasks))
```

### 2. Checkpoint Management

```python
from datetime import datetime, timezone

# Create a checkpoint
checkpoint = {
    "ts": datetime.now(timezone.utc).isoformat(),
    "metadata": {
        "step": 1,
        "status": "completed"
    }
}

# Generate checkpoint debug output
debug_output = map_debug_checkpoint(
    step=1,
    config={"conf": {}},
    channels={"output": BaseChannel()},
    stream_channels=["output"],
    metadata=checkpoint["metadata"],
    checkpoint=checkpoint,
    tasks=[],
    pending_writes=[],
    parent_config=None,
    output_keys=["output"]
)
```

## Testing Guide

### 1. Basic Task Testing

```python
# test_debug.py
from datetime import datetime, timezone
import pytest
from typing import Generator

@pytest.fixture
def sample_task() -> Generator[PregelExecutableTask, None, None]:
    """Provide a sample task for testing."""
    task = PregelExecutableTask(
        id="test_id",
        name="test_task",
        input={"test": "data"},
        triggers=["trigger1"],
        config={}
    )
    yield task

def test_map_debug_tasks(sample_task: PregelExecutableTask) -> None:
    """Test debug task mapping functionality."""
    debug_outputs = list(map_debug_tasks(1, [sample_task]))

    assert len(debug_outputs) == 1
    assert debug_outputs[0]["type"] == "task"
    assert debug_outputs[0]["step"] == 1
    assert debug_outputs[0]["payload"]["id"] == "test_id"
    assert debug_outputs[0]["payload"]["name"] == "test_task"
```

### 2. Task Result Testing

```python
@pytest.mark.asyncio
async def test_map_debug_task_results(
    sample_task: PregelExecutableTask,
    mocker: MockerFixture
) -> None:
    """Test debug task result mapping."""
    writes = [("output", "result_data")]
    results = list(map_debug_task_results(
        step=1,
        task_tup=(sample_task, writes),
        stream_keys="output"
    ))

    assert len(results) == 1
    assert results[0]["type"] == "task_result"
    assert results[0]["payload"]["name"] == "test_task"
    assert results[0]["payload"]["result"] == [("output", "result_data")]
```

### 3. Checkpoint Testing

```python
@pytest.fixture
def sample_checkpoint() -> Generator[dict, None, None]:
    """Provide a sample checkpoint for testing."""
    yield {
        "ts": datetime.now(timezone.utc).isoformat(),
        "metadata": {
            "step": 1,
            "status": "complete"
        }
    }

def test_map_debug_checkpoint(
    sample_checkpoint: dict,
    sample_task: PregelExecutableTask
) -> None:
    """Test checkpoint debug output mapping."""
    outputs = list(map_debug_checkpoint(
        step=1,
        config={"conf": {}},
        channels={},
        stream_channels=[],
        metadata=sample_checkpoint["metadata"],
        checkpoint=sample_checkpoint,
        tasks=[sample_task],
        pending_writes=[],
        parent_config=None,
        output_keys=[]
    ))

    assert len(outputs) == 1
    assert outputs[0]["type"] == "checkpoint"
    assert outputs[0]["step"] == 1
    assert "tasks" in outputs[0]["payload"]
```

### 4. Print Function Testing

```python
def test_print_step_tasks(
    sample_task: PregelExecutableTask,
    caplog: LogCaptureFixture
) -> None:
    """Test task printing functionality."""
    print_step_tasks(1, [sample_task])
    assert "[1:tasks]" in caplog.text
    assert "test_task" in caplog.text
```

## Testing Best Practices

1. Use Fixtures for Common Objects
```python
@pytest.fixture
def base_config() -> Generator[RunnableConfig, None, None]:
    """Provide base configuration for tests."""
    yield {
        "conf": {
            "thread_id": "test_thread",
            "checkpoint_ns": "test_ns"
        }
    }
```

2. Mock Time-Dependent Functions
```python
@pytest.fixture
def mock_datetime(mocker: MockerFixture) -> Generator[MockerFixture, None, None]:
    """Mock datetime for consistent testing."""
    mock_now = datetime(2023, 1, 1, tzinfo=timezone.utc)
    datetime_mock = mocker.patch('datetime.datetime')
    datetime_mock.now.return_value = mock_now
    yield datetime_mock
```

3. Test Error Cases
```python
def test_map_debug_task_results_with_error(sample_task: PregelExecutableTask) -> None:
    """Test handling of task errors in debug output."""
    writes = [("error", "Test error occurred")]
    results = list(map_debug_task_results(1, (sample_task, writes), "output"))

    assert results[0]["payload"]["error"] == "Test error occurred"
```

## Error Handling and Edge Cases

```python
def test_empty_task_list() -> None:
    """Test handling of empty task lists."""
    debug_outputs = list(map_debug_tasks(1, []))
    assert len(debug_outputs) == 0

def test_invalid_channel_name() -> None:
    """Test handling of invalid channel names."""
    with pytest.raises(KeyError):
        list(map_debug_checkpoint(
            step=1,
            config={},
            channels={},
            stream_channels=["nonexistent"],
            metadata={},
            checkpoint={},
            tasks=[],
            pending_writes=[],
            parent_config=None,
            output_keys=[]
        ))
```

This documentation covers the main functionality and testing approaches for the module. Additional sections could be added for performance testing, integration testing, and more specific use cases as needed.
