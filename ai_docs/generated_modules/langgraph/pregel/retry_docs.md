
> [!NOTE]
> Documentation auto-generated by [ai-docs.](https://github.com/connor-john/ai-docs)

I'll create comprehensive documentation and test examples for this module.

# Retry Handler Module Documentation

## Module Overview

This module provides robust retry functionality for both synchronous and asynchronous task execution in a graph-based workflow system. It implements exponential backoff, jitter, and configurable retry policies.

### Key Features
- Synchronous and asynchronous retry handling
- Configurable retry policies
- Exponential backoff with optional jitter
- Exception-based retry control
- Checkpoint namespace management
- Proper error propagation

### Dependencies
- Python 3.11+ (for full exception note support)
- asyncio
- dataclasses
- langgraph package

## Installation and Setup

```bash
pip install langgraph
```

## Usage Guide

### Basic Usage

```python
from langgraph.types import RetryPolicy, PregelExecutableTask

# Create a retry policy
retry_policy = RetryPolicy(
    max_attempts=3,
    initial_interval=1.0,
    max_interval=60.0,
    backoff_factor=2.0,
    jitter=True,
    retry_on=[ValueError, ConnectionError]
)

# Synchronous execution
run_with_retry(task, retry_policy)

# Asynchronous execution
await arun_with_retry(task, retry_policy)
```

## Testing Guide

### Test Setup

```python
# test_retry_handler.py
from __future__ import annotations

import pytest
import asyncio
from typing import Generator, Any
from pytest_mock import MockerFixture
from dataclasses import dataclass

from langgraph.types import RetryPolicy, PregelExecutableTask
from your_module import run_with_retry, arun_with_retry

# Test fixtures
@pytest.fixture
def retry_policy() -> RetryPolicy:
    """Create a standard retry policy for testing."""
    return RetryPolicy(
        max_attempts=3,
        initial_interval=0.1,
        max_interval=1.0,
        backoff_factor=2.0,
        jitter=False,
        retry_on=[ValueError, ConnectionError]
    )

@dataclass
class MockProcessor:
    """Mock processor for testing."""
    def invoke(self, input_data: Any, config: dict) -> Any:
        return input_data

    async def ainvoke(self, input_data: Any, config: dict) -> Any:
        return input_data

    async def astream(self, input_data: Any, config: dict) -> AsyncGenerator[Any, None]:
        yield input_data

@pytest.fixture
def mock_task(retry_policy: RetryPolicy) -> PregelExecutableTask:
    """Create a mock task for testing."""
    return PregelExecutableTask(
        name="test_task",
        id="test_id",
        proc=MockProcessor(),
        input={},
        config={"conf": {"checkpoint_ns": "test"}},
        retry_policy=retry_policy,
        writes=[],
        writers=[]
    )
```

### Synchronous Retry Tests

```python
def test_successful_execution(mock_task: PregelExecutableTask, retry_policy: RetryPolicy) -> None:
    """Test successful task execution without retries."""
    result = run_with_retry(mock_task, retry_policy)
    assert result == mock_task.input

def test_retry_on_error(
    mock_task: PregelExecutableTask,
    retry_policy: RetryPolicy,
    mocker: MockerFixture,
    caplog: pytest.LogCaptureFixture
) -> None:
    """Test retry behavior on error."""
    # Mock processor to fail twice then succeed
    attempt_count = 0
    def mock_invoke(*args: Any) -> Any:
        nonlocal attempt_count
        attempt_count += 1
        if attempt_count < 3:
            raise ValueError("Test error")
        return "success"

    mocker.patch.object(mock_task.proc, 'invoke', side_effect=mock_invoke)

    result = run_with_retry(mock_task, retry_policy)

    assert result == "success"
    assert attempt_count == 3
    assert "Retrying task test_task" in caplog.text

def test_max_retries_exceeded(
    mock_task: PregelExecutableTask,
    retry_policy: RetryPolicy,
    mocker: MockerFixture
) -> None:
    """Test behavior when max retries are exceeded."""
    mocker.patch.object(
        mock_task.proc,
        'invoke',
        side_effect=ValueError("Persistent error")
    )

    with pytest.raises(ValueError, match="Persistent error"):
        run_with_retry(mock_task, retry_policy)
```

### Asynchronous Retry Tests

```python
@pytest.mark.asyncio
async def test_async_successful_execution(
    mock_task: PregelExecutableTask,
    retry_policy: RetryPolicy
) -> None:
    """Test successful async task execution."""
    result = await arun_with_retry(mock_task, retry_policy)
    assert result == mock_task.input

@pytest.mark.asyncio
async def test_async_retry_on_error(
    mock_task: PregelExecutableTask,
    retry_policy: RetryPolicy,
    mocker: MockerFixture,
    caplog: pytest.LogCaptureFixture
) -> None:
    """Test async retry behavior on error."""
    attempt_count = 0
    async def mock_ainvoke(*args: Any) -> Any:
        nonlocal attempt_count
        attempt_count += 1
        if attempt_count < 3:
            raise ConnectionError("Test error")
        return "success"

    mocker.patch.object(mock_task.proc, 'ainvoke', side_effect=mock_ainvoke)

    result = await arun_with_retry(mock_task, retry_policy)

    assert result == "success"
    assert attempt_count == 3
    assert "Retrying task test_task" in caplog.text

@pytest.mark.asyncio
async def test_async_streaming(
    mock_task: PregelExecutableTask,
    retry_policy: RetryPolicy,
    mocker: MockerFixture
) -> None:
    """Test async streaming functionality."""
    async def mock_astream(*args: Any) -> AsyncGenerator[str, None]:
        yield "data1"
        yield "data2"

    mocker.patch.object(mock_task.proc, 'astream', side_effect=mock_astream)

    await arun_with_retry(mock_task, retry_policy, stream=True)
```

### Command Handling Tests

```python
def test_parent_command_handling(
    mock_task: PregelExecutableTask,
    retry_policy: RetryPolicy,
    mocker: MockerFixture
) -> None:
    """Test handling of parent commands."""
    from langgraph.errors import ParentCommand
    from langgraph.types import Command

    cmd = Command(graph="test", type="test_cmd", payload={})
    mocker.patch.object(
        mock_task.proc,
        'invoke',
        side_effect=ParentCommand(cmd)
    )

    with pytest.raises(ParentCommand) as exc_info:
        run_with_retry(mock_task, retry_policy)

    assert exc_info.value.args[0].graph == "test"
```

## Testing Best Practices

1. Use pytest markers appropriately:
   - `@pytest.mark.asyncio` for async tests
   - `@pytest.mark.parametrize` for multiple test cases

2. Mock time-based operations:
   ```python
   @pytest.fixture
   def mock_sleep(mocker: MockerFixture) -> None:
       mocker.patch('time.sleep')
       mocker.patch('asyncio.sleep')
   ```

3. Test error handling thoroughly:
   - Test different exception types
   - Verify retry counts
   - Check backoff intervals
   - Validate error messages

4. Use proper fixtures:
   - Keep tests isolated
   - Make fixtures reusable
   - Use appropriate scopes

5. Test both success and failure paths:
   - Immediate success
   - Success after retries
   - Complete failure
   - Invalid configurations

6. Validate logging:
   - Use caplog fixture
   - Check log levels
   - Verify log messages

7. Test configuration handling:
   - Test with different retry policies
   - Validate configurable parameters
   - Check default behaviors

This documentation provides a comprehensive foundation for testing the retry handler module. The test examples cover the main functionality while following best practices for Python testing.
