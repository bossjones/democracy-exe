
> [!NOTE]
> Documentation auto-generated by [ai-docs.](https://github.com/connor-john/ai-docs)

I'll create comprehensive documentation and test examples for the StreamMessagesHandler module.

# StreamMessagesHandler Documentation

## Module Overview
The `StreamMessagesHandler` is a specialized callback handler for managing message streaming in LangChain applications. It implements two key interfaces: `BaseCallbackHandler` and `_StreamingCallbackHandler`.

### Core Features
- Streams chat model messages and node outputs
- Handles message deduplication
- Supports async and sync operations
- Manages message metadata and checkpointing
- Processes various response types (single messages, sequences, dictionaries)

### Dependencies
- langchain_core
- langgraph
- Python 3.7+
- UUID support

## Installation and Setup

```bash
pip install langchain-core langgraph
```

## Usage Guide

### Basic Implementation

```python
from langgraph.types import StreamChunk

def stream_handler(chunk: StreamChunk) -> None:
    namespace, type_, payload = chunk
    print(f"Namespace: {namespace}")
    print(f"Type: {type_}")
    print(f"Payload: {payload}")

handler = StreamMessagesHandler(stream=stream_handler)
```

## Testing Guide

### Test Setup

```python
# test_stream_messages_handler.py
from __future__ import annotations

import pytest
from typing import Any, AsyncIterator, Iterator
from uuid import UUID
from unittest.mock import Mock
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.outputs import ChatGenerationChunk
from pytest_mock import MockerFixture

from your_module import StreamMessagesHandler

@pytest.fixture
def stream_mock() -> Mock:
    """Create a mock stream function."""
    return Mock()

@pytest.fixture
def handler(stream_mock: Mock) -> StreamMessagesHandler:
    """Create a StreamMessagesHandler instance with a mock stream."""
    return StreamMessagesHandler(stream=stream_mock)

@pytest.fixture
def sample_metadata() -> dict[str, Any]:
    """Create sample metadata for testing."""
    return {
        "langgraph_checkpoint_ns": "test.namespace",
        "langgraph_node": "test_node"
    }
```

### Basic Handler Tests

```python
def test_handler_initialization(handler: StreamMessagesHandler) -> None:
    """Test handler initialization."""
    assert handler.run_inline is True
    assert isinstance(handler.metadata, dict)
    assert isinstance(handler.seen, set)

@pytest.mark.asyncio
async def test_tap_output_aiter(handler: StreamMessagesHandler) -> None:
    """Test async iterator passthrough."""
    async def sample_aiter() -> AsyncIterator[int]:
        yield 1
        yield 2

    iterator = sample_aiter()
    result = handler.tap_output_aiter(UUID(int=0), iterator)
    assert result == iterator

def test_tap_output_iter(handler: StreamMessagesHandler) -> None:
    """Test sync iterator passthrough."""
    def sample_iter() -> Iterator[int]:
        yield 1
        yield 2

    iterator = sample_iter()
    result = handler.tap_output_iter(UUID(int=0), iterator)
    assert result == iterator
```

### Chat Model Event Tests

```python
def test_on_chat_model_start(
    handler: StreamMessagesHandler,
    sample_metadata: dict[str, Any]
) -> None:
    """Test chat model start event handling."""
    run_id = UUID(int=1)
    handler.on_chat_model_start(
        serialized={},
        messages=[],
        run_id=run_id,
        metadata=sample_metadata
    )

    assert run_id in handler.metadata
    assert handler.metadata[run_id][0] == ("test", "namespace")

def test_on_llm_new_token(
    handler: StreamMessagesHandler,
    stream_mock: Mock,
    sample_metadata: dict[str, Any]
) -> None:
    """Test LLM token streaming."""
    run_id = UUID(int=1)
    handler.metadata[run_id] = (("test", "namespace"), sample_metadata)

    message = AIMessage(content="Test content")
    chunk = ChatGenerationChunk(message=message)

    handler.on_llm_new_token(
        token="test",
        chunk=chunk,
        run_id=run_id
    )

    stream_mock.assert_called_once()
```

### Chain Event Tests

```python
def test_on_chain_end_single_message(
    handler: StreamMessagesHandler,
    stream_mock: Mock,
    sample_metadata: dict[str, Any]
) -> None:
    """Test chain end with single message response."""
    run_id = UUID(int=1)
    handler.metadata[run_id] = (("test", "namespace"), sample_metadata)

    message = HumanMessage(content="Test message")
    handler.on_chain_end(
        response=message,
        run_id=run_id
    )

    stream_mock.assert_called_once()

def test_on_chain_end_message_sequence(
    handler: StreamMessagesHandler,
    stream_mock: Mock,
    sample_metadata: dict[str, Any]
) -> None:
    """Test chain end with sequence of messages."""
    run_id = UUID(int=1)
    handler.metadata[run_id] = (("test", "namespace"), sample_metadata)

    messages = [
        HumanMessage(content="Message 1"),
        AIMessage(content="Message 2")
    ]

    handler.on_chain_end(
        response=messages,
        run_id=run_id
    )

    assert stream_mock.call_count == 2
```

### Error Handling Tests

```python
def test_on_chain_error(
    handler: StreamMessagesHandler,
    sample_metadata: dict[str, Any]
) -> None:
    """Test error handling in chain execution."""
    run_id = UUID(int=1)
    handler.metadata[run_id] = (("test", "namespace"), sample_metadata)

    handler.on_chain_error(
        error=ValueError("Test error"),
        run_id=run_id
    )

    assert run_id not in handler.metadata

def test_on_llm_error(
    handler: StreamMessagesHandler,
    sample_metadata: dict[str, Any]
) -> None:
    """Test error handling in LLM execution."""
    run_id = UUID(int=1)
    handler.metadata[run_id] = (("test", "namespace"), sample_metadata)

    handler.on_llm_error(
        error=ValueError("Test error"),
        run_id=run_id
    )

    assert run_id not in handler.metadata
```

## Testing Best Practices

1. Use proper type annotations
2. Implement fixtures for common test scenarios
3. Test both success and error paths
4. Verify metadata management
5. Test message deduplication
6. Verify stream callback behavior
7. Test different response types
8. Include edge cases
9. Test async and sync operations

## Common Edge Cases to Test

1. Empty messages
2. None values
3. Invalid metadata
4. Missing run IDs
5. Duplicate messages
6. Complex nested responses
7. Invalid message types
8. Concurrent operations

## Performance Considerations

1. Message deduplication overhead
2. Memory usage with large message sets
3. Stream callback performance
4. Metadata cleanup efficiency

Would you like me to provide more specific examples or elaborate on any particular aspect of the documentation?
