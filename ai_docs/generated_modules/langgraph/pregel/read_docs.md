
> [!NOTE]
> Documentation auto-generated by [ai-docs.](https://github.com/connor-john/ai-docs)

I'll create comprehensive documentation for this module, focusing on the PregelNode pattern and associated components.

# Pregel Node Module Documentation

## Module Overview

This module implements a node-based processing system using the Pregel pattern, primarily focused on managing state and data flow between distributed components. The core components include `ChannelRead` for state management and `PregelNode` for processing logic.

### Key Features
- Channel-based state management
- Configurable data processing nodes
- Retry policy support
- Flexible data mapping and transformation
- Async/sync operation support

### Dependencies
```toml
[dependencies]
langchain-core = ">=0.1.0"
typing-extensions = ">=4.5.0"
pytest = ">=7.0.0"
pytest-asyncio = ">=0.21.0"
```

## Installation and Setup

```bash
pip install langgraph
```

## Usage Guide

### Basic PregelNode Usage

```python
from langgraph.pregel import PregelNode
from langgraph.pregel.write import ChannelWrite

# Create a simple processing node
node = PregelNode(
    channels=["input_channel"],
    triggers=["trigger_channel"],
    bound=lambda x: x.upper(),
    writers=[ChannelWrite(writes=[("output_channel", lambda x: x)])]
)
```

### Channel Reading

```python
from langgraph.pregel import ChannelRead

# Create a channel reader
reader = ChannelRead(
    channel="my_channel",
    fresh=True,
    mapper=lambda x: x.strip()
)
```

## Testing Guide

### Test Setup

```python
# test_pregel.py
from __future__ import annotations

import pytest
from typing import Any, Dict
from pytest_mock import MockerFixture
from langgraph.pregel import PregelNode, ChannelRead
from langgraph.pregel.write import ChannelWrite

@pytest.fixture
def mock_config() -> Dict[str, Any]:
    """Provide mock configuration for testing.

    Returns:
        Dict[str, Any]: Mock configuration dictionary
    """
    return {
        "config": {
            "read": lambda channel, fresh: {"data": "test_value"}
        }
    }
```

### Basic PregelNode Tests

```python
def test_pregel_node_creation() -> None:
    """Test basic PregelNode initialization."""
    node = PregelNode(
        channels=["input"],
        triggers=["trigger"],
    )

    assert node.channels == ["input"]
    assert node.triggers == ["trigger"]
    assert node.bound is not None

@pytest.mark.asyncio
async def test_pregel_node_processing(
    mock_config: Dict[str, Any]
) -> None:
    """Test PregelNode processing functionality.

    Args:
        mock_config: Mock configuration fixture
    """
    node = PregelNode(
        channels=["input"],
        triggers=["trigger"],
        bound=lambda x: x.upper(),
        writers=[
            ChannelWrite(writes=[("output", lambda x: x)])
        ]
    )

    result = await node.ainvoke("test", config=mock_config)
    assert result == "TEST"
```

### ChannelRead Tests

```python
def test_channel_read(mock_config: Dict[str, Any]) -> None:
    """Test ChannelRead functionality.

    Args:
        mock_config: Mock configuration fixture
    """
    reader = ChannelRead(
        channel="test_channel",
        fresh=True
    )

    result = reader.invoke(None, config=mock_config)
    assert result == {"data": "test_value"}

@pytest.mark.asyncio
async def test_channel_read_async(
    mock_config: Dict[str, Any]
) -> None:
    """Test async ChannelRead functionality.

    Args:
        mock_config: Mock configuration fixture
    """
    reader = ChannelRead(
        channel="test_channel",
        fresh=True,
        mapper=lambda x: x["data"]
    )

    result = await reader.ainvoke(None, config=mock_config)
    assert result == "test_value"
```

### Testing Complex Scenarios

```python
@pytest.mark.asyncio
async def test_pregel_node_chain(
    mock_config: Dict[str, Any]
) -> None:
    """Test chaining multiple PregelNodes.

    Args:
        mock_config: Mock configuration fixture
    """
    node1 = PregelNode(
        channels=["input1"],
        triggers=["trigger1"],
        bound=lambda x: x.upper(),
    )

    node2 = PregelNode(
        channels=["input2"],
        triggers=["trigger2"],
        bound=lambda x: f"processed_{x}",
    )

    # Chain nodes using pipe
    combined = node1.pipe(node2)

    result = await combined.ainvoke("test", config=mock_config)
    assert result == "processed_TEST"
```

### Testing Error Handling

```python
def test_channel_read_error() -> None:
    """Test ChannelRead error handling."""
    reader = ChannelRead(channel="test")

    with pytest.raises(RuntimeError) as exc_info:
        reader.invoke(None, config={})

    assert "Not configured with a read function" in str(exc_info.value)

@pytest.mark.asyncio
async def test_pregel_node_retry(
    mocker: MockerFixture,
    mock_config: Dict[str, Any]
) -> None:
    """Test PregelNode retry policy.

    Args:
        mocker: Pytest mocker fixture
        mock_config: Mock configuration fixture
    """
    mock_function = mocker.Mock(side_effect=[ValueError, "SUCCESS"])

    node = PregelNode(
        channels=["input"],
        triggers=["trigger"],
        bound=mock_function,
        retry_policy=RetryPolicy(max_attempts=2)
    )

    result = await node.ainvoke("test", config=mock_config)
    assert result == "SUCCESS"
    assert mock_function.call_count == 2
```

## Performance Testing

```python
import time
import asyncio
from typing import List

async def test_pregel_node_performance() -> None:
    """Test PregelNode performance characteristics."""
    node = PregelNode(
        channels=["input"],
        triggers=["trigger"],
        bound=lambda x: x
    )

    start_time = time.time()
    results: List[str] = []

    # Test parallel processing
    tasks = [
        node.ainvoke(f"test_{i}", config={})
        for i in range(100)
    ]

    results = await asyncio.gather(*tasks)
    duration = time.time() - start_time

    assert len(results) == 100
    assert duration < 1.0  # Should complete within 1 second
```

## Integration Testing

```python
@pytest.mark.integration
async def test_pregel_integration(
    mock_database: Any,
    mock_api: Any
) -> None:
    """Test PregelNode integration with external services.

    Args:
        mock_database: Database fixture
        mock_api: API fixture
    """
    async def process_data(data: str) -> str:
        # Simulate external API call
        result = await mock_api.process(data)
        # Store in database
        await mock_database.store(result)
        return result

    node = PregelNode(
        channels=["input"],
        triggers=["trigger"],
        bound=process_data
    )

    result = await node.ainvoke("test_data", config={})

    # Verify database state
    stored_data = await mock_database.get_latest()
    assert stored_data == result
```

Remember to:
- Run tests with proper pytest configuration
- Use appropriate markers for different test types
- Implement proper cleanup in fixtures
- Monitor test coverage using pytest-cov
- Handle both sync and async test cases appropriately

This documentation provides a solid foundation for understanding and testing the PregelNode module. Adjust the examples and tests based on your specific use cases and requirements.
