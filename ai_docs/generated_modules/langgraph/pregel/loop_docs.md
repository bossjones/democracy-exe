
> [!NOTE]
> Documentation auto-generated by [ai-docs.](https://github.com/connor-john/ai-docs)

I'll create comprehensive documentation and test examples for this module. Let's break it down into sections:

# Module Overview

The module implements a Pregel-based graph computation model for managing asynchronous and synchronous graph processing workflows. It provides two main loop implementations:

- `SyncPregelLoop`: Synchronous implementation of the Pregel loop
- `AsyncPregelLoop`: Asynchronous implementation of the Pregel loop

Key features:
- Checkpoint management for graph state
- Stream-based output handling
- Task scheduling and execution
- Error handling and interruption support
- Managed value updates
- Debug output capabilities

# Installation & Setup

```python
pip install langgraph
```

Required dependencies:
```python
langchain-core>=0.0.1
typing-extensions>=4.5.0
```

# Usage Guide

Basic synchronous graph execution:

```python
from langgraph.pregel import SyncPregelLoop

# Configure graph nodes and channels
nodes = {
    "node1": PregelNode(...),
    "node2": PregelNode(...)
}

specs = {
    "channel1": BaseChannel(...),
    "channel2": ManagedValueSpec(...)
}

# Create loop instance
with SyncPregelLoop(
    input=initial_input,
    nodes=nodes,
    specs=specs,
    stream=output_stream,
    config=config,
    output_keys=["output1", "output2"],
    stream_keys=["stream1"]
) as loop:

    # Execute loop
    while loop.tick(input_keys=["input1"]):
        pass

    result = loop.output
```

# Testing Guide

Here are comprehensive test examples for the module:

```python
from typing import AsyncGenerator, Generator
import pytest
from pytest_mock import MockerFixture

# Test fixtures
@pytest.fixture
def mock_nodes() -> dict:
    """Provide mock graph nodes."""
    return {
        "node1": PregelNode(lambda x: x),
        "node2": PregelNode(lambda x: x * 2)
    }

@pytest.fixture
def mock_channels() -> dict:
    """Provide mock channels."""
    return {
        "channel1": BaseChannel(),
        "channel2": BaseChannel()
    }

@pytest.fixture
def mock_config() -> dict:
    """Provide mock configuration."""
    return {
        "recursion_limit": 10,
        "metadata": {},
        "conf": {}
    }

# Sync Tests
def test_sync_pregel_loop_basic(
    mock_nodes: dict,
    mock_channels: dict,
    mock_config: dict
) -> None:
    """Test basic synchronous loop execution."""
    with SyncPregelLoop(
        input={"test": 1},
        nodes=mock_nodes,
        specs=mock_channels,
        config=mock_config,
        output_keys=["output"]
    ) as loop:
        assert loop.tick(input_keys=["test"])
        assert loop.status == "pending"

@pytest.mark.asyncio
async def test_async_pregel_loop_basic(
    mock_nodes: dict,
    mock_channels: dict,
    mock_config: dict
) -> None:
    """Test basic asynchronous loop execution."""
    async with AsyncPregelLoop(
        input={"test": 1},
        nodes=mock_nodes,
        specs=mock_channels,
        config=mock_config,
        output_keys=["output"]
    ) as loop:
        assert loop.tick(input_keys=["test"])
        assert loop.status == "pending"

def test_checkpoint_handling(
    mocker: MockerFixture,
    mock_nodes: dict,
    mock_channels: dict,
    mock_config: dict
) -> None:
    """Test checkpoint save/restore functionality."""
    mock_checkpointer = mocker.Mock()
    mock_checkpointer.get_tuple.return_value = None

    with SyncPregelLoop(
        input={"test": 1},
        nodes=mock_nodes,
        specs=mock_channels,
        config=mock_config,
        checkpointer=mock_checkpointer,
        output_keys=["output"]
    ) as loop:
        loop.tick(input_keys=["test"])
        assert mock_checkpointer.put.called

def test_error_handling(
    mock_nodes: dict,
    mock_channels: dict,
    mock_config: dict
) -> None:
    """Test error handling during execution."""
    with pytest.raises(EmptyInputError):
        with SyncPregelLoop(
            input=None,
            nodes=mock_nodes,
            specs=mock_channels,
            config=mock_config,
            output_keys=["output"]
        ) as loop:
            loop.tick(input_keys=["test"])

@pytest.mark.asyncio
async def test_managed_value_updates(
    mocker: MockerFixture,
    mock_nodes: dict,
    mock_config: dict
) -> None:
    """Test managed value update handling."""
    mock_mv = mocker.Mock()
    mock_mv.aupdate = mocker.AsyncMock()

    specs = {"mv": mock_mv}

    async with AsyncPregelLoop(
        input={"test": 1},
        nodes=mock_nodes,
        specs=specs,
        config=mock_config,
        output_keys=["output"]
    ) as loop:
        loop._update_mv("mv", [1, 2, 3])
        await asyncio.sleep(0)
        mock_mv.aupdate.assert_called_with([1, 2, 3])

def test_stream_output(
    mocker: MockerFixture,
    mock_nodes: dict,
    mock_channels: dict,
    mock_config: dict
) -> None:
    """Test stream output handling."""
    mock_stream = mocker.Mock()

    with SyncPregelLoop(
        input={"test": 1},
        nodes=mock_nodes,
        specs=mock_channels,
        config=mock_config,
        stream=mock_stream,
        output_keys=["output"],
        stream_keys=["stream"]
    ) as loop:
        loop.tick(input_keys=["test"])
        assert mock_stream.called
```

# Important Test Fixtures

```python
@pytest.fixture
def mock_channel_manager() -> Generator[ChannelsManager, None, None]:
    """Provide a mock channel manager."""
    manager = ChannelsManager(
        specs={"test": BaseChannel()},
        checkpoint={"versions": {}},
        loop=None
    )
    yield manager
    manager.close()

@pytest.fixture
async def mock_async_channel_manager() -> AsyncGenerator[AsyncChannelsManager, None]:
    """Provide a mock async channel manager."""
    manager = AsyncChannelsManager(
        specs={"test": BaseChannel()},
        checkpoint={"versions": {}},
        loop=None
    )
    yield manager
    await manager.aclose()
```

# Testing Best Practices

1. Always test both sync and async implementations
2. Mock external dependencies (checkpointers, channels)
3. Test error conditions and edge cases
4. Verify checkpoint handling
5. Test stream output handling
6. Verify managed value updates
7. Test interruption handling
8. Check config propagation
9. Verify task execution order
10. Test debug output functionality

# Common Test Scenarios

1. Basic loop execution
2. Checkpoint save/restore
3. Error handling
4. Stream output
5. Managed value updates
6. Task scheduling
7. Interruption handling
8. Config inheritance
9. Debug output
10. Performance testing

Let me know if you would like me to add more examples or elaborate on any specific aspect of the documentation.
