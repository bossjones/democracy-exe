<documents>
<document index="1">
<source>/Users/malcolm/dev/bossjones/democracy-exe/tests/test_logsetup.py</source>
<document_content>
# pyright: reportUnusedFunction=false
# pyright: reportUndefinedVariable=false
# pyright: reportInvalidTypeForm=false


from __future__ import annotations

import datetime
import json
import logging
import multiprocessing
import sys

from collections.abc import Generator
from typing import TYPE_CHECKING, Any, Dict

import structlog

from structlog.testing import LogCapture

import pytest

from democracy_exe.bot_logger.logsetup import configure_logging, get_logger


if TYPE_CHECKING:
    from _pytest.logging import LogCaptureFixture


@pytest.fixture(autouse=True)
def clean_logging() -> Generator[None, None, None]:
    """Reset logging configuration before and after each test.

    Yields:
        None
    """
    # Reset before test
    logging.root.handlers = []
    structlog.reset_defaults()

    yield

    # Reset after test
    logging.root.handlers = []
    structlog.reset_defaults()


@pytest.mark.logsonly
def test_logger_initialization() -> None:
    """Test that the logger is properly initialized."""
    # Configure logging first
    configure_logging()

    # Get a logger instance
    logger = get_logger("test")

    # Verify logger is not None
    assert logger is not None

    # Log a test message and verify output
    with structlog.testing.capture_logs() as captured:
        test_message = "test message"
        logger.info(test_message)

        # Verify log output
        assert len(captured) == 1, "Expected exactly one log message"
        assert captured[0]["event"] == test_message, "Incorrect message content"

        # Verify logger is properly configured
        assert hasattr(logger, "info"), "Logger missing info method"
        assert hasattr(logger, "debug"), "Logger missing debug method"
        assert hasattr(logger, "warning"), "Logger missing warning method"
        assert hasattr(logger, "error"), "Logger missing error method"
        assert all(hasattr(logger, attr) for attr in ["info", "debug", "warning", "error"])


@pytest.mark.logsonly
@pytest.mark.parametrize("enable_json_logs", [True, False])
def test_configure_logging(enable_json_logs: bool) -> None:
    """Test the configure_logging function with different output formats.

    Args:
        enable_json_logs: Whether to enable JSON logging
    """
    configure_logging(enable_json_logs=enable_json_logs)
    logger = get_logger("test")

    with structlog.testing.capture_logs() as captured:
        test_message = "test message"
        logger.info(test_message)

        assert len(captured) == 1, "Expected exactly one log message"
        log_entry = captured[0]
        assert log_entry["event"] == test_message, "Incorrect message content"
        assert log_entry["log_level"] == "info", "Incorrect log level"


@pytest.mark.logsonly
@pytest.mark.asyncio
async def test_async_logging() -> None:
    """Test async logging capabilities."""
    configure_logging(enable_json_logs=False)
    logger = get_logger("test_async")

    test_message = "async test message"

    with structlog.testing.capture_logs() as captured:
        logger.info(test_message)  # Use regular info() since we're using BoundLogger

        assert len(captured) == 1, "Expected exactly one log message"
        assert captured[0]["event"] == test_message, "Incorrect message content"
        assert captured[0]["log_level"] == "info", "Incorrect log level"


@pytest.mark.logsonly
def test_logging_with_context() -> None:
    """Test logging with bound context information."""
    configure_logging(enable_json_logs=False)
    logger = get_logger("test_context")

    # Test context binding
    context = {"user": "test_user", "action": "test_action"}

    with structlog.testing.capture_logs() as captured:
        bound_logger = logger.bind(**context)
        test_message = "context test message"
        bound_logger.info(test_message)

        assert len(captured) == 1, "Expected exactly one log message"
        log_entry = captured[0]
        assert log_entry["event"] == test_message, "Incorrect message content"
        assert log_entry["user"] == context["user"], "Missing or incorrect user context"
        assert log_entry["action"] == context["action"], "Missing or incorrect action context"


@pytest.mark.logsonly
def test_process_info_logging() -> None:
    """Test that process information is correctly added to log entries."""
    # Test the processor directly first
    event_dict = _add_process_info("test_logger", "info", {"event": "main message"})

    # Verify processor adds the expected fields
    assert "process_name" in event_dict, "Processor failed to add process name"
    assert "process_id" in event_dict, "Processor failed to add process ID"
    assert isinstance(event_dict["process_id"], int), "Process ID should be an integer"

    # Now test through the logging system
    configure_logging(enable_json_logs=False)
    logger = get_logger("test_process")

    with structlog.testing.capture_logs() as captured:
        logger.info("main message")

        assert len(captured) == 1, "Expected exactly one log message"
        log_entry = captured[0]
        assert log_entry["event"] == "main message", "Incorrect message content"


@pytest.mark.logsonly
def test_third_party_logger_levels() -> None:
    """Test that third-party logger levels are properly configured."""
    custom_levels = {"discord": "ERROR", "aiohttp": "DEBUG"}

    configure_logging(third_party_loggers=custom_levels)

    # Check logger levels are set correctly
    discord_logger = logging.getLogger("discord")
    aiohttp_logger = logging.getLogger("aiohttp")

    assert discord_logger.level == logging.ERROR
    assert aiohttp_logger.level == logging.DEBUG


@pytest.mark.logsonly
def test_logging_to_correct_streams() -> None:
    """Test that logs go to the correct output streams."""
    configure_logging()
    logger = get_logger("test_streams")

    # Get handlers for different levels
    root_logger = logging.getLogger()
    handlers = root_logger.handlers

    # Verify we have both stdout and stderr handlers
    stdout_handlers = [h for h in handlers if getattr(h, "stream", None) is sys.stdout]
    stderr_handlers = [h for h in handlers if getattr(h, "stream", None) is sys.stderr]

    assert len(stdout_handlers) > 0, "No stdout handler found"
    assert len(stderr_handlers) > 0, "No stderr handler found"


@pytest.mark.logsonly
def test_module_name_processor() -> None:
    """Test that the module name processor adds correct module information.

    This test verifies both:
    1. Direct processor functionality
    2. Integration with the logging system
    """
    # Test the processor function directly
    test_event = {"event": "test message"}
    processed_event = _add_module_name("test_module", "info", test_event)

    assert "module" in processed_event, "Processor failed to add module field"
    assert processed_event["module"] == "test_module", "Processor added incorrect module name"
    assert processed_event["event"] == "test message", "Processor modified event message"

    # Test the processor integrated in the logging system
    configure_logging()
    logger = get_logger("test_module")

    with structlog.testing.capture_logs() as captured:
        test_message = "integration test message"
        logger.info(test_message)

        assert len(captured) == 1, "Expected exactly one log message"
        log_entry = captured[0]

        # Verify logger name is preserved
        assert log_entry["logger"] == "test_module", "Missing or incorrect logger name"

        # Verify module name is added
        assert log_entry["module"] == "test_module", "Missing or incorrect module name"

        # Verify original message is preserved
        assert log_entry["event"] == test_message, "Log message was modified"

</document_content>
</document>
<document index="2">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/bot_logger/logsetup.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
# pyright: reportUnusedFunction=false

from __future__ import annotations

import logging
import logging.config
import multiprocessing
import sys

from typing import Any, Dict, List, Optional, Union

import structlog

from structlog.contextvars import bind_contextvars, clear_contextvars, merge_contextvars, reset_contextvars
from structlog.stdlib import BoundLogger
from structlog.typing import Processor

from democracy_exe.aio_settings import aiosettings


# so we have logger names
structlog.stdlib.recreate_defaults()

logger: structlog.stdlib.BoundLogger = structlog.get_logger(__name__)


def get_log_level(level: str | int) -> int:
    """Convert a log level string to its corresponding logging level value.

    This function takes a string or integer log level and returns the corresponding
    logging level value. It supports both string names (e.g., 'INFO', 'DEBUG') and
    integer values.

    Args:
        level: The log level as a string (e.g., 'INFO', 'DEBUG') or integer

    Returns:
        int: The corresponding logging level value

    Raises:
        ValueError: If the log level string is not valid
        TypeError: If the level is not a string or integer
    """
    if isinstance(level, int):
        if level < 0:
            raise ValueError(f"Invalid level value, it should be a positive integer, not: {level}")
        return level

    if not isinstance(level, str):
        raise TypeError(
            f"Invalid level, it should be an integer or a string, not: '{type(level).__name__}'"
        )

    # Convert to upper case for case-insensitive comparison
    level_upper = level.upper()

    # Map of level names to level numbers
    level_map = {
        'CRITICAL': logging.CRITICAL,  # 50
        'FATAL': logging.FATAL,        # 50
        'ERROR': logging.ERROR,        # 40
        'WARNING': logging.WARNING,    # 30
        'WARN': logging.WARN,          # 30
        'INFO': logging.INFO,          # 20
        'DEBUG': logging.DEBUG,        # 10
        'NOTSET': logging.NOTSET      # 0
    }

    try:
        return level_map[level_upper]
    except KeyError:
        raise ValueError(f"Invalid log level: '{level}'") from None


def configure_logging(
    enable_json_logs: bool = False,
    log_level: str = "INFO",
    third_party_loggers: dict[str, str] | None = None,
    environment: str = "development",
) -> dict[str, Any]:
    """Configure structured logging with comprehensive async support and third-party integration.

    This function sets up a unified logging system following structlog best practices:
    1. Environment-aware configuration (development vs production)
    2. Proper async context handling with contextvars
    3. Performance-optimized processor chains
    4. Comprehensive error and exception handling
    5. Integration with standard library logging
    6. Support for both development (pretty) and production (JSON) output
    7. Proper timestamp handling in UTC
    8. Canonical log lines support through bound loggers

    Args:
        enable_json_logs: If True, outputs logs in JSON format. Otherwise, uses colored console output.
        log_level: The minimum log level to capture.
        third_party_loggers: Dict of logger names and their minimum levels to configure.
        environment: The runtime environment ("development", "production", "testing")

    Returns:
        dict[str, Any]: The current structlog configuration

    Example:
        >>> configure_logging(
        ...     enable_json_logs=True,
        ...     log_level="INFO",
        ...     environment="production"
        ... )
    """
    # Default third-party logger configuration
    default_third_party = {
        "discord": "WARNING",
        "discord.client": "WARNING",
        "discord.gateway": "WARNING",
        "aiohttp": "WARNING",
        "asyncio": "WARNING",
        "urllib3": "WARNING",
        "requests": "WARNING",
        "PIL": "WARNING",
    }

    if third_party_loggers:
        default_third_party.update(third_party_loggers)

    # Clear any existing context for clean configuration
    clear_contextvars()

    def get_processor_chain(enable_dev_processors: bool = False) -> list[Processor]:
        """Build the processor chain based on environment needs.

        Follows structlog's best practices for processor ordering and optimization.
        See: https://www.structlog.org/en/stable/logging-best-practices.html

        Args:
            enable_dev_processors: Whether to include development-specific processors

        Returns:
            List of configured processors
        """
        # Core processors that are always needed
        processors: list[Processor] = [
            # Context management
            structlog.contextvars.merge_contextvars,

            # Add basic event metadata
            structlog.processors.add_log_level,
            structlog.stdlib.add_logger_name,

            # Format any positional arguments
            structlog.stdlib.PositionalArgumentsFormatter(),

            # Add timestamps in UTC
            structlog.processors.TimeStamper(fmt="iso", utc=True),

            # Add stack information for errors
            structlog.processors.StackInfoRenderer(),

            # Handle exceptions
            structlog.processors.format_exc_info,

            # Decode any bytes to strings
            structlog.processors.UnicodeDecoder(),
        ]

        # Development-specific processors
        if enable_dev_processors and environment == "development":
            processors.append(
                structlog.processors.CallsiteParameterAdder(
                    {
                        structlog.processors.CallsiteParameter.PATHNAME,
                        structlog.processors.CallsiteParameter.FILENAME,
                        structlog.processors.CallsiteParameter.MODULE,
                        structlog.processors.CallsiteParameter.FUNC_NAME,
                        structlog.processors.CallsiteParameter.LINENO,
                        structlog.processors.CallsiteParameter.THREAD,
                        structlog.processors.CallsiteParameter.THREAD_NAME,
                        structlog.processors.CallsiteParameter.PROCESS,
                        structlog.processors.CallsiteParameter.PROCESS_NAME,
                    }
                )
            )

        return processors

    # Get shared processors
    shared_processors = get_processor_chain(not enable_json_logs)

    def get_renderer(enable_json_logs: bool) -> Processor:
        """Get the appropriate renderer based on environment.

        Args:
            enable_json_logs: Whether to use JSON logging

        Returns:
            Configured renderer processor
        """
        if enable_json_logs or environment == "production":
            return structlog.processors.JSONRenderer(
                sort_keys=False,
                serializer=lambda obj: str(obj)  # Simple serializer that handles non-JSON types
            )

        return structlog.dev.ConsoleRenderer(
            colors=True,
            sort_keys=True,
        )

    # Get the appropriate renderer
    renderer = get_renderer(enable_json_logs)
    if enable_json_logs:
        shared_processors.append(structlog.processors.format_exc_info)

    # Configure structlog with performance optimizations
    structlog.configure(
        processors=shared_processors + [
            structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.make_filtering_bound_logger(get_log_level(log_level)),
        cache_logger_on_first_use=True,  # Performance optimization
    )

    return structlog.get_config()


def get_logger(name: str) -> BoundLogger:
    """Get a configured logger instance.

    This function returns a properly configured structlog logger instance.
    It's safe to call this at module level since it returns a lazy proxy
    that gets properly configured on first use.

    Args:
        name: The name for the logger, typically __name__

    Returns:
        A configured structlog logger instance

    Example:
        >>> logger = get_logger(__name__)
        >>> logger.info("message", key="value")
    """
    return structlog.get_logger(name)


def reset_logging() -> None:
    """Reset logging configuration to default state.

    This is particularly useful in tests to ensure a clean slate between test cases.
    It:
    1. Clears all context variables
    2. Resets structlog configuration
    3. Cleans up any bound loggers

    Example:
        >>> def test_logging():
        ...     reset_logging()
        ...     configure_logging(environment="testing")
        ...     # Run test...
    """
    clear_contextvars()
    structlog.reset_defaults()
    # Recreate stdlib defaults for logger names
    structlog.stdlib.recreate_defaults()


def configure_test_logging() -> dict[str, Any]:
    """Configure logging specifically for testing.

    Sets up a test-optimized logging configuration that:
    1. Uses a capturing logger for test verification
    2. Disables caching for test isolation
    3. Enables all processors for thorough testing
    4. Sets DEBUG level for maximum visibility

    Returns:
        dict[str, Any]: The test logging configuration

    Example:
        >>> def test_feature():
        ...     configure_test_logging()
        ...     logger = get_logger(__name__)
        ...     # Run test...
    """
    return configure_logging(
        enable_json_logs=True,
        log_level="DEBUG",
        environment="testing"
    )


if __name__ == "__main__":
    import rich
    # Example usage and testing
    logger = structlog.get_logger(__name__)
    configure_logging(enable_json_logs=False, log_level="DEBUG")
    logger = structlog.get_logger(__name__)
    rich.print(structlog.get_config())

    import bpdb
    bpdb.set_trace()

    def worker_process(name: str) -> None:
        """Example worker process function.

        Args:
            name: Name of the worker
        """
        proc_logger = get_logger(f"worker.{name}")

        # Clear context for this process
        clear_contextvars()

        # Bind process-specific context
        bind_contextvars(
            worker_name=name,
            worker_type="example"
        )

        proc_logger.info(f"Worker {name} starting")

        try:
            # Simulate some work
            1/0
        except Exception as e:
            proc_logger.error("Worker error", exc_info=e)

        proc_logger.info(f"Worker {name} finished")

    # Test in main process
    logger = get_logger("test_logger")
    clear_contextvars()

    # Bind some context variables that will be included in all log entries
    bind_contextvars(
        app_version="1.0.0",
        environment="development"
    )

    # Start some worker processes
    processes = []
    for i in range(3):
        p = multiprocessing.Process(
            target=worker_process,
            args=(f"worker_{i}",)
        )
        processes.append(p)
        p.start()

    # Test structured logging while workers run
    logger.info("Main process running",
                num_workers=len(processes))

    # Wait for all processes
    for p in processes:
        p.join()

    logger.info("All workers finished")

    # Clear context at the end
    clear_contextvars()

</document_content>
</document>
</documents>
