<documents>
<document index="1">
<source>/Users/malcolm/dev/home-assistant/core/homeassistant/__main__.py</source>
<document_content>
"""Start Home Assistant."""

from __future__ import annotations

import argparse
from contextlib import suppress
import faulthandler
import os
import sys
import threading

from .backup_restore import restore_backup
from .const import REQUIRED_PYTHON_VER, RESTART_EXIT_CODE, __version__

FAULT_LOG_FILENAME = "home-assistant.log.fault"


def validate_os() -> None:
    """Validate that Home Assistant is running in a supported operating system."""
    if not sys.platform.startswith(("darwin", "linux")):
        print(
            "Home Assistant only supports Linux, OSX and Windows using WSL",
            file=sys.stderr,
        )
        sys.exit(1)


def validate_python() -> None:
    """Validate that the right Python version is running."""
    if sys.version_info[:3] < REQUIRED_PYTHON_VER:
        print(
            "Home Assistant requires at least Python "
            f"{REQUIRED_PYTHON_VER[0]}.{REQUIRED_PYTHON_VER[1]}.{REQUIRED_PYTHON_VER[2]}",
            file=sys.stderr,
        )
        sys.exit(1)


def ensure_config_path(config_dir: str) -> None:
    """Validate the configuration directory."""
    # pylint: disable-next=import-outside-toplevel
    from . import config as config_util

    lib_dir = os.path.join(config_dir, "deps")

    # Test if configuration directory exists
    if not os.path.isdir(config_dir):
        if config_dir != config_util.get_default_config_dir():
            if os.path.exists(config_dir):
                reason = "is not a directory"
            else:
                reason = "does not exist"
            print(
                f"Fatal Error: Specified configuration directory {config_dir} {reason}",
                file=sys.stderr,
            )
            sys.exit(1)

        try:
            os.mkdir(config_dir)
        except OSError as ex:
            print(
                "Fatal Error: Unable to create default configuration "
                f"directory {config_dir}: {ex}",
                file=sys.stderr,
            )
            sys.exit(1)

    # Test if library directory exists
    if not os.path.isdir(lib_dir):
        try:
            os.mkdir(lib_dir)
        except OSError as ex:
            print(
                f"Fatal Error: Unable to create library directory {lib_dir}: {ex}",
                file=sys.stderr,
            )
            sys.exit(1)


def get_arguments() -> argparse.Namespace:
    """Get parsed passed in arguments."""
    # pylint: disable-next=import-outside-toplevel
    from . import config as config_util

    parser = argparse.ArgumentParser(
        description="Home Assistant: Observe, Control, Automate.",
        epilog=f"If restart is requested, exits with code {RESTART_EXIT_CODE}",
    )
    parser.add_argument("--version", action="version", version=__version__)
    parser.add_argument(
        "-c",
        "--config",
        metavar="path_to_config_dir",
        default=config_util.get_default_config_dir(),
        help="Directory that contains the Home Assistant configuration",
    )
    parser.add_argument(
        "--recovery-mode",
        action="store_true",
        help="Start Home Assistant in recovery mode",
    )
    parser.add_argument(
        "--debug", action="store_true", help="Start Home Assistant in debug mode"
    )
    parser.add_argument(
        "--open-ui", action="store_true", help="Open the webinterface in a browser"
    )

    skip_pip_group = parser.add_mutually_exclusive_group()
    skip_pip_group.add_argument(
        "--skip-pip",
        action="store_true",
        help="Skips pip install of required packages on startup",
    )
    skip_pip_group.add_argument(
        "--skip-pip-packages",
        metavar="package_names",
        type=lambda arg: arg.split(","),
        default=[],
        help="Skip pip install of specific packages on startup",
    )

    parser.add_argument(
        "-v", "--verbose", action="store_true", help="Enable verbose logging to file."
    )
    parser.add_argument(
        "--log-rotate-days",
        type=int,
        default=None,
        help="Enables daily log rotation and keeps up to the specified days",
    )
    parser.add_argument(
        "--log-file",
        type=str,
        default=None,
        help="Log file to write to.  If not set, CONFIG/home-assistant.log is used",
    )
    parser.add_argument(
        "--log-no-color", action="store_true", help="Disable color logs"
    )
    parser.add_argument(
        "--script", nargs=argparse.REMAINDER, help="Run one of the embedded scripts"
    )
    parser.add_argument(
        "--ignore-os-check",
        action="store_true",
        help="Skips validation of operating system",
    )

    return parser.parse_args()


def check_threads() -> None:
    """Check if there are any lingering threads."""
    try:
        nthreads = sum(
            thread.is_alive() and not thread.daemon for thread in threading.enumerate()
        )
        if nthreads > 1:
            sys.stderr.write(f"Found {nthreads} non-daemonic threads.\n")

    # Somehow we sometimes seem to trigger an assertion in the python threading
    # module. It seems we find threads that have no associated OS level thread
    # which are not marked as stopped at the python level.
    except AssertionError:
        sys.stderr.write("Failed to count non-daemonic threads.\n")


def main() -> int:
    """Start Home Assistant."""
    validate_python()

    args = get_arguments()

    if not args.ignore_os_check:
        validate_os()

    if args.script is not None:
        # pylint: disable-next=import-outside-toplevel
        from . import scripts

        return scripts.run(args.script)

    config_dir = os.path.abspath(os.path.join(os.getcwd(), args.config))
    if restore_backup(config_dir):
        return RESTART_EXIT_CODE

    ensure_config_path(config_dir)

    # pylint: disable-next=import-outside-toplevel
    from . import config, runner

    safe_mode = config.safe_mode_enabled(config_dir)

    runtime_conf = runner.RuntimeConfig(
        config_dir=config_dir,
        verbose=args.verbose,
        log_rotate_days=args.log_rotate_days,
        log_file=args.log_file,
        log_no_color=args.log_no_color,
        skip_pip=args.skip_pip,
        skip_pip_packages=args.skip_pip_packages,
        recovery_mode=args.recovery_mode,
        debug=args.debug,
        open_ui=args.open_ui,
        safe_mode=safe_mode,
    )

    fault_file_name = os.path.join(config_dir, FAULT_LOG_FILENAME)
    with open(fault_file_name, mode="a", encoding="utf8") as fault_file:
        faulthandler.enable(fault_file)
        exit_code = runner.run(runtime_conf)
        faulthandler.disable()

    # It's possible for the fault file to disappear, so suppress obvious errors
    with suppress(FileNotFoundError):
        if os.path.getsize(fault_file_name) == 0:
            os.remove(fault_file_name)

    check_threads()

    return exit_code


if __name__ == "__main__":
    sys.exit(main())

</document_content>
</document>
<document index="2">
<source>/Users/malcolm/dev/home-assistant/core/homeassistant/components/profiler/__init__.py</source>
<document_content>
"""The profiler integration."""

import asyncio
from collections.abc import Generator
import contextlib
from contextlib import suppress
from datetime import timedelta
from functools import _lru_cache_wrapper
import logging
import reprlib
import sys
import threading
import time
import traceback
from typing import Any, cast

from lru import LRU
import voluptuous as vol

from homeassistant.components import persistent_notification
from homeassistant.config_entries import ConfigEntry
from homeassistant.const import CONF_SCAN_INTERVAL, CONF_TYPE
from homeassistant.core import HomeAssistant, ServiceCall, callback
from homeassistant.exceptions import HomeAssistantError
import homeassistant.helpers.config_validation as cv
from homeassistant.helpers.event import async_track_time_interval
from homeassistant.helpers.service import async_register_admin_service

from .const import DOMAIN

SERVICE_START = "start"
SERVICE_MEMORY = "memory"
SERVICE_START_LOG_OBJECTS = "start_log_objects"
SERVICE_STOP_LOG_OBJECTS = "stop_log_objects"
SERVICE_START_LOG_OBJECT_SOURCES = "start_log_object_sources"
SERVICE_STOP_LOG_OBJECT_SOURCES = "stop_log_object_sources"
SERVICE_DUMP_LOG_OBJECTS = "dump_log_objects"
SERVICE_LRU_STATS = "lru_stats"
SERVICE_LOG_THREAD_FRAMES = "log_thread_frames"
SERVICE_LOG_EVENT_LOOP_SCHEDULED = "log_event_loop_scheduled"
SERVICE_SET_ASYNCIO_DEBUG = "set_asyncio_debug"
SERVICE_LOG_CURRENT_TASKS = "log_current_tasks"

_LRU_CACHE_WRAPPER_OBJECT = _lru_cache_wrapper.__name__
_SQLALCHEMY_LRU_OBJECT = "LRUCache"

_KNOWN_LRU_CLASSES = (
    "EventDataManager",
    "EventTypeManager",
    "StatesMetaManager",
    "StateAttributesManager",
    "StatisticsMetaManager",
)

SERVICES = (
    SERVICE_START,
    SERVICE_MEMORY,
    SERVICE_START_LOG_OBJECTS,
    SERVICE_STOP_LOG_OBJECTS,
    SERVICE_DUMP_LOG_OBJECTS,
    SERVICE_LRU_STATS,
    SERVICE_LOG_THREAD_FRAMES,
    SERVICE_LOG_EVENT_LOOP_SCHEDULED,
    SERVICE_SET_ASYNCIO_DEBUG,
    SERVICE_LOG_CURRENT_TASKS,
)

DEFAULT_SCAN_INTERVAL = timedelta(seconds=30)

DEFAULT_MAX_OBJECTS = 5

CONF_ENABLED = "enabled"
CONF_SECONDS = "seconds"
CONF_MAX_OBJECTS = "max_objects"

LOG_INTERVAL_SUB = "log_interval_subscription"


_LOGGER = logging.getLogger(__name__)


async def async_setup_entry(  # noqa: C901
    hass: HomeAssistant, entry: ConfigEntry
) -> bool:
    """Set up Profiler from a config entry."""
    lock = asyncio.Lock()
    domain_data = hass.data[DOMAIN] = {}

    async def _async_run_profile(call: ServiceCall) -> None:
        async with lock:
            await _async_generate_profile(hass, call)

    async def _async_run_memory_profile(call: ServiceCall) -> None:
        async with lock:
            await _async_generate_memory_profile(hass, call)

    async def _async_start_log_objects(call: ServiceCall) -> None:
        if LOG_INTERVAL_SUB in domain_data:
            raise HomeAssistantError("Object logging already started")

        persistent_notification.async_create(
            hass,
            (
                "Object growth logging has started. See [the logs](/config/logs) to"
                " track the growth of new objects."
            ),
            title="Object growth logging started",
            notification_id="profile_object_logging",
        )
        await hass.async_add_executor_job(_log_objects)
        domain_data[LOG_INTERVAL_SUB] = async_track_time_interval(
            hass, _log_objects, call.data[CONF_SCAN_INTERVAL]
        )

    async def _async_stop_log_objects(call: ServiceCall) -> None:
        if LOG_INTERVAL_SUB not in domain_data:
            raise HomeAssistantError("Object logging not running")

        persistent_notification.async_dismiss(hass, "profile_object_logging")
        domain_data.pop(LOG_INTERVAL_SUB)()

    async def _async_start_object_sources(call: ServiceCall) -> None:
        if LOG_INTERVAL_SUB in domain_data:
            raise HomeAssistantError("Object logging already started")

        persistent_notification.async_create(
            hass,
            (
                "Object source logging has started. See [the logs](/config/logs) to"
                " track the growth of new objects."
            ),
            title="Object source logging started",
            notification_id="profile_object_source_logging",
        )

        last_ids: set[int] = set()
        last_stats: dict[str, int] = {}

        async def _log_object_sources_with_max(*_: Any) -> None:
            await hass.async_add_executor_job(
                _log_object_sources, call.data[CONF_MAX_OBJECTS], last_ids, last_stats
            )

        await _log_object_sources_with_max()
        cancel_track = async_track_time_interval(
            hass, _log_object_sources_with_max, call.data[CONF_SCAN_INTERVAL]
        )

        @callback
        def _cancel():
            cancel_track()
            last_ids.clear()
            last_stats.clear()

        domain_data[LOG_INTERVAL_SUB] = _cancel

    @callback
    def _async_stop_object_sources(call: ServiceCall) -> None:
        if LOG_INTERVAL_SUB not in domain_data:
            raise HomeAssistantError("Object logging not running")

        persistent_notification.async_dismiss(hass, "profile_object_source_logging")
        domain_data.pop(LOG_INTERVAL_SUB)()

    def _dump_log_objects(call: ServiceCall) -> None:
        # Imports deferred to avoid loading modules
        # in memory since usually only one part of this
        # integration is used at a time
        import objgraph  # pylint: disable=import-outside-toplevel

        obj_type = call.data[CONF_TYPE]

        for obj in objgraph.by_type(obj_type):
            _LOGGER.critical(
                "%s object in memory: %s",
                obj_type,
                _safe_repr(obj),
            )

        persistent_notification.create(
            hass,
            (
                f"Objects with type {obj_type} have been dumped to the log. See [the"
                " logs](/config/logs) to review the repr of the objects."
            ),
            title="Object dump completed",
            notification_id="profile_object_dump",
        )

    def _lru_stats(call: ServiceCall) -> None:
        """Log the stats of all lru caches."""
        # Imports deferred to avoid loading modules
        # in memory since usually only one part of this
        # integration is used at a time
        import objgraph  # pylint: disable=import-outside-toplevel

        for lru in objgraph.by_type(_LRU_CACHE_WRAPPER_OBJECT):
            lru = cast(_lru_cache_wrapper, lru)
            _LOGGER.critical(
                "Cache stats for lru_cache %s at %s: %s",
                lru.__wrapped__,
                _get_function_absfile(lru.__wrapped__) or "unknown",
                lru.cache_info(),
            )

        for _class in _KNOWN_LRU_CLASSES:
            for class_with_lru_attr in objgraph.by_type(_class):
                for maybe_lru in class_with_lru_attr.__dict__.values():
                    if isinstance(maybe_lru, LRU):
                        _LOGGER.critical(
                            "Cache stats for LRU %s at %s: %s",
                            type(class_with_lru_attr),
                            _get_function_absfile(class_with_lru_attr) or "unknown",
                            maybe_lru.get_stats(),
                        )

        for lru in objgraph.by_type(_SQLALCHEMY_LRU_OBJECT):
            if (data := getattr(lru, "_data", None)) and isinstance(data, dict):
                for key, value in dict(data).items():
                    _LOGGER.critical(
                        "Cache data for sqlalchemy LRUCache %s: %s: %s", lru, key, value
                    )

        persistent_notification.create(
            hass,
            (
                "LRU cache states have been dumped to the log. See [the"
                " logs](/config/logs) to review the stats."
            ),
            title="LRU stats completed",
            notification_id="profile_lru_stats",
        )

    async def _async_dump_thread_frames(call: ServiceCall) -> None:
        """Log all thread frames."""
        frames = sys._current_frames()  # noqa: SLF001
        main_thread = threading.main_thread()
        for thread in threading.enumerate():
            if thread == main_thread:
                continue
            ident = cast(int, thread.ident)
            _LOGGER.critical(
                "Thread [%s]: %s",
                thread.name,
                "".join(traceback.format_stack(frames.get(ident))).strip(),
            )

    async def _async_dump_current_tasks(call: ServiceCall) -> None:
        """Log all current tasks in the event loop."""
        with _increase_repr_limit():
            for task in asyncio.all_tasks():
                if not task.cancelled():
                    _LOGGER.critical("Task: %s", _safe_repr(task))

    async def _async_dump_scheduled(call: ServiceCall) -> None:
        """Log all scheduled in the event loop."""
        with _increase_repr_limit():
            handle: asyncio.Handle
            for handle in getattr(hass.loop, "_scheduled"):
                if not handle.cancelled():
                    _LOGGER.critical("Scheduled: %s", handle)

    async def _async_asyncio_debug(call: ServiceCall) -> None:
        """Enable or disable asyncio debug."""
        enabled = call.data[CONF_ENABLED]
        # Always log this at critical level so we know when
        # it's been changed when reviewing logs
        _LOGGER.critical("Setting asyncio debug to %s", enabled)
        # Make sure the logger is set to at least INFO or
        # we won't see the messages
        base_logger = logging.getLogger()
        if enabled and base_logger.getEffectiveLevel() > logging.INFO:
            base_logger.setLevel(logging.INFO)
        hass.loop.set_debug(enabled)

    async_register_admin_service(
        hass,
        DOMAIN,
        SERVICE_START,
        _async_run_profile,
        schema=vol.Schema(
            {vol.Optional(CONF_SECONDS, default=60.0): vol.Coerce(float)}
        ),
    )

    async_register_admin_service(
        hass,
        DOMAIN,
        SERVICE_MEMORY,
        _async_run_memory_profile,
        schema=vol.Schema(
            {vol.Optional(CONF_SECONDS, default=60.0): vol.Coerce(float)}
        ),
    )

    async_register_admin_service(
        hass,
        DOMAIN,
        SERVICE_START_LOG_OBJECTS,
        _async_start_log_objects,
        schema=vol.Schema(
            {
                vol.Optional(
                    CONF_SCAN_INTERVAL, default=DEFAULT_SCAN_INTERVAL
                ): cv.time_period
            }
        ),
    )

    async_register_admin_service(
        hass,
        DOMAIN,
        SERVICE_STOP_LOG_OBJECTS,
        _async_stop_log_objects,
    )

    async_register_admin_service(
        hass,
        DOMAIN,
        SERVICE_START_LOG_OBJECT_SOURCES,
        _async_start_object_sources,
        schema=vol.Schema(
            {
                vol.Optional(
                    CONF_SCAN_INTERVAL, default=DEFAULT_SCAN_INTERVAL
                ): cv.time_period,
                vol.Optional(CONF_MAX_OBJECTS, default=DEFAULT_MAX_OBJECTS): vol.Range(
                    min=1, max=1024
                ),
            }
        ),
    )

    async_register_admin_service(
        hass,
        DOMAIN,
        SERVICE_STOP_LOG_OBJECT_SOURCES,
        _async_stop_object_sources,
    )

    async_register_admin_service(
        hass,
        DOMAIN,
        SERVICE_DUMP_LOG_OBJECTS,
        _dump_log_objects,
        schema=vol.Schema({vol.Required(CONF_TYPE): str}),
    )

    async_register_admin_service(
        hass,
        DOMAIN,
        SERVICE_LRU_STATS,
        _lru_stats,
    )

    async_register_admin_service(
        hass,
        DOMAIN,
        SERVICE_LOG_THREAD_FRAMES,
        _async_dump_thread_frames,
    )

    async_register_admin_service(
        hass,
        DOMAIN,
        SERVICE_LOG_EVENT_LOOP_SCHEDULED,
        _async_dump_scheduled,
    )

    async_register_admin_service(
        hass,
        DOMAIN,
        SERVICE_SET_ASYNCIO_DEBUG,
        _async_asyncio_debug,
        schema=vol.Schema({vol.Optional(CONF_ENABLED, default=True): cv.boolean}),
    )

    async_register_admin_service(
        hass,
        DOMAIN,
        SERVICE_LOG_CURRENT_TASKS,
        _async_dump_current_tasks,
    )

    return True


async def async_unload_entry(hass: HomeAssistant, entry: ConfigEntry) -> bool:
    """Unload a config entry."""
    for service in SERVICES:
        hass.services.async_remove(domain=DOMAIN, service=service)
    if LOG_INTERVAL_SUB in hass.data[DOMAIN]:
        hass.data[DOMAIN][LOG_INTERVAL_SUB]()
    hass.data.pop(DOMAIN)
    return True


async def _async_generate_profile(hass: HomeAssistant, call: ServiceCall):
    # Imports deferred to avoid loading modules
    # in memory since usually only one part of this
    # integration is used at a time
    import cProfile  # pylint: disable=import-outside-toplevel

    start_time = int(time.time() * 1000000)
    persistent_notification.async_create(
        hass,
        (
            "The profile has started. This notification will be updated when it is"
            " complete."
        ),
        title="Profile Started",
        notification_id=f"profiler_{start_time}",
    )
    profiler = cProfile.Profile()
    profiler.enable()
    await asyncio.sleep(float(call.data[CONF_SECONDS]))
    profiler.disable()

    cprofile_path = hass.config.path(f"profile.{start_time}.cprof")
    callgrind_path = hass.config.path(f"callgrind.out.{start_time}")
    await hass.async_add_executor_job(
        _write_profile, profiler, cprofile_path, callgrind_path
    )
    persistent_notification.async_create(
        hass,
        (
            f"Wrote cProfile data to {cprofile_path} and callgrind data to"
            f" {callgrind_path}"
        ),
        title="Profile Complete",
        notification_id=f"profiler_{start_time}",
    )


async def _async_generate_memory_profile(hass: HomeAssistant, call: ServiceCall):
    # Imports deferred to avoid loading modules
    # in memory since usually only one part of this
    # integration is used at a time
    from guppy import hpy  # pylint: disable=import-outside-toplevel

    start_time = int(time.time() * 1000000)
    persistent_notification.async_create(
        hass,
        (
            "The memory profile has started. This notification will be updated when it"
            " is complete."
        ),
        title="Profile Started",
        notification_id=f"memory_profiler_{start_time}",
    )
    heap_profiler = hpy()
    heap_profiler.setref()
    await asyncio.sleep(float(call.data[CONF_SECONDS]))
    heap = heap_profiler.heap()

    heap_path = hass.config.path(f"heap_profile.{start_time}.hpy")
    await hass.async_add_executor_job(_write_memory_profile, heap, heap_path)
    persistent_notification.async_create(
        hass,
        f"Wrote heapy memory profile to {heap_path}",
        title="Profile Complete",
        notification_id=f"memory_profiler_{start_time}",
    )


def _write_profile(profiler, cprofile_path, callgrind_path):
    # Imports deferred to avoid loading modules
    # in memory since usually only one part of this
    # integration is used at a time
    from pyprof2calltree import convert  # pylint: disable=import-outside-toplevel

    profiler.create_stats()
    profiler.dump_stats(cprofile_path)
    convert(profiler.getstats(), callgrind_path)


def _write_memory_profile(heap, heap_path):
    heap.byrcs.dump(heap_path)


def _log_objects(*_):
    # Imports deferred to avoid loading modules
    # in memory since usually only one part of this
    # integration is used at a time
    import objgraph  # pylint: disable=import-outside-toplevel

    _LOGGER.critical("Memory Growth: %s", objgraph.growth(limit=1000))


def _get_function_absfile(func: Any) -> str | None:
    """Get the absolute file path of a function."""
    import inspect  # pylint: disable=import-outside-toplevel

    abs_file: str | None = None
    with suppress(Exception):
        abs_file = inspect.getabsfile(func)
    return abs_file


def _safe_repr(obj: Any) -> str:
    """Get the repr of an object but keep going if there is an exception.

    We wrap repr to ensure if one object cannot be serialized, we can
    still get the rest.
    """
    try:
        return repr(obj)
    except Exception:  # noqa: BLE001
        return f"Failed to serialize {type(obj)}"


def _find_backrefs_not_to_self(_object: Any) -> list[str]:
    import objgraph  # pylint: disable=import-outside-toplevel

    return [
        _safe_repr(backref)
        for backref in objgraph.find_backref_chain(
            _object, lambda obj: obj is not _object
        )
    ]


def _log_object_sources(
    max_objects: int, last_ids: set[int], last_stats: dict[str, int]
) -> None:
    # Imports deferred to avoid loading modules
    # in memory since usually only one part of this
    # integration is used at a time
    import gc  # pylint: disable=import-outside-toplevel

    gc.collect()

    objects = gc.get_objects()
    new_objects: list[object] = []
    new_objects_overflow: dict[str, int] = {}
    current_ids = set()
    new_stats: dict[str, int] = {}
    had_new_object_growth = False
    try:
        for _object in objects:
            object_type = type(_object).__name__
            new_stats[object_type] = new_stats.get(object_type, 0) + 1

        for _object in objects:
            id_ = id(_object)
            current_ids.add(id_)
            if id_ in last_ids:
                continue
            object_type = type(_object).__name__
            if last_stats.get(object_type, 0) < new_stats[object_type]:
                if len(new_objects) < max_objects:
                    new_objects.append(_object)
                else:
                    new_objects_overflow.setdefault(object_type, 0)
                    new_objects_overflow[object_type] += 1

        for _object in new_objects:
            had_new_object_growth = True
            object_type = type(_object).__name__
            _LOGGER.critical(
                "New object %s (%s/%s) at %s: %s",
                object_type,
                last_stats.get(object_type, 0),
                new_stats[object_type],
                _get_function_absfile(_object) or _find_backrefs_not_to_self(_object),
                _safe_repr(_object),
            )

        for object_type, count in last_stats.items():
            new_stats[object_type] = max(new_stats.get(object_type, 0), count)
    finally:
        # Break reference cycles
        del objects
        del new_objects
        last_ids.clear()
        last_ids.update(current_ids)
        last_stats.clear()
        last_stats.update(new_stats)
        del new_stats
        del current_ids

    if new_objects_overflow:
        _LOGGER.critical("New objects overflowed by %s", new_objects_overflow)
    elif not had_new_object_growth:
        _LOGGER.critical("No new object growth found")


@contextlib.contextmanager
def _increase_repr_limit() -> Generator[None]:
    """Increase the repr limit."""
    arepr = reprlib.aRepr
    original_maxstring = arepr.maxstring
    original_maxother = arepr.maxother
    arepr.maxstring = 300
    arepr.maxother = 300
    try:
        yield
    finally:
        arepr.maxstring = original_maxstring
        arepr.maxother = original_maxother

</document_content>
</document>
<document index="3">
<source>/Users/malcolm/dev/home-assistant/core/homeassistant/components/profiler/config_flow.py</source>
<document_content>
"""Config flow for Profiler integration."""

from typing import Any

from homeassistant.config_entries import ConfigFlow, ConfigFlowResult

from .const import DEFAULT_NAME, DOMAIN


class ProfilerConfigFlow(ConfigFlow, domain=DOMAIN):
    """Handle a config flow for Profiler."""

    VERSION = 1

    async def async_step_user(
        self, user_input: dict[str, Any] | None = None
    ) -> ConfigFlowResult:
        """Handle the initial step."""
        if user_input is not None:
            return self.async_create_entry(title=DEFAULT_NAME, data={})

        return self.async_show_form(step_id="user")

</document_content>
</document>
<document index="4">
<source>/Users/malcolm/dev/home-assistant/core/homeassistant/components/profiler/const.py</source>
<document_content>
"""Consts used by profiler."""

DOMAIN = "profiler"
DEFAULT_NAME = "Profiler"

</document_content>
</document>
<document index="5">
<source>/Users/malcolm/dev/home-assistant/core/homeassistant/components/profiler/icons.json</source>
<document_content>
{
  "services": {
    "start": {
      "service": "mdi:play"
    },
    "memory": {
      "service": "mdi:memory"
    },
    "start_log_objects": {
      "service": "mdi:invoice-text-plus"
    },
    "stop_log_objects": {
      "service": "mdi:invoice-text-remove"
    },
    "dump_log_objects": {
      "service": "mdi:invoice-export-outline"
    },
    "start_log_object_sources": {
      "service": "mdi:play"
    },
    "stop_log_object_sources": {
      "service": "mdi:stop"
    },
    "lru_stats": {
      "service": "mdi:chart-areaspline"
    },
    "log_current_tasks": {
      "service": "mdi:format-list-bulleted"
    },
    "log_thread_frames": {
      "service": "mdi:format-list-bulleted"
    },
    "log_event_loop_scheduled": {
      "service": "mdi:calendar-clock"
    },
    "set_asyncio_debug": {
      "service": "mdi:bug-check"
    }
  }
}

</document_content>
</document>
<document index="6">
<source>/Users/malcolm/dev/home-assistant/core/homeassistant/components/profiler/manifest.json</source>
<document_content>
{
  "domain": "profiler",
  "name": "Profiler",
  "codeowners": ["@bdraco"],
  "config_flow": true,
  "documentation": "https://www.home-assistant.io/integrations/profiler",
  "quality_scale": "internal",
  "requirements": [
    "pyprof2calltree==1.4.5",
    "guppy3==3.1.5",
    "objgraph==3.5.0"
  ],
  "single_config_entry": true
}

</document_content>
</document>
<document index="7">
<source>/Users/malcolm/dev/home-assistant/core/homeassistant/components/profiler/services.yaml</source>
<document_content>
start:
  fields:
    seconds:
      default: 60.0
      selector:
        number:
          min: 1
          max: 3600
          unit_of_measurement: seconds
memory:
  fields:
    seconds:
      default: 60.0
      selector:
        number:
          min: 1
          max: 3600
          unit_of_measurement: seconds
start_log_objects:
  fields:
    scan_interval:
      default: 30.0
      selector:
        number:
          min: 1
          max: 3600
          unit_of_measurement: seconds
stop_log_objects:
dump_log_objects:
  fields:
    type:
      required: true
      example: State
      selector:
        text:
start_log_object_sources:
  fields:
    scan_interval:
      default: 30.0
      selector:
        number:
          min: 1
          max: 3600
          unit_of_measurement: seconds
    max_objects:
      default: 5
      selector:
        number:
          min: 1
          max: 30
          unit_of_measurement: objects
stop_log_object_sources:
lru_stats:
log_thread_frames:
log_event_loop_scheduled:
set_asyncio_debug:
  fields:
    enabled:
      default: true
      selector:
        boolean:
log_current_tasks:

</document_content>
</document>
<document index="8">
<source>/Users/malcolm/dev/home-assistant/core/homeassistant/components/profiler/strings.json</source>
<document_content>
{
  "config": {
    "step": {
      "user": {
        "description": "[%key:common::config_flow::description::confirm_setup%]"
      }
    }
  },
  "services": {
    "start": {
      "name": "[%key:common::action::start%]",
      "description": "Starts the Profiler.",
      "fields": {
        "seconds": {
          "name": "Seconds",
          "description": "The number of seconds to run the profiler."
        }
      }
    },
    "memory": {
      "name": "Memory",
      "description": "Starts the Memory Profiler.",
      "fields": {
        "seconds": {
          "name": "Seconds",
          "description": "The number of seconds to run the memory profiler."
        }
      }
    },
    "start_log_objects": {
      "name": "Start logging objects",
      "description": "Starts logging growth of objects in memory.",
      "fields": {
        "scan_interval": {
          "name": "Scan interval",
          "description": "The number of seconds between logging objects."
        }
      }
    },
    "stop_log_objects": {
      "name": "Stop logging objects",
      "description": "Stops logging growth of objects in memory."
    },
    "dump_log_objects": {
      "name": "Dump log objects",
      "description": "Dumps the repr of all matching objects to the log.",
      "fields": {
        "type": {
          "name": "Type",
          "description": "The type of objects to dump to the log."
        }
      }
    },
    "start_log_object_sources": {
      "name": "Start logging object sources",
      "description": "Starts logging sources of new objects in memory.",
      "fields": {
        "scan_interval": {
          "name": "Scan interval",
          "description": "[%key:component::profiler::services::start_log_objects::fields::scan_interval::description%]"
        },
        "max_objects": {
          "name": "Maximum objects",
          "description": "The maximum number of objects to log."
        }
      }
    },
    "stop_log_object_sources": {
      "name": "Stop logging object sources",
      "description": "Stops logging sources of new objects in memory."
    },
    "lru_stats": {
      "name": "Log LRU stats",
      "description": "Logs the stats of all lru caches."
    },
    "log_thread_frames": {
      "name": "Log thread frames",
      "description": "Logs the current frames for all threads."
    },
    "log_event_loop_scheduled": {
      "name": "Log event loop scheduled",
      "description": "Logs what is scheduled in the event loop."
    },
    "set_asyncio_debug": {
      "name": "Set asyncio debug",
      "description": "Enable or disable asyncio debug.",
      "fields": {
        "enabled": {
          "name": "Enabled",
          "description": "Whether to enable or disable asyncio debug."
        }
      }
    },
    "log_current_tasks": {
      "name": "Log current asyncio tasks",
      "description": "Logs all the current asyncio tasks."
    }
  }
}

</document_content>
</document>
<document index="9">
<source>/Users/malcolm/dev/home-assistant/core/homeassistant/bootstrap.py</source>
<document_content>
"""Provide methods to bootstrap a Home Assistant instance."""

from __future__ import annotations

import asyncio
from collections import defaultdict
import contextlib
from functools import partial
from itertools import chain
import logging
from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler
import mimetypes
from operator import contains, itemgetter
import os
import platform
import sys
import threading
from time import monotonic
from typing import TYPE_CHECKING, Any

# Import cryptography early since import openssl is not thread-safe
# _frozen_importlib._DeadlockError: deadlock detected by _ModuleLock('cryptography.hazmat.backends.openssl.backend')
import cryptography.hazmat.backends.openssl.backend  # noqa: F401
import voluptuous as vol
import yarl

from . import (
    block_async_io,
    config as conf_util,
    config_entries,
    core,
    loader,
    requirements,
)

# Pre-import frontend deps which have no requirements here to avoid
# loading them at run time and blocking the event loop. We do this ahead
# of time so that we do not have to flag frontend deps with `import_executor`
# as it would create a thundering heard of executor jobs trying to import
# frontend deps at the same time.
from .components import (
    api as api_pre_import,  # noqa: F401
    auth as auth_pre_import,  # noqa: F401
    config as config_pre_import,  # noqa: F401
    default_config as default_config_pre_import,  # noqa: F401
    device_automation as device_automation_pre_import,  # noqa: F401
    diagnostics as diagnostics_pre_import,  # noqa: F401
    file_upload as file_upload_pre_import,  # noqa: F401
    group as group_pre_import,  # noqa: F401
    history as history_pre_import,  # noqa: F401
    http,  # not named pre_import since it has requirements
    image_upload as image_upload_import,  # noqa: F401 - not named pre_import since it has requirements
    logbook as logbook_pre_import,  # noqa: F401
    lovelace as lovelace_pre_import,  # noqa: F401
    onboarding as onboarding_pre_import,  # noqa: F401
    recorder as recorder_import,  # noqa: F401 - not named pre_import since it has requirements
    repairs as repairs_pre_import,  # noqa: F401
    search as search_pre_import,  # noqa: F401
    sensor as sensor_pre_import,  # noqa: F401
    system_log as system_log_pre_import,  # noqa: F401
    webhook as webhook_pre_import,  # noqa: F401
    websocket_api as websocket_api_pre_import,  # noqa: F401
)
from .components.sensor import recorder as sensor_recorder  # noqa: F401
from .const import (
    BASE_PLATFORMS,
    FORMAT_DATETIME,
    KEY_DATA_LOGGING as DATA_LOGGING,
    REQUIRED_NEXT_PYTHON_HA_RELEASE,
    REQUIRED_NEXT_PYTHON_VER,
    SIGNAL_BOOTSTRAP_INTEGRATIONS,
)
from .core_config import async_process_ha_core_config
from .exceptions import HomeAssistantError
from .helpers import (
    area_registry,
    category_registry,
    config_validation as cv,
    device_registry,
    entity,
    entity_registry,
    floor_registry,
    issue_registry,
    label_registry,
    recorder,
    restore_state,
    template,
    translation,
)
from .helpers.dispatcher import async_dispatcher_send_internal
from .helpers.storage import get_internal_store_manager
from .helpers.system_info import async_get_system_info
from .helpers.typing import ConfigType
from .setup import (
    # _setup_started is marked as protected to make it clear
    # that it is not part of the public API and should not be used
    # by integrations. It is only used for internal tracking of
    # which integrations are being set up.
    _setup_started,
    async_get_setup_timings,
    async_notify_setup_error,
    async_set_domains_to_be_loaded,
    async_setup_component,
)
from .util.async_ import create_eager_task
from .util.hass_dict import HassKey
from .util.logging import async_activate_log_queue_handler
from .util.package import async_get_user_site, is_docker_env, is_virtual_env
from .util.system_info import is_official_image

with contextlib.suppress(ImportError):
    # Ensure anyio backend is imported to avoid it being imported in the event loop
    from anyio._backends import _asyncio  # noqa: F401


if TYPE_CHECKING:
    from .runner import RuntimeConfig

_LOGGER = logging.getLogger(__name__)

SETUP_ORDER_SORT_KEY = partial(contains, BASE_PLATFORMS)


ERROR_LOG_FILENAME = "home-assistant.log"

# hass.data key for logging information.
DATA_REGISTRIES_LOADED: HassKey[None] = HassKey("bootstrap_registries_loaded")

LOG_SLOW_STARTUP_INTERVAL = 60
SLOW_STARTUP_CHECK_INTERVAL = 1

STAGE_1_TIMEOUT = 120
STAGE_2_TIMEOUT = 300
WRAP_UP_TIMEOUT = 300
COOLDOWN_TIME = 60


DEBUGGER_INTEGRATIONS = {"debugpy"}

# Core integrations are unconditionally loaded
CORE_INTEGRATIONS = {"homeassistant", "persistent_notification"}

# Integrations that are loaded right after the core is set up
LOGGING_AND_HTTP_DEPS_INTEGRATIONS = {
    # isal is loaded right away before `http` to ensure if its
    # enabled, that `isal` is up to date.
    "isal",
    # Set log levels
    "logger",
    # Error logging
    "system_log",
    "sentry",
}
FRONTEND_INTEGRATIONS = {
    # Get the frontend up and running as soon as possible so problem
    # integrations can be removed and database migration status is
    # visible in frontend
    "frontend",
}
RECORDER_INTEGRATIONS = {
    # Setup after frontend
    # To record data
    "recorder",
}
DISCOVERY_INTEGRATIONS = ("bluetooth", "dhcp", "ssdp", "usb", "zeroconf")
STAGE_1_INTEGRATIONS = {
    # We need to make sure discovery integrations
    # update their deps before stage 2 integrations
    # load them inadvertently before their deps have
    # been updated which leads to using an old version
    # of the dep, or worse (import errors).
    *DISCOVERY_INTEGRATIONS,
    # To make sure we forward data to other instances
    "mqtt_eventstream",
    # To provide account link implementations
    "cloud",
    # Ensure supervisor is available
    "hassio",
}
DEFAULT_INTEGRATIONS = {
    # These integrations are set up unless recovery mode is activated.
    #
    # Integrations providing core functionality:
    "analytics",  # Needed for onboarding
    "application_credentials",
    "backup",
    "frontend",
    "hardware",
    "logger",
    "network",
    "system_health",
    #
    # Key-feature:
    "automation",
    "person",
    "scene",
    "script",
    "tag",
    "zone",
    #
    # Built-in helpers:
    "counter",
    "input_boolean",
    "input_button",
    "input_datetime",
    "input_number",
    "input_select",
    "input_text",
    "schedule",
    "timer",
}
DEFAULT_INTEGRATIONS_RECOVERY_MODE = {
    # These integrations are set up if recovery mode is activated.
    "frontend",
}
DEFAULT_INTEGRATIONS_SUPERVISOR = {
    # These integrations are set up if using the Supervisor
    "hassio",
}
CRITICAL_INTEGRATIONS = {
    # Recovery mode is activated if these integrations fail to set up
    "frontend",
}

SETUP_ORDER = (
    # Load logging and http deps as soon as possible
    ("logging, http deps", LOGGING_AND_HTTP_DEPS_INTEGRATIONS),
    # Setup frontend
    ("frontend", FRONTEND_INTEGRATIONS),
    # Setup recorder
    ("recorder", RECORDER_INTEGRATIONS),
    # Start up debuggers. Start these first in case they want to wait.
    ("debugger", DEBUGGER_INTEGRATIONS),
)

#
# Storage keys we are likely to load during startup
# in order of when we expect to load them.
#
# If they do not exist they will not be loaded
#
PRELOAD_STORAGE = [
    "core.logger",
    "core.network",
    "http.auth",
    "image",
    "lovelace_dashboards",
    "lovelace_resources",
    "core.uuid",
    "lovelace.map",
    "bluetooth.passive_update_processor",
    "bluetooth.remote_scanners",
    "assist_pipeline.pipelines",
    "core.analytics",
    "auth_module.totp",
    "backup",
]


async def async_setup_hass(
    runtime_config: RuntimeConfig,
) -> core.HomeAssistant | None:
    """Set up Home Assistant."""

    async def create_hass() -> core.HomeAssistant:
        """Create the hass object and do basic setup."""
        hass = core.HomeAssistant(runtime_config.config_dir)
        loader.async_setup(hass)

        await async_enable_logging(
            hass,
            runtime_config.verbose,
            runtime_config.log_rotate_days,
            runtime_config.log_file,
            runtime_config.log_no_color,
        )

        if runtime_config.debug or hass.loop.get_debug():
            hass.config.debug = True

        hass.config.safe_mode = runtime_config.safe_mode
        hass.config.skip_pip = runtime_config.skip_pip
        hass.config.skip_pip_packages = runtime_config.skip_pip_packages

        return hass

    async def stop_hass(hass: core.HomeAssistant) -> None:
        """Stop hass."""
        # Ask integrations to shut down. It's messy but we can't
        # do a clean stop without knowing what is broken
        with contextlib.suppress(TimeoutError):
            async with hass.timeout.async_timeout(10):
                await hass.async_stop()

    hass = await create_hass()

    if runtime_config.skip_pip or runtime_config.skip_pip_packages:
        _LOGGER.warning(
            "Skipping pip installation of required modules. This may cause issues"
        )

    if not await conf_util.async_ensure_config_exists(hass):
        _LOGGER.error("Error getting configuration path")
        return None

    _LOGGER.info("Config directory: %s", runtime_config.config_dir)

    block_async_io.enable()

    config_dict = None
    basic_setup_success = False

    if not (recovery_mode := runtime_config.recovery_mode):
        await hass.async_add_executor_job(conf_util.process_ha_config_upgrade, hass)

        try:
            config_dict = await conf_util.async_hass_config_yaml(hass)
        except HomeAssistantError as err:
            _LOGGER.error(
                "Failed to parse configuration.yaml: %s. Activating recovery mode",
                err,
            )
        else:
            if not is_virtual_env():
                await async_mount_local_lib_path(runtime_config.config_dir)

            basic_setup_success = (
                await async_from_config_dict(config_dict, hass) is not None
            )

    if config_dict is None:
        recovery_mode = True
        await stop_hass(hass)
        hass = await create_hass()

    elif not basic_setup_success:
        _LOGGER.warning("Unable to set up core integrations. Activating recovery mode")
        recovery_mode = True
        await stop_hass(hass)
        hass = await create_hass()

    elif any(domain not in hass.config.components for domain in CRITICAL_INTEGRATIONS):
        _LOGGER.warning(
            "Detected that %s did not load. Activating recovery mode",
            ",".join(CRITICAL_INTEGRATIONS),
        )

        old_config = hass.config
        old_logging = hass.data.get(DATA_LOGGING)

        recovery_mode = True
        await stop_hass(hass)
        hass = await create_hass()

        if old_logging:
            hass.data[DATA_LOGGING] = old_logging
        hass.config.debug = old_config.debug
        hass.config.skip_pip = old_config.skip_pip
        hass.config.skip_pip_packages = old_config.skip_pip_packages
        hass.config.internal_url = old_config.internal_url
        hass.config.external_url = old_config.external_url
        # Setup loader cache after the config dir has been set
        loader.async_setup(hass)

    if recovery_mode:
        _LOGGER.info("Starting in recovery mode")
        hass.config.recovery_mode = True

        http_conf = (await http.async_get_last_config(hass)) or {}

        await async_from_config_dict(
            {"recovery_mode": {}, "http": http_conf},
            hass,
        )
    elif hass.config.safe_mode:
        _LOGGER.info("Starting in safe mode")

    if runtime_config.open_ui:
        hass.add_job(open_hass_ui, hass)

    return hass


def open_hass_ui(hass: core.HomeAssistant) -> None:
    """Open the UI."""
    import webbrowser  # pylint: disable=import-outside-toplevel

    if hass.config.api is None or "frontend" not in hass.config.components:
        _LOGGER.warning("Cannot launch the UI because frontend not loaded")
        return

    scheme = "https" if hass.config.api.use_ssl else "http"
    url = str(
        yarl.URL.build(scheme=scheme, host="127.0.0.1", port=hass.config.api.port)
    )

    if not webbrowser.open(url):
        _LOGGER.warning(
            "Unable to open the Home Assistant UI in a browser. Open it yourself at %s",
            url,
        )


def _init_blocking_io_modules_in_executor() -> None:
    """Initialize modules that do blocking I/O in executor."""
    # Cache the result of platform.uname().processor in the executor.
    # Multiple modules call this function at startup which
    # executes a blocking subprocess call. This is a problem for the
    # asyncio event loop. By priming the cache of uname we can
    # avoid the blocking call in the event loop.
    _ = platform.uname().processor
    # Initialize the mimetypes module to avoid blocking calls
    # to the filesystem to load the mime.types file.
    mimetypes.init()
    # Initialize is_official_image and is_docker_env to avoid blocking calls
    # to the filesystem.
    is_official_image()
    is_docker_env()


async def async_load_base_functionality(hass: core.HomeAssistant) -> None:
    """Load the registries and modules that will do blocking I/O."""
    if DATA_REGISTRIES_LOADED in hass.data:
        return
    hass.data[DATA_REGISTRIES_LOADED] = None
    translation.async_setup(hass)
    entity.async_setup(hass)
    template.async_setup(hass)
    await asyncio.gather(
        create_eager_task(get_internal_store_manager(hass).async_initialize()),
        create_eager_task(area_registry.async_load(hass)),
        create_eager_task(category_registry.async_load(hass)),
        create_eager_task(device_registry.async_load(hass)),
        create_eager_task(entity_registry.async_load(hass)),
        create_eager_task(floor_registry.async_load(hass)),
        create_eager_task(issue_registry.async_load(hass)),
        create_eager_task(label_registry.async_load(hass)),
        hass.async_add_executor_job(_init_blocking_io_modules_in_executor),
        create_eager_task(template.async_load_custom_templates(hass)),
        create_eager_task(restore_state.async_load(hass)),
        create_eager_task(hass.config_entries.async_initialize()),
        create_eager_task(async_get_system_info(hass)),
    )


async def async_from_config_dict(
    config: ConfigType, hass: core.HomeAssistant
) -> core.HomeAssistant | None:
    """Try to configure Home Assistant from a configuration dictionary.

    Dynamically loads required components and its dependencies.
    This method is a coroutine.
    """
    start = monotonic()

    hass.config_entries = config_entries.ConfigEntries(hass, config)
    # Prime custom component cache early so we know if registry entries are tied
    # to a custom integration
    await loader.async_get_custom_components(hass)
    await async_load_base_functionality(hass)

    # Set up core.
    _LOGGER.debug("Setting up %s", CORE_INTEGRATIONS)

    if not all(
        await asyncio.gather(
            *(
                create_eager_task(
                    async_setup_component(hass, domain, config),
                    name=f"bootstrap setup {domain}",
                    loop=hass.loop,
                )
                for domain in CORE_INTEGRATIONS
            )
        )
    ):
        _LOGGER.error("Home Assistant core failed to initialize. ")
        return None

    _LOGGER.debug("Home Assistant core initialized")

    core_config = config.get(core.DOMAIN, {})

    try:
        await async_process_ha_core_config(hass, core_config)
    except vol.Invalid as config_err:
        conf_util.async_log_schema_error(config_err, core.DOMAIN, core_config, hass)
        async_notify_setup_error(hass, core.DOMAIN)
        return None
    except HomeAssistantError:
        _LOGGER.error(
            "Home Assistant core failed to initialize. Further initialization aborted"
        )
        return None

    await _async_set_up_integrations(hass, config)

    stop = monotonic()
    _LOGGER.info("Home Assistant initialized in %.2fs", stop - start)

    if (
        REQUIRED_NEXT_PYTHON_HA_RELEASE
        and sys.version_info[:3] < REQUIRED_NEXT_PYTHON_VER
    ):
        current_python_version = ".".join(str(x) for x in sys.version_info[:3])
        required_python_version = ".".join(str(x) for x in REQUIRED_NEXT_PYTHON_VER[:2])
        _LOGGER.warning(
            (
                "Support for the running Python version %s is deprecated and "
                "will be removed in Home Assistant %s; "
                "Please upgrade Python to %s"
            ),
            current_python_version,
            REQUIRED_NEXT_PYTHON_HA_RELEASE,
            required_python_version,
        )
        issue_registry.async_create_issue(
            hass,
            core.DOMAIN,
            f"python_version_{required_python_version}",
            is_fixable=False,
            severity=issue_registry.IssueSeverity.WARNING,
            breaks_in_ha_version=REQUIRED_NEXT_PYTHON_HA_RELEASE,
            translation_key="python_version",
            translation_placeholders={
                "current_python_version": current_python_version,
                "required_python_version": required_python_version,
                "breaks_in_ha_version": REQUIRED_NEXT_PYTHON_HA_RELEASE,
            },
        )

    return hass


async def async_enable_logging(
    hass: core.HomeAssistant,
    verbose: bool = False,
    log_rotate_days: int | None = None,
    log_file: str | None = None,
    log_no_color: bool = False,
) -> None:
    """Set up the logging.

    This method must be run in the event loop.
    """
    fmt = (
        "%(asctime)s.%(msecs)03d %(levelname)s (%(threadName)s) [%(name)s] %(message)s"
    )

    if not log_no_color:
        try:
            # pylint: disable-next=import-outside-toplevel
            from colorlog import ColoredFormatter

            # basicConfig must be called after importing colorlog in order to
            # ensure that the handlers it sets up wraps the correct streams.
            logging.basicConfig(level=logging.INFO)

            colorfmt = f"%(log_color)s{fmt}%(reset)s"
            logging.getLogger().handlers[0].setFormatter(
                ColoredFormatter(
                    colorfmt,
                    datefmt=FORMAT_DATETIME,
                    reset=True,
                    log_colors={
                        "DEBUG": "cyan",
                        "INFO": "green",
                        "WARNING": "yellow",
                        "ERROR": "red",
                        "CRITICAL": "red",
                    },
                )
            )
        except ImportError:
            pass

    # If the above initialization failed for any reason, setup the default
    # formatting.  If the above succeeds, this will result in a no-op.
    logging.basicConfig(format=fmt, datefmt=FORMAT_DATETIME, level=logging.INFO)

    # Capture warnings.warn(...) and friends messages in logs.
    # The standard destination for them is stderr, which may end up unnoticed.
    # This way they're where other messages are, and can be filtered as usual.
    logging.captureWarnings(True)

    # Suppress overly verbose logs from libraries that aren't helpful
    logging.getLogger("requests").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("aiohttp.access").setLevel(logging.WARNING)
    logging.getLogger("httpx").setLevel(logging.WARNING)

    sys.excepthook = lambda *args: logging.getLogger().exception(
        "Uncaught exception", exc_info=args
    )
    threading.excepthook = lambda args: logging.getLogger().exception(
        "Uncaught thread exception",
        exc_info=(  # type: ignore[arg-type]
            args.exc_type,
            args.exc_value,
            args.exc_traceback,
        ),
    )

    # Log errors to a file if we have write access to file or config dir
    if log_file is None:
        err_log_path = hass.config.path(ERROR_LOG_FILENAME)
    else:
        err_log_path = os.path.abspath(log_file)

    err_path_exists = os.path.isfile(err_log_path)
    err_dir = os.path.dirname(err_log_path)

    # Check if we can write to the error log if it exists or that
    # we can create files in the containing directory if not.
    if (err_path_exists and os.access(err_log_path, os.W_OK)) or (
        not err_path_exists and os.access(err_dir, os.W_OK)
    ):
        err_handler = await hass.async_add_executor_job(
            _create_log_file, err_log_path, log_rotate_days
        )

        err_handler.setFormatter(logging.Formatter(fmt, datefmt=FORMAT_DATETIME))

        logger = logging.getLogger()
        logger.addHandler(err_handler)
        logger.setLevel(logging.INFO if verbose else logging.WARNING)

        # Save the log file location for access by other components.
        hass.data[DATA_LOGGING] = err_log_path
    else:
        _LOGGER.error("Unable to set up error log %s (access denied)", err_log_path)

    async_activate_log_queue_handler(hass)


def _create_log_file(
    err_log_path: str, log_rotate_days: int | None
) -> RotatingFileHandler | TimedRotatingFileHandler:
    """Create log file and do roll over."""
    err_handler: RotatingFileHandler | TimedRotatingFileHandler
    if log_rotate_days:
        err_handler = TimedRotatingFileHandler(
            err_log_path, when="midnight", backupCount=log_rotate_days
        )
    else:
        err_handler = _RotatingFileHandlerWithoutShouldRollOver(
            err_log_path, backupCount=1
        )

    try:
        err_handler.doRollover()
    except OSError as err:
        _LOGGER.error("Error rolling over log file: %s", err)

    return err_handler


class _RotatingFileHandlerWithoutShouldRollOver(RotatingFileHandler):
    """RotatingFileHandler that does not check if it should roll over on every log."""

    def shouldRollover(self, record: logging.LogRecord) -> bool:
        """Never roll over.

        The shouldRollover check is expensive because it has to stat
        the log file for every log record. Since we do not set maxBytes
        the result of this check is always False.
        """
        return False


async def async_mount_local_lib_path(config_dir: str) -> str:
    """Add local library to Python Path.

    This function is a coroutine.
    """
    deps_dir = os.path.join(config_dir, "deps")
    if (lib_dir := await async_get_user_site(deps_dir)) not in sys.path:
        sys.path.insert(0, lib_dir)
    return deps_dir


@core.callback
def _get_domains(hass: core.HomeAssistant, config: dict[str, Any]) -> set[str]:
    """Get domains of components to set up."""
    # Filter out the repeating and common config section [homeassistant]
    domains = {
        domain for key in config if (domain := cv.domain_key(key)) != core.DOMAIN
    }

    # Add config entry and default domains
    if not hass.config.recovery_mode:
        domains.update(DEFAULT_INTEGRATIONS)
        domains.update(hass.config_entries.async_domains())
    else:
        domains.update(DEFAULT_INTEGRATIONS_RECOVERY_MODE)

    # Add domains depending on if the Supervisor is used or not
    if "SUPERVISOR" in os.environ:
        domains.update(DEFAULT_INTEGRATIONS_SUPERVISOR)

    return domains


class _WatchPendingSetups:
    """Periodic log and dispatch of setups that are pending."""

    def __init__(
        self,
        hass: core.HomeAssistant,
        setup_started: dict[tuple[str, str | None], float],
    ) -> None:
        """Initialize the WatchPendingSetups class."""
        self._hass = hass
        self._setup_started = setup_started
        self._duration_count = 0
        self._handle: asyncio.TimerHandle | None = None
        self._previous_was_empty = True
        self._loop = hass.loop

    def _async_watch(self) -> None:
        """Periodic log of setups that are pending."""
        now = monotonic()
        self._duration_count += SLOW_STARTUP_CHECK_INTERVAL

        remaining_with_setup_started: defaultdict[str, float] = defaultdict(float)
        for integration_group, start_time in self._setup_started.items():
            domain, _ = integration_group
            remaining_with_setup_started[domain] += now - start_time

        if remaining_with_setup_started:
            _LOGGER.debug("Integration remaining: %s", remaining_with_setup_started)
        elif waiting_tasks := self._hass._active_tasks:  # noqa: SLF001
            _LOGGER.debug("Waiting on tasks: %s", waiting_tasks)
        self._async_dispatch(remaining_with_setup_started)
        if (
            self._setup_started
            and self._duration_count % LOG_SLOW_STARTUP_INTERVAL == 0
        ):
            # We log every LOG_SLOW_STARTUP_INTERVAL until all integrations are done
            # once we take over LOG_SLOW_STARTUP_INTERVAL (60s) to start up
            _LOGGER.warning(
                "Waiting on integrations to complete setup: %s",
                self._setup_started,
            )

        _LOGGER.debug("Running timeout Zones: %s", self._hass.timeout.zones)
        self._async_schedule_next()

    def _async_dispatch(self, remaining_with_setup_started: dict[str, float]) -> None:
        """Dispatch the signal."""
        if remaining_with_setup_started or not self._previous_was_empty:
            async_dispatcher_send_internal(
                self._hass, SIGNAL_BOOTSTRAP_INTEGRATIONS, remaining_with_setup_started
            )
        self._previous_was_empty = not remaining_with_setup_started

    def _async_schedule_next(self) -> None:
        """Schedule the next call."""
        self._handle = self._loop.call_later(
            SLOW_STARTUP_CHECK_INTERVAL, self._async_watch
        )

    def async_start(self) -> None:
        """Start watching."""
        self._async_schedule_next()

    def async_stop(self) -> None:
        """Stop watching."""
        self._async_dispatch({})
        if self._handle:
            self._handle.cancel()
            self._handle = None


async def async_setup_multi_components(
    hass: core.HomeAssistant,
    domains: set[str],
    config: dict[str, Any],
) -> None:
    """Set up multiple domains. Log on failure."""
    # Avoid creating tasks for domains that were setup in a previous stage
    domains_not_yet_setup = domains - hass.config.components
    # Create setup tasks for base platforms first since everything will have
    # to wait to be imported, and the sooner we can get the base platforms
    # loaded the sooner we can start loading the rest of the integrations.
    futures = {
        domain: hass.async_create_task_internal(
            async_setup_component(hass, domain, config),
            f"setup component {domain}",
            eager_start=True,
        )
        for domain in sorted(
            domains_not_yet_setup, key=SETUP_ORDER_SORT_KEY, reverse=True
        )
    }
    results = await asyncio.gather(*futures.values(), return_exceptions=True)
    for idx, domain in enumerate(futures):
        result = results[idx]
        if isinstance(result, BaseException):
            _LOGGER.error(
                "Error setting up integration %s - received exception",
                domain,
                exc_info=(type(result), result, result.__traceback__),
            )


async def _async_resolve_domains_to_setup(
    hass: core.HomeAssistant, config: dict[str, Any]
) -> tuple[set[str], dict[str, loader.Integration]]:
    """Resolve all dependencies and return list of domains to set up."""
    domains_to_setup = _get_domains(hass, config)
    needed_requirements: set[str] = set()
    platform_integrations = conf_util.extract_platform_integrations(
        config, BASE_PLATFORMS
    )
    # Ensure base platforms that have platform integrations are added to
    # to `domains_to_setup so they can be setup first instead of
    # discovering them when later when a config entry setup task
    # notices its needed and there is already a long line to use
    # the import executor.
    #
    # For example if we have
    # sensor:
    #   - platform: template
    #
    # `template` has to be loaded to validate the config for sensor
    # so we want to start loading `sensor` as soon as we know
    # it will be needed. The more platforms under `sensor:`, the longer
    # it will take to finish setup for `sensor` because each of these
    # platforms has to be imported before we can validate the config.
    #
    # Thankfully we are migrating away from the platform pattern
    # so this will be less of a problem in the future.
    domains_to_setup.update(platform_integrations)

    # Load manifests for base platforms and platform based integrations
    # that are defined under base platforms right away since we do not require
    # the manifest to list them as dependencies and we want to avoid the lock
    # contention when multiple integrations try to load them at once
    additional_manifests_to_load = {
        *BASE_PLATFORMS,
        *chain.from_iterable(platform_integrations.values()),
    }

    translations_to_load = additional_manifests_to_load.copy()

    # Resolve all dependencies so we know all integrations
    # that will have to be loaded and start right-away
    integration_cache: dict[str, loader.Integration] = {}
    to_resolve: set[str] = domains_to_setup
    while to_resolve or additional_manifests_to_load:
        old_to_resolve: set[str] = to_resolve
        to_resolve = set()

        if additional_manifests_to_load:
            to_get = {*old_to_resolve, *additional_manifests_to_load}
            additional_manifests_to_load.clear()
        else:
            to_get = old_to_resolve

        manifest_deps: set[str] = set()
        resolve_dependencies_tasks: list[asyncio.Task[bool]] = []
        integrations_to_process: list[loader.Integration] = []

        for domain, itg in (await loader.async_get_integrations(hass, to_get)).items():
            if not isinstance(itg, loader.Integration):
                continue
            integration_cache[domain] = itg
            needed_requirements.update(itg.requirements)

            # Make sure manifests for dependencies are loaded in the next
            # loop to try to group as many as manifest loads in a single
            # call to avoid the creating one-off executor jobs later in
            # the setup process
            additional_manifests_to_load.update(
                dep
                for dep in chain(itg.dependencies, itg.after_dependencies)
                if dep not in integration_cache
            )

            if domain not in old_to_resolve:
                continue

            integrations_to_process.append(itg)
            manifest_deps.update(itg.dependencies)
            manifest_deps.update(itg.after_dependencies)
            if not itg.all_dependencies_resolved:
                resolve_dependencies_tasks.append(
                    create_eager_task(
                        itg.resolve_dependencies(),
                        name=f"resolve dependencies {domain}",
                        loop=hass.loop,
                    )
                )

        if unseen_deps := manifest_deps - integration_cache.keys():
            # If there are dependencies, try to preload all
            # the integrations manifest at once and add them
            # to the list of requirements we need to install
            # so we can try to check if they are already installed
            # in a single call below which avoids each integration
            # having to wait for the lock to do it individually
            deps = await loader.async_get_integrations(hass, unseen_deps)
            for dependant_domain, dependant_itg in deps.items():
                if isinstance(dependant_itg, loader.Integration):
                    integration_cache[dependant_domain] = dependant_itg
                    needed_requirements.update(dependant_itg.requirements)

        if resolve_dependencies_tasks:
            await asyncio.gather(*resolve_dependencies_tasks)

        for itg in integrations_to_process:
            try:
                all_deps = itg.all_dependencies
            except RuntimeError:
                # Integration.all_dependencies raises RuntimeError if
                # dependencies could not be resolved
                continue
            for dep in all_deps:
                if dep in domains_to_setup:
                    continue
                domains_to_setup.add(dep)
                to_resolve.add(dep)

    _LOGGER.info("Domains to be set up: %s", domains_to_setup)

    # Optimistically check if requirements are already installed
    # ahead of setting up the integrations so we can prime the cache
    # We do not wait for this since its an optimization only
    hass.async_create_background_task(
        requirements.async_load_installed_versions(hass, needed_requirements),
        "check installed requirements",
        eager_start=True,
    )

    #
    # Only add the domains_to_setup after we finish resolving
    # as new domains are likely to added in the process
    #
    translations_to_load.update(domains_to_setup)
    # Start loading translations for all integrations we are going to set up
    # in the background so they are ready when we need them. This avoids a
    # lot of waiting for the translation load lock and a thundering herd of
    # tasks trying to load the same translations at the same time as each
    # integration is loaded.
    #
    # We do not wait for this since as soon as the task runs it will
    # hold the translation load lock and if anything is fast enough to
    # wait for the translation load lock, loading will be done by the
    # time it gets to it.
    hass.async_create_background_task(
        translation.async_load_integrations(hass, translations_to_load),
        "load translations",
        eager_start=True,
    )

    # Preload storage for all integrations we are going to set up
    # so we do not have to wait for it to be loaded when we need it
    # in the setup process.
    hass.async_create_background_task(
        get_internal_store_manager(hass).async_preload(
            [*PRELOAD_STORAGE, *domains_to_setup]
        ),
        "preload storage",
        eager_start=True,
    )

    return domains_to_setup, integration_cache


async def _async_set_up_integrations(
    hass: core.HomeAssistant, config: dict[str, Any]
) -> None:
    """Set up all the integrations."""
    watcher = _WatchPendingSetups(hass, _setup_started(hass))
    watcher.async_start()

    domains_to_setup, integration_cache = await _async_resolve_domains_to_setup(
        hass, config
    )

    # Initialize recorder
    if "recorder" in domains_to_setup:
        recorder.async_initialize_recorder(hass)

    pre_stage_domains = [
        (name, domains_to_setup & domain_group) for name, domain_group in SETUP_ORDER
    ]

    # calculate what components to setup in what stage
    stage_1_domains: set[str] = set()

    # Find all dependencies of any dependency of any stage 1 integration that
    # we plan on loading and promote them to stage 1. This is done only to not
    # get misleading log messages
    deps_promotion: set[str] = STAGE_1_INTEGRATIONS
    while deps_promotion:
        old_deps_promotion = deps_promotion
        deps_promotion = set()

        for domain in old_deps_promotion:
            if domain not in domains_to_setup or domain in stage_1_domains:
                continue

            stage_1_domains.add(domain)

            if (dep_itg := integration_cache.get(domain)) is None:
                continue

            deps_promotion.update(dep_itg.all_dependencies)

    stage_2_domains = domains_to_setup - stage_1_domains

    for name, domain_group in pre_stage_domains:
        if domain_group:
            stage_2_domains -= domain_group
            _LOGGER.info("Setting up %s: %s", name, domain_group)
            to_be_loaded = domain_group.copy()
            to_be_loaded.update(
                dep
                for domain in domain_group
                if (integration := integration_cache.get(domain)) is not None
                for dep in integration.all_dependencies
            )
            async_set_domains_to_be_loaded(hass, to_be_loaded)
            await async_setup_multi_components(hass, domain_group, config)

    # Enables after dependencies when setting up stage 1 domains
    async_set_domains_to_be_loaded(hass, stage_1_domains)

    # Start setup
    if stage_1_domains:
        _LOGGER.info("Setting up stage 1: %s", stage_1_domains)
        try:
            async with hass.timeout.async_timeout(
                STAGE_1_TIMEOUT, cool_down=COOLDOWN_TIME
            ):
                await async_setup_multi_components(hass, stage_1_domains, config)
        except TimeoutError:
            _LOGGER.warning(
                "Setup timed out for stage 1 waiting on %s - moving forward",
                hass._active_tasks,  # noqa: SLF001
            )

    # Add after dependencies when setting up stage 2 domains
    async_set_domains_to_be_loaded(hass, stage_2_domains)

    if stage_2_domains:
        _LOGGER.info("Setting up stage 2: %s", stage_2_domains)
        try:
            async with hass.timeout.async_timeout(
                STAGE_2_TIMEOUT, cool_down=COOLDOWN_TIME
            ):
                await async_setup_multi_components(hass, stage_2_domains, config)
        except TimeoutError:
            _LOGGER.warning(
                "Setup timed out for stage 2 waiting on %s - moving forward",
                hass._active_tasks,  # noqa: SLF001
            )

    # Wrap up startup
    _LOGGER.debug("Waiting for startup to wrap up")
    try:
        async with hass.timeout.async_timeout(WRAP_UP_TIMEOUT, cool_down=COOLDOWN_TIME):
            await hass.async_block_till_done()
    except TimeoutError:
        _LOGGER.warning(
            "Setup timed out for bootstrap waiting on %s - moving forward",
            hass._active_tasks,  # noqa: SLF001
        )

    watcher.async_stop()

    if _LOGGER.isEnabledFor(logging.DEBUG):
        setup_time = async_get_setup_timings(hass)
        _LOGGER.debug(
            "Integration setup times: %s",
            dict(sorted(setup_time.items(), key=itemgetter(1), reverse=True)),
        )

</document_content>
</document>
</documents>
