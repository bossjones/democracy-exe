<documents>
<document index="1">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/exceptions/__init__.py</source>
<document_content>
from __future__ import annotations

from discord.ext import commands


class UserBlacklisted(commands.CheckFailure):
    """Thrown when a user is attempting something, but is blacklisted."""

    def __init__(self, message="User is blacklisted!"):
        self.message = message
        super().__init__(self.message)


class UserNotOwner(commands.CheckFailure):
    """Thrown when a user is attempting something, but is not an owner of the bot."""

    def __init__(self, message="User is not an owner of the bot!"):
        self.message = message
        super().__init__(self.message)


"""Custom exceptions for the sandbox agent."""


class SandboxAgentException(Exception):
    """Base exception for sandbox agent."""

    pass


class ModelNotFoundError(SandboxAgentException):
    """Raised when a requested model is not found."""

    pass


class ConfigurationError(SandboxAgentException):
    """Raised when there's an error in the configuration."""

    pass

</document_content>
</document>
<document index="2">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/exceptions/__init__.py</source>
<document_content>
from __future__ import annotations

from discord.ext import commands


class UserBlacklisted(commands.CheckFailure):
    """Thrown when a user is attempting something, but is blacklisted."""

    def __init__(self, message="User is blacklisted!"):
        self.message = message
        super().__init__(self.message)


class UserNotOwner(commands.CheckFailure):
    """Thrown when a user is attempting something, but is not an owner of the bot."""

    def __init__(self, message="User is not an owner of the bot!"):
        self.message = message
        super().__init__(self.message)


"""Custom exceptions for the sandbox agent."""


class SandboxAgentException(Exception):
    """Base exception for sandbox agent."""

    pass


class ModelNotFoundError(SandboxAgentException):
    """Raised when a requested model is not found."""

    pass


class ConfigurationError(SandboxAgentException):
    """Raised when there's an error in the configuration."""

    pass

</document_content>
</document>
<document index="3">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/factories/__init__.py</source>
<document_content>
"""Factories for the sandbox agent."""

# Import factories from other modules as needed
from __future__ import annotations

import dataclasses

from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional


# from democracy_exe.ai.chat_models import ChatModelFactory
# from democracy_exe.ai.document_loaders import DocumentLoaderFactory
# from democracy_exe.ai.embedding_models import EmbeddingModelFactory
# from democracy_exe.ai.evaluators import EvaluatorFactory
# from democracy_exe.ai.key_value_stores import KeyValueStoreFactory
# from democracy_exe.ai.memory import MemoryFactory
# from democracy_exe.ai.retrievers import RetrieverFactory
# from democracy_exe.ai.text_splitters import TextSplitterFactory
# from democracy_exe.ai.tools import ToolFactory
# from democracy_exe.ai.vector_stores import VectorStoreFactory


READ_ONLY = "read_only"
COERCE_TO = "coerce_to"


@dataclass
class SerializerFactory:
    def as_dict(self) -> dict:
        d: dict[str, Any] = {}
        for f in dataclasses.fields(self):
            value = getattr(self, f.name)
            if READ_ONLY not in f.metadata:
                if COERCE_TO in f.metadata:
                    value = f.metadata[COERCE_TO](value)
                d[f.name] = value
        return d

</document_content>
</document>
<document index="4">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/factories/cmd_factory.py</source>
<document_content>
"""democracy_exe.factories.cmd_factory."""

# pylint: disable=no-value-for-parameter
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Optional

from pydantic import ValidationError, validate_call

from democracy_exe.factories import SerializerFactory


# SOURCE: https://stackoverflow.com/questions/54863458/force-type-conversion-in-python-dataclass-init-method
@validate_call
@dataclass
class CmdSerializer(SerializerFactory):
    name: str
    cmd: str | None
    uri: str | None

    @staticmethod
    def create(d: dict) -> CmdSerializer:
        return CmdSerializer(name=d["name"])


# SOURCE: https://stackoverflow.com/questions/54863458/force-type-conversion-in-python-dataclass-init-method

</document_content>
</document>
<document index="5">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/factories/guild_factory.py</source>
<document_content>
"""guild_factory.py"""

from __future__ import annotations

from democracy_exe.aio_settings import aiosettings


# SOURCE: https://stackoverflow.com/a/63483209/814221
class Singleton(type):
    # Inherit from "type" in order to gain access to method __call__
    def __init__(self, *args, **kwargs):
        self.__instance = None  # Create a variable to store the object reference
        super().__init__(*args, **kwargs)

    def __call__(self, *args, **kwargs):
        if self.__instance is None:
            # if the object has not already been created
            self.__instance = super().__call__(
                *args, **kwargs
            )  # Call the __init__ method of the subclass (Spam) and save the reference
        return self.__instance


# SOURCE: https://stackoverflow.com/a/63483209/814221
class Guild(metaclass=Singleton):
    def __init__(self, id=aiosettings.discord_server_id, prefix=aiosettings.prefix):
        # print('Creating Guild')
        self.id = id
        self.prefix = prefix


# smoke tests
if __name__ == "__main__":
    test_guild_metadata = Guild(id=int(aiosettings.discord_server_id), prefix=aiosettings.prefix)
    print(test_guild_metadata)
    print(test_guild_metadata.id)
    print(test_guild_metadata.prefix)

    test_guild_metadata2 = Guild()
    print(test_guild_metadata2)
    print(test_guild_metadata2.id)
    print(test_guild_metadata2.prefix)

</document_content>
</document>
<document index="6">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/models/__init__.py</source>
<document_content>
"""Data models for the sandbox agent."""

# Define data models here
# For example:
# from pydantic import BaseModel
#
# class User(BaseModel):
#     id: int
#     name: str
#     email: str
#
# class Message(BaseModel):
#     id: int
#     user_id: int
#     content: str
#     timestamp: datetime

</document_content>
</document>
<document index="7">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/models/loggers/__init__.py</source>
<document_content>
"""democracy_exe.models.loggers"""

# pyright: reportAssignmentType=false
# pyright: strictParameterNoneValue=false
# pylint: disable=no-name-in-module

# SOURCE: https://blog.bartab.fr/fastapi-logging-on-the-fly/
from __future__ import annotations

from typing import Any, ForwardRef, List, Optional

from pydantic import BaseModel, Field


LoggerModel = ForwardRef("LoggerModel")


class LoggerPatch(BaseModel):
    name: str
    level: str


class LoggerModel(BaseModel):
    name: str
    level: int | None
    # children: Optional[List["LoggerModel"]] = None
    # fixes: https://github.com/samuelcolvin/pydantic/issues/545
    children: list[Any] | None = None
    # children: ListLoggerModel = None


LoggerModel.model_rebuild()

</document_content>
</document>
<document index="8">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/models/cmds/__init__.py</source>
<document_content>
"""democracy_exe.models.cmds"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Optional, Union

from democracy_exe import types


@dataclass
class CmdArgs:
    name: str


@dataclass
class DataCmd:
    name: str
    command_args: list[str] | None = []
    command_kargs: dict[str, str] = {}

</document_content>
</document>
<document index="9">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/models/vectorstores/__init__.py</source>
<document_content>
"""vector store models"""

from __future__ import annotations

from democracy_exe.models.vectorstores.chroma_input_model import ChromaIntegration
from democracy_exe.models.vectorstores.pgvector_input_model import PgvectorIntegration
from democracy_exe.models.vectorstores.pinecone_input_model import EmbeddingsProvider, PineconeIntegration

</document_content>
</document>
<document index="10">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/models/vectorstores/chroma_input_model.py</source>
<document_content>
# pyright: reportMissingTypeStubs=false
# pylint: disable=no-member
# pylint: disable=no-value-for-parameter
# generated by datamodel-codegen:
#   filename:  input_schema.json
#   timestamp: 2024-07-23T09:53:47+00:00

from __future__ import annotations

from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field, SecretStr
from pydantic_settings import BaseSettings, SettingsConfigDict

from democracy_exe.aio_settings import aiosettings


class EmbeddingsProvider(Enum):
    OpenAI = "OpenAI"
    Cohere = "Cohere"


class ChromaIntegration(BaseModel):
    class Config:
        arbitrary_types_allowed = True

    model_config = SettingsConfigDict(
        extra="ignore",
        arbitrary_types_allowed=True,
    )

    chromaCollectionName: str | None = Field(
        "chroma",
        description="Name of the chroma collection where the data will be stored",
        title="Chroma collection name",
    )
    chromaClientHost: str = Field(
        aiosettings.chroma_host, description="Host argument for Chroma HTTP Client", title="Chroma host"
    )
    chromaClientPort: int | None = Field(
        aiosettings.chroma_port, description="Port argument for Chroma HTTP Client", title="Chroma port"
    )
    chromaClientSsl: bool | None = Field(False, description="Enable/Disable SSL", title="Chroma SSL enabled")
    chromaServerAuthCredentials: str | None = Field(
        None,
        description="Chroma server Auth Static API token.",
        title="Chroma server Auth Static API token credentials",
    )
    chromaClientAuthProvider: str | None = Field(
        "chromadb.auth.token_authn.TokenAuthClientProvider",
        description="Chroma client auth provider",
        title="Chroma client auth provider",
    )
    embeddingsProvider: EmbeddingsProvider = Field(
        ...,
        description="Choose the embeddings provider to use for generating embeddings",
        title="Embeddings provider (as defined in the langchain API)",
    )
    embeddingsConfig: dict[str, Any] | None = Field(
        None,
        description='Configure the parameters for the LangChain embedding class. Key points to consider:\n\n1. Typically, you only need to specify the model name. For example, for OpenAI, set the model name as {"model": "text-embedding-3-small"}.\n\n2. It\'s crucial to ensure that the vector size of your embeddings matches the size of embeddings in the database.\n\n3. Here are some examples of embedding models:\n   - [OpenAI](https://platform.openai.com/docs/guides/embeddings): `text-embedding-3-small`, `text-embedding-3-large`, etc.\n   - [Cohere](https://docs.cohere.com/docs/cohere-embed): `embed-english-v3.0`, `embed-multilingual-light-v3.0`, etc.\n\n4. For more details about other parameters, refer to the [LangChain documentation](https://python.langchain.com/v0.2/docs/integrations/text_embedding/).',
        title="Configuration for embeddings provider",
    )
    embeddingsApiKey: SecretStr = Field(
        aiosettings.openai_api_key.get_secret_value(),  # pylint: disable=no-member
        description="Value of the API KEY for the embeddings provider (if required).\n\n For example for OpenAI it is OPENAI_API_KEY, for Cohere it is COHERE_API_KEY)",
        title="Embeddings API KEY (whenever applicable, depends on provider)",
    )
    datasetFields: list = Field(
        ...,
        description="This array specifies the dataset fields to be selected and stored in the vector store. Only the fields listed here will be included in the vector store.\n\nFor instance, when using the Website Content Crawler, you might choose to include fields such as `text`, `url`, and `metadata.title` in the vector store.",
        title="Dataset fields to select from the dataset results and store in the database",
    )
    metadataDatasetFields: dict[str, Any] | None = Field(
        None,
        description='A list of dataset fields which should be selected from the dataset and stored as metadata in the vector stores.\n\nFor example, when using the Website Content Crawler, you might want to store `url` in metadata. In this case, use `metadataDatasetFields parameter as follows {"url": "url"}`',
        title="Dataset fields to select from the dataset and store as metadata in the database",
    )
    metadataObject: dict[str, Any] | None = Field(
        None,
        description='This object allows you to store custom metadata for every item in the vector store.\n\nFor example, if you want to store the `domain` as metadata, use the `metadataObject` like this: {"domain": "apify.com"}.',
        title="Custom object to be stored as metadata in the vector store database",
    )
    datasetId: str | None = Field(
        None,
        description="Dataset ID (when running standalone without integration)",
        title="Dataset ID",
    )
    enableDeltaUpdates: bool | None = Field(
        True,
        description="When set to true, this setting enables incremental updates for objects in the database by comparing the changes (deltas) between the crawled dataset items and the existing objects, uniquely identified by the `datasetKeysToItemId` field.\n\n The integration will only add new objects and update those that have changed, reducing unnecessary updates. The `datasetFields`, `metadataDatasetFields`, and `metadataObject` fields are used to determine the changes.",
        title="Enable incremental updates for objects based on deltas",
    )
    deltaUpdatesPrimaryDatasetFields: list[str] | None = Field(
        ["url"],
        description="This array contains fields that are used to uniquely identify dataset items, which helps to handle content changes across different runs.\n\nFor instance, in a web content crawling scenario, the `url` field could serve as a unique identifier for each item.",
        title="Dataset fields to uniquely identify dataset items (only relevant when `enableDeltaUpdates` is enabled)",
    )
    deleteExpiredObjects: bool | None = Field(
        True,
        description="When set to true, delete objects from the database that have not been crawled for a specified period.",
        title="Delete expired objects from the database",
    )
    expiredObjectDeletionPeriodDays: int | None = Field(
        30,
        description="This setting allows the integration to manage the deletion of objects from the database that have not been crawled for a specified period. It is typically used in subsequent runs after the initial crawl.\n\nWhen the value is greater than 0, the integration checks if objects have been seen within the last X days (determined by the expiration period). If the objects are expired, they are deleted from the database. The specific value for `deletedExpiredObjectsDays` depends on your use case and how frequently you crawl data.\n\nFor example, if you crawl data daily, you can set `deletedExpiredObjectsDays` to 7 days. If you crawl data weekly, you can set `deletedExpiredObjectsDays` to 30 days.",
        ge=0,
        title="Delete expired objects from the database after a specified number of days",
    )
    performChunking: bool | None = Field(
        False,
        description="When set to true, the text will be divided into smaller chunks based on the settings provided below. Proper chunking helps optimize retrieval and ensures accurate and efficient responses.",
        title="Enable text chunking",
    )
    chunkSize: int | None = Field(
        1000,
        description="Defines the maximum number of characters in each text chunk. Choosing the right size balances between detailed context and system performance. Optimal sizes ensure high relevancy and minimal response time.",
        ge=1,
        title="Maximum chunk size",
    )
    chunkOverlap: int | None = Field(
        0,
        description="Specifies the number of overlapping characters between consecutive text chunks. Adjusting this helps maintain context across chunks, which is crucial for accuracy in retrieval-augmented generation systems.",
        ge=0,
        title="Chunk overlap",
    )

</document_content>
</document>
<document index="11">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/models/vectorstores/pgvector_input_model.py</source>
<document_content>
# pyright: reportMissingTypeStubs=false
# pylint: disable=no-member
# pylint: disable=no-value-for-parameter
# generated by datamodel-codegen:
#   filename:  input_schema.json
#   timestamp: 2024-07-23T09:53:48+00:00

from __future__ import annotations

from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field, SecretStr
from pydantic_settings import BaseSettings, SettingsConfigDict

from democracy_exe.aio_settings import aiosettings


class EmbeddingsProvider(Enum):
    OpenAI = "OpenAI"
    Cohere = "Cohere"


class PgvectorIntegration(BaseModel):
    class Config:
        arbitrary_types_allowed = True

    model_config = SettingsConfigDict(
        extra="ignore",
        arbitrary_types_allowed=True,
    )
    postgresSqlConnectionStr: str = Field(
        aiosettings.postgres_url,
        description="Connection string for the Postgres SQL database in the format `postgresql://user:password@host:port/database`",
        title="Postgres SQL connection string",
    )
    postgresCollectionName: str = Field(
        ...,
        description="The name of the collection to use. NOTE: This is not the name of the table, but the name of the collection",
        title="Postgres SQL collection name",
    )
    embeddingsProvider: EmbeddingsProvider = Field(
        ...,
        description="Choose the embeddings provider to use for generating embeddings",
        title="Embeddings provider (as defined in the langchain API)",
    )
    embeddingsConfig: dict[str, Any] | None = Field(
        None,
        description='Configure the parameters for the LangChain embedding class. Key points to consider:\n\n1. Typically, you only need to specify the model name. For example, for OpenAI, set the model name as {"model": "text-embedding-3-small"}.\n\n2. It\'s crucial to ensure that the vector size of your embeddings matches the size of embeddings in the database.\n\n3. Here are some examples of embedding models:\n   - [OpenAI](https://platform.openai.com/docs/guides/embeddings): `text-embedding-3-small`, `text-embedding-3-large`, etc.\n   - [Cohere](https://docs.cohere.com/docs/cohere-embed): `embed-english-v3.0`, `embed-multilingual-light-v3.0`, etc.\n\n4. For more details about other parameters, refer to the [LangChain documentation](https://python.langchain.com/v0.2/docs/integrations/text_embedding/).',
        title="Configuration for embeddings provider",
    )
    embeddingsApiKey: SecretStr = Field(
        aiosettings.openai_api_key.get_secret_value(),
        description="Value of the API KEY for the embeddings provider (if required).\n\n For example for OpenAI it is OPENAI_API_KEY, for Cohere it is COHERE_API_KEY)",
        title="Embeddings API KEY (whenever applicable, depends on provider)",
    )
    datasetFields: list = Field(
        ...,
        description="This array specifies the dataset fields to be selected and stored in the vector store. Only the fields listed here will be included in the vector store.\n\nFor instance, when using the Website Content Crawler, you might choose to include fields such as `text`, `url`, and `metadata.title` in the vector store.",
        title="Dataset fields to select from the dataset results and store in the database",
    )
    metadataDatasetFields: dict[str, Any] | None = Field(
        None,
        description='A list of dataset fields which should be selected from the dataset and stored as metadata in the vector stores.\n\nFor example, when using the Website Content Crawler, you might want to store `url` in metadata. In this case, use `metadataDatasetFields parameter as follows {"url": "url"}`',
        title="Dataset fields to select from the dataset and store as metadata in the database",
    )
    metadataObject: dict[str, Any] | None = Field(
        None,
        description='This object allows you to store custom metadata for every item in the vector store.\n\nFor example, if you want to store the `domain` as metadata, use the `metadataObject` like this: {"domain": "apify.com"}.',
        title="Custom object to be stored as metadata in the vector store database",
    )
    datasetId: str | None = Field(
        None,
        description="Dataset ID (when running standalone without integration)",
        title="Dataset ID",
    )
    enableDeltaUpdates: bool | None = Field(
        True,
        description="When set to true, this setting enables incremental updates for objects in the database by comparing the changes (deltas) between the crawled dataset items and the existing objects, uniquely identified by the `datasetKeysToItemId` field.\n\n The integration will only add new objects and update those that have changed, reducing unnecessary updates. The `datasetFields`, `metadataDatasetFields`, and `metadataObject` fields are used to determine the changes.",
        title="Enable incremental updates for objects based on deltas",
    )
    deltaUpdatesPrimaryDatasetFields: list[str] | None = Field(
        ["url"],
        description="This array contains fields that are used to uniquely identify dataset items, which helps to handle content changes across different runs.\n\nFor instance, in a web content crawling scenario, the `url` field could serve as a unique identifier for each item.",
        title="Dataset fields to uniquely identify dataset items (only relevant when `enableDeltaUpdates` is enabled)",
    )
    deleteExpiredObjects: bool | None = Field(
        True,
        description="When set to true, delete objects from the database that have not been crawled for a specified period.",
        title="Delete expired objects from the database",
    )
    expiredObjectDeletionPeriodDays: int | None = Field(
        30,
        description="This setting allows the integration to manage the deletion of objects from the database that have not been crawled for a specified period. It is typically used in subsequent runs after the initial crawl.\n\nWhen the value is greater than 0, the integration checks if objects have been seen within the last X days (determined by the expiration period). If the objects are expired, they are deleted from the database. The specific value for `deletedExpiredObjectsDays` depends on your use case and how frequently you crawl data.\n\nFor example, if you crawl data daily, you can set `deletedExpiredObjectsDays` to 7 days. If you crawl data weekly, you can set `deletedExpiredObjectsDays` to 30 days.",
        ge=0,
        title="Delete expired objects from the database after a specified number of days",
    )
    performChunking: bool | None = Field(
        False,
        description="When set to true, the text will be divided into smaller chunks based on the settings provided below. Proper chunking helps optimize retrieval and ensures accurate and efficient responses.",
        title="Enable text chunking",
    )
    chunkSize: int | None = Field(
        1000,
        description="Defines the maximum number of characters in each text chunk. Choosing the right size balances between detailed context and system performance. Optimal sizes ensure high relevancy and minimal response time.",
        ge=1,
        title="Maximum chunk size",
    )
    chunkOverlap: int | None = Field(
        0,
        description="Specifies the number of overlapping characters between consecutive text chunks. Adjusting this helps maintain context across chunks, which is crucial for accuracy in retrieval-augmented generation systems.",
        ge=0,
        title="Chunk overlap",
    )

</document_content>
</document>
<document index="12">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/models/vectorstores/pinecone_input_model.py</source>
<document_content>
# pyright: reportMissingTypeStubs=false
# pylint: disable=no-member
# pylint: disable=no-value-for-parameter
# generated by datamodel-codegen:
#   filename:  input_schema.json
#   timestamp: 2024-07-23T09:53:49+00:00

from __future__ import annotations

from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field, SecretStr
from pydantic_settings import BaseSettings, SettingsConfigDict

from democracy_exe.aio_settings import aiosettings


class EmbeddingsProvider(Enum):
    OpenAI = "OpenAI"
    Cohere = "Cohere"


class PineconeIntegration(BaseModel):
    class Config:
        arbitrary_types_allowed = True

    model_config = SettingsConfigDict(
        extra="ignore",
        arbitrary_types_allowed=True,
    )
    pineconeApiKey: SecretStr = Field(
        aiosettings.pinecone_api_key.get_secret_value(), description="Pinecone API KEY", title="Pinecone API KEY"
    )
    pineconeIndexName: str = Field(
        ...,
        description="Name of the Pinecone index where the data will be stored",
        title="Pinecone index name",
    )
    embeddingsProvider: EmbeddingsProvider = Field(
        ...,
        description="Choose the embeddings provider to use for generating embeddings",
        title="Embeddings provider (as defined in the langchain API)",
    )
    embeddingsConfig: dict[str, Any] | None = Field(
        None,
        description='Configure the parameters for the LangChain embedding class. Key points to consider:\n\n1. Typically, you only need to specify the model name. For example, for OpenAI, set the model name as {"model": "text-embedding-3-small"}.\n\n2. It\'s crucial to ensure that the vector size of your embeddings matches the size of embeddings in the database.\n\n3. Here are some examples of embedding models:\n   - [OpenAI](https://platform.openai.com/docs/guides/embeddings): `text-embedding-3-small`, `text-embedding-3-large`, etc.\n   - [Cohere](https://docs.cohere.com/docs/cohere-embed): `embed-english-v3.0`, `embed-multilingual-light-v3.0`, etc.\n\n4. For more details about other parameters, refer to the [LangChain documentation](https://python.langchain.com/v0.2/docs/integrations/text_embedding/).',
        title="Configuration for embeddings provider",
    )
    embeddingsApiKey: SecretStr = Field(
        aiosettings.openai_api_key.get_secret_value(),
        description="Value of the API KEY for the embeddings provider (if required).\n\n For example for OpenAI it is OPENAI_API_KEY, for Cohere it is COHERE_API_KEY)",
        title="Embeddings API KEY (whenever applicable, depends on provider)",
    )
    datasetFields: list = Field(
        ...,
        description="This array specifies the dataset fields to be selected and stored in the vector store. Only the fields listed here will be included in the vector store.\n\nFor instance, when using the Website Content Crawler, you might choose to include fields such as `text`, `url`, and `metadata.title` in the vector store.",
        title="Dataset fields to select from the dataset results and store in the database",
    )
    metadataDatasetFields: dict[str, Any] | None = Field(
        None,
        description='A list of dataset fields which should be selected from the dataset and stored as metadata in the vector stores.\n\nFor example, when using the Website Content Crawler, you might want to store `url` in metadata. In this case, use `metadataDatasetFields parameter as follows {"url": "url"}`',
        title="Dataset fields to select from the dataset and store as metadata in the database",
    )
    metadataObject: dict[str, Any] | None = Field(
        None,
        description='This object allows you to store custom metadata for every item in the vector store.\n\nFor example, if you want to store the `domain` as metadata, use the `metadataObject` like this: {"domain": "apify.com"}.',
        title="Custom object to be stored as metadata in the vector store database",
    )
    datasetId: str | None = Field(
        None,
        description="Dataset ID (when running standalone without integration)",
        title="Dataset ID",
    )
    enableDeltaUpdates: bool | None = Field(
        True,
        description="When set to true, this setting enables incremental updates for objects in the database by comparing the changes (deltas) between the crawled dataset items and the existing objects, uniquely identified by the `datasetKeysToItemId` field.\n\n The integration will only add new objects and update those that have changed, reducing unnecessary updates. The `datasetFields`, `metadataDatasetFields`, and `metadataObject` fields are used to determine the changes.",
        title="Enable incremental updates for objects based on deltas",
    )
    deltaUpdatesPrimaryDatasetFields: list[str] | None = Field(
        ["url"],
        description="This array contains fields that are used to uniquely identify dataset items, which helps to handle content changes across different runs.\n\nFor instance, in a web content crawling scenario, the `url` field could serve as a unique identifier for each item.",
        title="Dataset fields to uniquely identify dataset items (only relevant when `enableDeltaUpdates` is enabled)",
    )
    deleteExpiredObjects: bool | None = Field(
        True,
        description="When set to true, delete objects from the database that have not been crawled for a specified period.",
        title="Delete expired objects from the database",
    )
    expiredObjectDeletionPeriodDays: int | None = Field(
        30,
        description="This setting allows the integration to manage the deletion of objects from the database that have not been crawled for a specified period. It is typically used in subsequent runs after the initial crawl.\n\nWhen the value is greater than 0, the integration checks if objects have been seen within the last X days (determined by the expiration period). If the objects are expired, they are deleted from the database. The specific value for `deletedExpiredObjectsDays` depends on your use case and how frequently you crawl data.\n\nFor example, if you crawl data daily, you can set `deletedExpiredObjectsDays` to 7 days. If you crawl data weekly, you can set `deletedExpiredObjectsDays` to 30 days.",
        ge=0,
        title="Delete expired objects from the database after a specified number of days",
    )
    performChunking: bool | None = Field(
        False,
        description="When set to true, the text will be divided into smaller chunks based on the settings provided below. Proper chunking helps optimize retrieval and ensures accurate and efficient responses.",
        title="Enable text chunking",
    )
    chunkSize: int | None = Field(
        1000,
        description="Defines the maximum number of characters in each text chunk. Choosing the right size balances between detailed context and system performance. Optimal sizes ensure high relevancy and minimal response time.",
        ge=1,
        title="Maximum chunk size",
    )
    chunkOverlap: int | None = Field(
        0,
        description="Specifies the number of overlapping characters between consecutive text chunks. Adjusting this helps maintain context across chunks, which is crucial for accuracy in retrieval-augmented generation systems.",
        ge=0,
        title="Chunk overlap",
    )

</document_content>
</document>
<document index="13">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/services/base_service.py</source>
<document_content>
"""Base service class for all services."""
from __future__ import annotations

import logging

from typing import Any, Optional

import structlog


logger = structlog.get_logger(__name__)


class BaseService:
    """Base class for all services."""

    def __init__(self, bot: Any = None, service_logger: logging.Logger | None = None) -> None:
        """Initialize the service.

        Args:
            bot: The Discord bot instance
            logger: Optional logger instance
        """
        self.bot = bot
        self._logger = service_logger or logger.bind(service=self.__class__.__name__)

    @property
    def logger(self) -> logging.Logger:
        """Get the logger instance."""
        return self._logger

</document_content>
</document>
<document index="14">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/services/twitter_service.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
"""Twitter service for handling Twitter-related operations."""
from __future__ import annotations

import asyncio
import pathlib
import tempfile

from typing import List, Optional, Tuple

import aiohttp
import discord
import structlog


logger = structlog.get_logger(__name__)

from democracy_exe.services.base_service import BaseService
from democracy_exe.utils import file_functions
from democracy_exe.utils.events import aio_create_thumbnail_attachment


class TwitterService(BaseService):
    """Service for handling Twitter operations."""

    async def download_media(self, url: str, download_dir: str) -> tuple[list[str], list[str]]:
        """Download media from Twitter URL.

        Args:
            url: Twitter URL to download from
            download_dir: Directory to download files to

        Returns:
            Tuple of (media files, json files)
        """
        # Download logic here
        pass

    async def process_attachments(
        self,
        message: discord.Message,
        download_dir: str
    ) -> tuple[list[str], list[str]]:
        """Process message attachments.

        Args:
            message: Discord message with attachments
            download_dir: Directory to save attachments to

        Returns:
            Tuple of (media files, json files)
        """
        media_files = []
        json_files = []

        for attachment in message.attachments:
            file_path = pathlib.Path(download_dir) / attachment.filename
            await attachment.save(file_path)

            if str(file_path) in file_functions.filter_media([str(file_path)]):
                media_files.append(str(file_path))
            elif file_path.suffix.lower() in file_functions.JSON_EXTENSIONS:
                json_files.append(str(file_path))

        return media_files, json_files

    async def create_embed(
        self,
        url: str,
        channel: discord.TextChannel,
        json_data: dict,
        is_dropbox: bool = False
    ) -> discord.Embed:
        """Create Twitter embed message.

        Args:
            url: Original Twitter URL
            channel: Discord channel
            json_data: Tweet metadata
            is_dropbox: Whether content was uploaded to Dropbox

        Returns:
            Discord embed object
        """
        # Embed creation logic here
        pass

</document_content>
</document>
<document index="15">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/shell/__init__.py</source>
<document_content>
#!/usr/bin/env python
"""democracy_exe.shell module containing utility functions for running shell commands."""

from __future__ import annotations

import asyncio
import os
import pathlib
import subprocess
import sys
import time

from asyncio import Lock, Semaphore
from asyncio.subprocess import Process
from contextlib import asynccontextmanager
from pathlib import Path
from typing import List, Tuple, Union

import uritools

from codetiming import Timer


HOME_PATH = os.environ.get("HOME")


async def _aio_run_process_and_communicate(cmd: list[str], cwd: str | None = None) -> str:
    """
    Run a command asynchronously using asyncio.create_subprocess_exec and communicate.

    Args:
        cmd (List[str]): The command to run as a list of strings.
        cwd (Union[str, None], optional): The working directory to run the command in. Defaults to None.

    Returns:
        str: The decoded stdout output of the command.
    """
    program = cmd
    process: Process = await asyncio.create_subprocess_exec(*program, stdout=asyncio.subprocess.PIPE, cwd=cwd)
    print(f"Process pid is: {process.pid}")
    stdout, stderr = await process.communicate()
    return stdout.decode("utf-8").strip()


def _stat_y_file(fname: str, env: dict = None, cwd: str | None = None) -> str:
    """
    Get the timestamp of a file using the 'stat' command.

    Args:
        fname (str): The name of the file to get the timestamp for.
        env (dict, optional): Environment variables to use when running the command. Defaults to None.
        cwd (Union[str, None], optional): The working directory to run the command in. Defaults to None.

    Returns:
        str: The timestamp of the file.
    """
    if env is None:
        env = {}
    cmd_arg_without_str_fmt = f"""stat -c %y {fname}"""
    print(f"cmd_arg_without_str_fmt={cmd_arg_without_str_fmt}")

    cmd_arg = rf"""{cmd_arg_without_str_fmt}"""
    print(f"cmd_arg={cmd_arg}")
    try:
        result = subprocess.run(
            [cmd_arg],
            stdout=subprocess.PIPE,
            stderr=subprocess.DEVNULL,
            text=True,
            shell=True,
            check=False,
        )
        timestamp = result.stdout.replace("\n", "")
        print(f"timestamp={timestamp}")
    except subprocess.CalledProcessError as e:
        print(e.output)
    return timestamp


def _popen(cmd_arg: tuple, env: dict = None, cwd: str | None = None) -> bytes:
    """
    Run a command using subprocess.Popen and read the stdout output.

    Args:
        cmd_arg (Tuple): The command to run as a tuple of arguments.
        env (dict, optional): Environment variables to use when running the command. Defaults to None.
        cwd (Union[str, None], optional): The working directory to run the command in. Defaults to None.

    Raises:
        RuntimeError: If there was an error closing the command stream.

    Returns:
        bytes: The stdout output of the command.
    """
    if env is None:
        env = {}
    with open("/dev/null") as devnull:
        with subprocess.Popen(cmd_arg, stdout=subprocess.PIPE, stderr=devnull, env=env, cwd=cwd) as cmd:
            retval = cmd.stdout.read().strip()
            err = cmd.wait()
        if err:
            raise RuntimeError(f"Failed to close {cmd_arg} stream")
    return retval


def _popen_communicate(cmd_arg: tuple, env: dict = None, cwd: str | None = None) -> bytes:
    """
    Run a command using subprocess.Popen, wait for it to finish, and read the stdout output.

    Args:
        cmd_arg (Tuple): The command to run as a tuple of arguments.
        env (dict, optional): Environment variables to use when running the command. Defaults to None.
        cwd (Union[str, None], optional): The working directory to run the command in. Defaults to None.

    Raises:
        RuntimeError: If there was an error closing the command stream.

    Returns:
        bytes: The stdout output of the command.
    """
    if env is None:
        env = {}
    # devnull = open("/dev/null")
    with subprocess.Popen(cmd_arg, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env, cwd=cwd) as cmd:
        try:
            time.sleep(0.2)
            retval = cmd.stdout.read().strip()
            err = cmd.wait()

        finally:
            cmd.terminate()
            try:
                outs, _ = cmd.communicate(timeout=0.2)
                print("== subprocess exited with rc =", cmd.returncode)
                print(outs.decode("utf-8"))
            except subprocess.TimeoutExpired:
                print("subprocess did not terminate in time")

    if err:
        raise RuntimeError(f"Failed to close {cmd_arg} stream")
    return retval


# SOURCE: https://github.com/ARMmbed/mbed-cli/blob/f168237fabd0e32edcb48e214fc6ce2250046ab3/test/util.py
# Process execution
class ProcessException(Exception):
    """Exception raised when there is an error running a process."""


class ShellConsole:  # pylint: disable=too-few-public-methods
    """Utility class for printing messages to the console."""

    quiet = False

    @classmethod
    def message(cls, str_format: str, *args: any) -> None:
        """
        Print a message to the console if quiet mode is not enabled.

        Args:
            str_format (str): The message format string.
            *args (any): Arguments to substitute into the format string.
        """
        if cls.quiet:
            return

        if args:
            print(str_format % args)
        else:
            print(str_format)

        # Flush so that messages are printed at the right time
        # as we use many subprocesses.
        sys.stdout.flush()


def pquery(command: str | list, stdin: bool = None, **kwargs: any) -> str:
    """
    Run a command using subprocess.Popen and return the decoded stdout output.

    Args:
        command (Union[str, list]): The command to run as a string or list of arguments.
        stdin (bool, optional): Whether to enable stdin for the process. Defaults to None.
        **kwargs (any): Additional keyword arguments to pass to subprocess.Popen.

    Raises:
        ProcessException: If there was an error running the command.

    Returns:
        str: The decoded stdout output of the command.
    """
    # SOURCE: https://github.com/ARMmbed/mbed-cli/blob/f168237fabd0e32edcb48e214fc6ce2250046ab3/test/util.py
    # Example:
    if type(command) == list:
        print(" ".join(command))
    elif type(command) == str:
        command = command.split(" ")
        print(f"cmd: {command}")
    try:
        with subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, **kwargs) as proc:
            stdout, _ = proc.communicate(stdin)
    except Exception:
        print("[fail] cmd={command}, ret={proc.returncode}")
        raise

    if proc.returncode != 0:
        raise ProcessException(proc.returncode)

    return stdout.decode("utf-8")


def _popen_stdout(cmd_arg: str, cwd: str | None = None) -> None:
    """
    Run a command using subprocess.Popen and print the stdout output line by line.

    Args:
        cmd_arg (str): The command to run as a string.
        cwd (Union[str, None], optional): The working directory to run the command in. Defaults to None.
    """
    # if passing a single string, either shell mut be True or else the string must simply name the program to be executed without specifying any arguments
    with subprocess.Popen(
        cmd_arg,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        cwd=cwd,
        bufsize=4096,
        shell=True,
    ) as cmd:
        ShellConsole.message(f"BEGIN: {cmd_arg}")
        # output, err = cmd.communicate()

        for line in iter(cmd.stdout.readline, b""):
            # Print line
            _line = line.rstrip()
            ShellConsole.message(f'>>> {_line.decode("utf-8")}')

        ShellConsole.message(f"END: {cmd_arg}")
        # subprocess.CompletedProcess(args=cmd_arg, returncode=0)


# Global lock for synchronous operations
_SYNC_LOCK = asyncio.Lock()

def _popen_stdout_lock(cmd_arg: str, cwd: str | None = None) -> None:
    """
    Run a command using subprocess.Popen with a lock and print the stdout output line by line.

    Args:
        cmd_arg (str): The command to run as a string.
        cwd (Union[str, None], optional): The working directory to run the command in. Defaults to None.
    """
    # if passing a single string, either shell mut be True or else the string must simply name the program to be executed without specifying any arguments
    with subprocess.Popen(
        cmd_arg,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        cwd=cwd,
        bufsize=4096,
        shell=True,
    ) as cmd:
        ShellConsole.message(f"BEGIN: {cmd_arg}")
        # output, err = cmd.communicate()

        for line in iter(cmd.stdout.readline, b""):
            # Print line
            _line = line.rstrip()
            ShellConsole.message(f'>>> {_line.decode("utf-8")}')

        ShellConsole.message(f"END: {cmd_arg}")
        subprocess.CompletedProcess(args=cmd_arg, returncode=0)


# Global synchronization primitives
_SHELL_SEMAPHORE = Semaphore(1)
_SHELL_LOCK = Lock()

@asynccontextmanager
async def _acquire_shell_lock():
    """Context manager that acquires both semaphore and lock for shell operations."""
    async with _SHELL_SEMAPHORE:
        async with _SHELL_LOCK:
            yield

async def run_coroutine_subprocess(cmd: str, uri: str, working_dir: str | None = None) -> str:
    """
    Run a command as a coroutine subprocess using asyncio.create_subprocess_shell.

    Uses a semaphore to prevent concurrent executions of fast commands.

    Args:
        cmd (str): The command to run as a string.
        uri (str): The URI associated with the command.
        working_dir (str | None, optional): The working directory to run the command in. Defaults to the current directory.

    Returns:
        str: The decoded stdout output of the command.
    """
    async with _acquire_shell_lock():
        if working_dir is None:
            working_dir = f"{pathlib.Path('./').absolute()}"

        # Sleep moved inside the protected section
        await asyncio.sleep(0.05)

    timer = Timer(text=f"Task {__name__} elapsed time: {{:.1f}}")

    env = dict(os.environ)
    dl_uri = uritools.urisplit(uri)

    result = "0"
    cmd = f"{cmd}"

    timer.start()
    process = await asyncio.create_subprocess_shell(
        cmd,
        env=env,
        cwd=working_dir,
        stdin=asyncio.subprocess.PIPE,
        stdout=asyncio.subprocess.PIPE,
        stderr=None,
    )
    stdout, stderr = await process.communicate()
    result = stdout.decode("utf-8").strip()
    timer.stop()
    return result

</document_content>
</document>
<document index="16">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/subcommands/__init__.py</source>
<document_content>

</document_content>
</document>
<document index="17">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/subcommands/dummy_cmd.py</source>
<document_content>
"""Generate dspy.Signatures"""

from __future__ import annotations

import asyncio

import typer

from democracy_exe.asynctyper import AsyncTyper, AsyncTyperImproved


# APP = AsyncTyper(help="dummy command")
APP = AsyncTyperImproved(help="dummy command")


@APP.command("dummy")
def cli_dummy_cmd(prompt: str):
    """Generate a new dspy.Module. Example: dspygen sig new 'text -> summary'"""
    return f"dummy cmd: {prompt}"


@APP.command()
async def aio_cli_dummy_cmd() -> str:
    """Returns information about the bot."""
    await asyncio.sleep(1)
    return "slept for 1 second"


if __name__ == "__main__":
    APP()

</document_content>
</document>
<document index="18">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/__init__.py</source>
<document_content>
"""democracy_exe.utils"""
# pyright: reportGeneralTypeIssues=false
# pyright: reportOperatorIssue=false
# pyright: reportOptionalIterable=false

# NOTE: Via Red https://github.com/Cog-Creators/Red-DiscordBot/tree/V3/develop/redbot
from __future__ import annotations

import asyncio
import contextlib
import inspect
import json
import logging
import os
import time
import warnings

from asyncio import Semaphore, as_completed
from asyncio.futures import isfuture
from collections.abc import AsyncIterable, AsyncIterator, Awaitable, Callable, Coroutine, Generator, Iterable, Iterator
from importlib.util import find_spec
from itertools import chain
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, NoReturn, Optional, Tuple, TypeVar, Union

import structlog

from discord.utils import maybe_coroutine

from democracy_exe import constants
from democracy_exe.aio_settings import aiosettings
from democracy_exe.types import CoroType, FuncType, TypeGuard


logger = structlog.get_logger(__name__)


if TYPE_CHECKING:
    from typing import TypeGuard

_T = TypeVar("_T")

_S = TypeVar("_S")


def _env_bool(key: str) -> bool:
    return os.environ.get(key, "").lower() in {"1", "t", "true"}


class _NoneType:  # pyright: ignore[reportUnusedClass]
    def __bool__(self) -> bool:
        return False


def time_since(start: float, precision: int = 4) -> str:
    # TODO: prettier output
    delta = round(time.monotonic() - start, precision)
    return f"{delta}s"


def maybe_async_run(
    func: FuncType | CoroType,
    *args: Any,
    **kwargs: Any,
) -> object:
    if is_coroutine(func):
        return async_run(func(*args, **kwargs))
    return func(*args, **kwargs)


def async_run(coro: Coroutine[Any, Any, _T]) -> _T:
    """Execute the coroutine and return the result."""
    return get_or_create_event_loop().run_until_complete(coro)


def is_coroutine(obj: Any) -> TypeGuard[CoroType]:
    return asyncio.iscoroutinefunction(obj) or inspect.isgeneratorfunction(obj)


def module_exists(name: str) -> bool:
    return find_spec(name) is not None


@contextlib.contextmanager
def temp_env_update(env: dict[str, str]) -> Iterator[None]:
    old = os.environ.copy()

    try:
        os.environ |= env
        yield
    finally:
        for key in env:
            os.environ.pop(key, None)

        os.environ |= old


# @contextlib.contextmanager
# def cxt_monkeypatch(obj: Any, attr: str, new: Any) -> Any:
#     """
#     Temporarily replace a method with a new funtion

#     The previously set method is passed as the first argument to the new function
#     """

#     def patched(*args: Any, **kwargs: Any) -> Any:
#         return new(old, *args, **kwargs)

#     old = getattr(obj, attr)

#     try:
#         setattr(obj, attr, patched)
#         yield
#     finally:
#         setattr(obj, attr, old)


def get_or_create_event_loop() -> asyncio.AbstractEventLoop:
    """
    Return the currently set event loop or create a new event loop if there
    is no set event loop.

    Starting from python3.10, asyncio.get_event_loop() raises a DeprecationWarning
    when there is no event loop set, this deprecation will be enforced starting from
    python3.12

    This function serves as a future-proof wrapper over asyncio.get_event_loop()
    that preserves the old behaviour.
    """
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=DeprecationWarning)

        try:
            return asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            return loop


def assert_never(value: NoReturn) -> NoReturn:
    """
    Used by type checkers for exhaustive match cases.

    https://github.com/microsoft/pyright/issues/767
    """
    raise AssertionError(f"Unhandled type: {type(value).__name__}")


def make_optional(value: _T) -> _T | None:
    """
    Helper function for type checkers to change the given type to include None.

    This is useful in cases where you do not have an explicit type for a symbol (e.g. modules)
    but want to mark it as potentially None.
    """
    return value


def is_dict(obj: object) -> TypeGuard[dict[object, object]]:
    return isinstance(obj, dict)


def deduplicate_iterables(*iterables) -> list[int | Any]:
    """
    Returns a list of all unique items in ``iterables``, in the order they
    were first encountered.
    """
    # dict insertion order is guaranteed to be preserved in 3.6+
    return list(dict.fromkeys(chain.from_iterable(iterables)))


# https://github.com/PyCQA/pylint/issues/2717
class AsyncFilter(AsyncIterator[_T], Awaitable[list[_T]]):  # pylint: disable=duplicate-bases
    """
    Class returned by `async_filter`. See that function for details.

    We don't recommend instantiating this class directly.
    """

    def __init__(
        self,
        func: Callable[[_T], bool | Awaitable[bool]],
        iterable: AsyncIterable[_T] | Iterable[_T],
    ) -> None:
        self.__func: Callable[[_T], bool | Awaitable[bool]] = func
        self.__iterable: AsyncIterable[_T] | Iterable[_T] = iterable

        # We assign the generator strategy based on the arguments' types
        if isinstance(iterable, AsyncIterable):
            if asyncio.iscoroutinefunction(func):
                self.__generator_instance = self.__async_generator_async_pred()
            else:
                self.__generator_instance = self.__async_generator_sync_pred()
        elif asyncio.iscoroutinefunction(func):
            self.__generator_instance = self.__sync_generator_async_pred()
        else:
            raise TypeError("Must be either an async predicate, an async iterable, or both.")

    async def __sync_generator_async_pred(self) -> AsyncIterator[_T]:
        for item in self.__iterable:
            if await self.__func(item):
                yield item

    async def __async_generator_sync_pred(self) -> AsyncIterator[_T]:
        async for item in self.__iterable:
            if self.__func(item):
                yield item

    async def __async_generator_async_pred(self) -> AsyncIterator[_T]:
        async for item in self.__iterable:
            if await self.__func(item):
                yield item

    async def __flatten(self) -> list[_T]:
        return [item async for item in self]

    def __aiter__(self):
        return self

    def __await__(self):
        # Simply return the generator filled into a list
        return self.__flatten().__await__()

    def __anext__(self) -> Awaitable[_T]:
        # This will use the generator strategy set in __init__
        return self.__generator_instance.__anext__()


def async_filter(
    func: Callable[[_T], bool | Awaitable[bool]],
    iterable: AsyncIterable[_T] | Iterable[_T],
) -> AsyncFilter[_T]:
    """
    Filter an (optionally async) iterable with an (optionally async) predicate.

    At least one of the arguments must be async.

    Parameters
    ----------
    func : Callable[[T], Union[bool, Awaitable[bool]]]
        A function or coroutine function which takes one item of ``iterable``
        as an argument, and returns ``True`` or ``False``.
    iterable : Union[AsyncIterable[_T], Iterable[_T]]
        An iterable or async iterable which is to be filtered.

    Raises
    ------
    TypeError
        If neither of the arguments are async.

    Returns
    -------
    AsyncFilter[T]
        An object which can either be awaited to yield a list of the filtered
        items, or can also act as an async iterator to yield items one by one.

    """
    return AsyncFilter(func, iterable)


async def async_enumerate(async_iterable: AsyncIterable[_T], start: int = 0) -> AsyncIterator[tuple[int, _T]]:
    """
    Async iterable version of `enumerate`.

    Parameters
    ----------
    async_iterable : AsyncIterable[T]
        The iterable to enumerate.
    start : int
        The index to start from. Defaults to 0.

    Returns
    -------
    AsyncIterator[Tuple[int, T]]
        An async iterator of tuples in the form of ``(index, item)``.

    """
    async for item in async_iterable:
        yield start, item
        start += 1


async def _sem_wrapper(sem, task):
    async with sem:
        return await task


def bounded_gather_iter(
    *coros_or_futures, limit: int = 4, semaphore: Semaphore | None = None
) -> Iterator[Awaitable[Any]]:
    """
    An iterator that returns tasks as they are ready, but limits the
    number of tasks running at a time.

    Parameters
    ----------
    *coros_or_futures
        The awaitables to run in a bounded concurrent fashion.
    limit : Optional[`int`]
        The maximum number of concurrent tasks. Used when no ``semaphore``
        is passed.
    semaphore : Optional[:class:`asyncio.Semaphore`]
        The semaphore to use for bounding tasks. If `None`, create one
        using ``loop`` and ``limit``.

    Raises
    ------
    TypeError
        When invalid parameters are passed

    """
    loop = asyncio.get_running_loop()

    if semaphore is None:
        if not isinstance(limit, int) or limit <= 0:
            raise TypeError("limit must be an int > 0")

        semaphore = Semaphore(limit)

    pending = []

    for cof in coros_or_futures:
        if isfuture(cof) and cof._loop is not loop:
            raise ValueError("futures are tied to different event loops")

        cof = _sem_wrapper(semaphore, cof)
        pending.append(cof)

    return as_completed(pending)


def bounded_gather(
    *coros_or_futures,
    return_exceptions: bool = False,
    limit: int = 4,
    semaphore: Semaphore | None = None,
) -> Awaitable[list[Any]]:
    """
    A semaphore-bounded wrapper to :meth:`asyncio.gather`.

    Parameters
    ----------
    *coros_or_futures
        The awaitables to run in a bounded concurrent fashion.
    return_exceptions : bool
        If true, gather exceptions in the result list instead of raising.
    limit : Optional[`int`]
        The maximum number of concurrent tasks. Used when no ``semaphore``
        is passed.
    semaphore : Optional[:class:`asyncio.Semaphore`]
        The semaphore to use for bounding tasks. If `None`, create one
        using ``loop`` and ``limit``.

    Raises
    ------
    TypeError
        When invalid parameters are passed

    """
    loop = asyncio.get_running_loop()

    if semaphore is None:
        if not isinstance(limit, int) or limit <= 0:
            raise TypeError("limit must be an int > 0")

        semaphore = Semaphore(limit)

    tasks = (_sem_wrapper(semaphore, task) for task in coros_or_futures)

    return asyncio.gather(*tasks, return_exceptions=return_exceptions)


class AsyncIter(AsyncIterator[_T], Awaitable[list[_T]]):  # pylint: disable=duplicate-bases
    """
    Asynchronous iterator yielding items from ``iterable``
    that sleeps for ``delay`` seconds every ``steps`` items.

    Parameters
    ----------
    iterable: Iterable
        The iterable to make async.
    delay: Union[float, int]
        The amount of time in seconds to sleep.
    steps: int
        The number of iterations between sleeps.

    Raises
    ------
    ValueError
        When ``steps`` is lower than 1.

    Examples
    --------
    >>> from redbot.core.utils import AsyncIter
    >>> async for value in AsyncIter(range(3)):
    ...     print(value)
    0
    1
    2

    """

    def __init__(self, iterable: Iterable[_T], delay: float | int = 0, steps: int = 1) -> None:
        if steps < 1:
            raise ValueError("Steps must be higher than or equals to 1")
        self._delay = delay
        self._iterator = iter(iterable)
        self._i = 0
        self._steps = steps
        self._map = None

    def __aiter__(self) -> AsyncIter[_T]:
        return self

    async def __anext__(self) -> _T:
        try:
            item = next(self._iterator)
        except StopIteration as e:
            raise StopAsyncIteration from e
        if self._i == self._steps:
            self._i = 0
            await asyncio.sleep(self._delay)
        self._i += 1
        return await maybe_coroutine(self._map, item) if self._map is not None else item

    def __await__(self) -> Generator[Any, None, list[_T]]:
        """
        Returns a list of the iterable.

        Examples
        --------
        >>> from redbot.core.utils import AsyncIter
        >>> iterator = AsyncIter(range(5))
        >>> await iterator
        [0, 1, 2, 3, 4]

        """
        return self.flatten().__await__()

    async def next(self, default: Any = ...) -> _T:
        """
        Returns a next entry of the iterable.

        Parameters
        ----------
        default: Optional[Any]
            The value to return if the iterator is exhausted.

        Raises
        ------
        StopAsyncIteration
            When ``default`` is not specified and the iterator has been exhausted.

        Examples
        --------
        >>> from redbot.core.utils import AsyncIter
        >>> iterator = AsyncIter(range(5))
        >>> await iterator.next()
        0
        >>> await iterator.next()
        1

        """
        try:
            value = await self.__anext__()
        except StopAsyncIteration:
            if default is ...:
                raise
            value = default
        return value

    async def flatten(self) -> list[_T]:
        """
        Returns a list of the iterable.

        Examples
        --------
        >>> from redbot.core.utils import AsyncIter
        >>> iterator = AsyncIter(range(5))
        >>> await iterator.flatten()
        [0, 1, 2, 3, 4]

        """
        return [item async for item in self]

    def filter(self, function: Callable[[_T], bool | Awaitable[bool]]) -> AsyncFilter[_T]:
        """
        Filter the iterable with an (optionally async) predicate.

        Parameters
        ----------
        function: Callable[[T], Union[bool, Awaitable[bool]]]
            A function or coroutine function which takes one item of ``iterable``
            as an argument, and returns ``True`` or ``False``.

        Returns
        -------
        AsyncFilter[T]
            An object which can either be awaited to yield a list of the filtered
            items, or can also act as an async iterator to yield items one by one.

        Examples
        --------
        >>> from redbot.core.utils import AsyncIter
        >>> def predicate(value):
        ...     return value <= 5
        >>> iterator = AsyncIter([1, 10, 5, 100])
        >>> async for i in iterator.filter(predicate):
        ...     print(i)
        1
        5

        >>> from redbot.core.utils import AsyncIter
        >>> def predicate(value):
        ...     return value <= 5
        >>> iterator = AsyncIter([1, 10, 5, 100])
        >>> await iterator.filter(predicate)
        [1, 5]

        """
        return async_filter(function, self)

    def enumerate(self, start: int = 0) -> AsyncIterator[tuple[int, _T]]:
        """
        Async iterable version of `enumerate`.

        Parameters
        ----------
        start: int
            The index to start from. Defaults to 0.

        Returns
        -------
        AsyncIterator[Tuple[int, T]]
            An async iterator of tuples in the form of ``(index, item)``.

        Examples
        --------
        >>> from redbot.core.utils import AsyncIter
        >>> iterator = AsyncIter(["one", "two", "three"])
        >>> async for i in iterator.enumerate(start=10):
        ...     print(i)
        (10, 'one')
        (11, 'two')
        (12, 'three')

        """
        return async_enumerate(self, start)

    async def without_duplicates(self) -> AsyncIterator[_T]:
        """
        Iterates while omitting duplicated entries.

        Examples
        --------
        >>> from redbot.core.utils import AsyncIter
        >>> iterator = AsyncIter([1, 2, 3, 3, 4, 4, 5])
        >>> async for i in iterator.without_duplicates():
        ...     print(i)
        1
        2
        3
        4
        5

        """
        _temp = set()
        async for item in self:
            if item not in _temp:
                yield item
                _temp.add(item)
        del _temp

    async def find(
        self,
        predicate: Callable[[_T], bool | Awaitable[bool]],
        default: Any | None = None,
    ) -> AsyncIterator[_T]:
        """
        Calls ``predicate`` over items in iterable and return first value to match.

        Parameters
        ----------
        predicate: Union[Callable, Coroutine]
            A function that returns a boolean-like result. The predicate provided can be a coroutine.
        default: Optional[Any]
            The value to return if there are no matches.

        Raises
        ------
        TypeError
            When ``predicate`` is not a callable.

        Examples
        --------
        >>> from redbot.core.utils import AsyncIter
        >>> await AsyncIter(range(3)).find(lambda x: x == 1)
        1

        """
        while True:
            try:
                elem = await self.__anext__()
            except StopAsyncIteration:
                return default
            ret = await maybe_coroutine(predicate, elem)
            if ret:
                return elem

    def map(self, func: Callable[[_T], _S | Awaitable[_S]]) -> AsyncIter[_S]:
        """
        Set the mapping callable for this instance of `AsyncIter`.

        .. important::
            This should be called after AsyncIter initialization and before any other of its methods.

        Parameters
        ----------
        func: Union[Callable, Coroutine]
            The function to map values to. The function provided can be a coroutine.

        Raises
        ------
        TypeError
            When ``func`` is not a callable.

        Examples
        --------
        >>> from redbot.core.utils import AsyncIter
        >>> async for value in AsyncIter(range(3)).map(bool):
        ...     print(value)
        False
        True
        True

        """
        if not callable(func):
            raise TypeError("Mapping must be a callable.")
        self._map = func
        return self


def get_end_user_data_statement(file: Path | str) -> str | None:
    """
    This function attempts to get the ``end_user_data_statement`` key from cog's ``info.json``.
    This will log the reason if ``None`` is returned.

    Parameters
    ----------
    file: Union[pathlib.Path, str]
        The ``__file__`` variable for the cog's ``__init__.py`` file.

    Returns
    -------
    Optional[str]
        The end user data statement found in the info.json
        or ``None`` if there was an issue finding one.

    Examples
    --------
    >>> # In cog's `__init__.py`
    >>> from redbot.core.utils import get_end_user_data_statement
    >>> __red_end_user_data_statement__ = get_end_user_data_statement(__file__)
    >>> def setup(bot): ...

    """
    try:
        file = Path(file).parent.absolute()
        info_json = file / "info.json"
        statement = get_end_user_data_statement_or_raise(info_json)
    except FileNotFoundError:
        logger.critical("'%s' does not exist.", str(info_json))
    except KeyError:
        logger.critical("'%s' is missing an entry for 'end_user_data_statement'", str(info_json))
    except json.JSONDecodeError as exc:
        logger.critical("'%s' is not a valid JSON file.", str(info_json), exc_info=exc)
    except UnicodeError as exc:
        logger.critical("'%s' has a bad encoding.", str(info_json), exc_info=exc)
    except Exception as exc:
        logger.critical(
            "There was an error when trying to load the end user data statement from '%s'.",
            str(info_json),
            exc_info=exc,
        )
    else:
        return statement
    return None


def get_end_user_data_statement_or_raise(file: Path | str) -> str:
    """
    This function attempts to get the ``end_user_data_statement`` key from cog's ``info.json``.

    Parameters
    ----------
    file: Union[pathlib.Path, str]
        The ``__file__`` variable for the cog's ``__init__.py`` file.

    Returns
    -------
    str
        The end user data statement found in the info.json.

    Raises
    ------
    FileNotFoundError
        When ``info.json`` does not exist.
    KeyError
        When ``info.json`` does not have the ``end_user_data_statement`` key.
    json.JSONDecodeError
        When ``info.json`` can't be decoded with ``json.load()``
    UnicodeError
        When ``info.json`` can't be decoded due to bad encoding.
    Exception
        Any other exception raised from ``pathlib`` and ``json`` modules
        when attempting to parse the ``info.json`` for the ``end_user_data_statement`` key.

    """
    file = Path(file).parent.absolute()
    info_json = file / "info.json"
    with info_json.open(encoding="utf-8") as fp:
        return json.load(fp)["end_user_data_statement"]


# SOURCE: https://github.com/makupi/cookiecutter-discord.py-postgres/blob/133702ceb8682ec3927530ac35ad28d47a42802e/%7B%7Bcookiecutter.bot_slug%7D%7D/bot/utils/__init__.py
# config = Config()


# SOURCE: https://github.com/makupi/cookiecutter-discord.py-postgres/blob/master/%7B%7Bcookiecutter.bot_slug%7D%7D/bot/utils/__init__.py
def get_guild_prefix(_bot, guild_id):
    logger.info(f"get_guild_prefix(_bot, guild_id) - > get_guild_prefix({_bot}, {guild_id})")
    prefix = aiosettings.prefix
    guild_data = _bot.guild_data.get(guild_id, None)
    if guild_data is not None:
        _prefix = guild_data.get("prefix")
        if _prefix is not None:
            prefix = _prefix

    logger.info(f"inside get_guild_prefix(_bot, guild_id) - > get_guild_prefix({_bot}, {guild_id})")
    return prefix

</document_content>
</document>
<document index="19">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/_testing.py</source>
<document_content>
# # SOURCE: https://github.com/roocs/clisops/blob/dd87a2e335b8f06d54e2a15ae67ab3e79609ec77/clisops/utils/testing.py#L417
# from __future__ import annotations

# from typing import Any, Optional, Type


# class ContextLogger:
#     """Helper class for safe logging management in pytests.

#     This context manager handles enabling and disabling loggers during pytest execution,
#     with special handling for pytest's caplog fixture.

#     Args:
#         caplog: Whether pytest's caplog fixture is being used.
#     """
#     import loguru


#     logger: loguru.Logger
#     using_caplog: bool
#     _package: str

#     def __init__(self, caplog: bool = False) -> None:
#         """Initialize the ContextLogger.

#         Args:
#             caplog: Flag indicating if pytest's caplog fixture is being used.
#         """
#         import structlog

# logger = structlog.get_logger(__name__)

#         self.logger = logger
#         self.using_caplog = False
#         self._package = ""
#         if caplog:
#             self.using_caplog = True

#     def __enter__(self, package_name: str = "clisops") -> loguru.Logger:
#         """Enter the context manager.

#         Args:
#             package_name: Name of the package to enable logging for.

#         Returns:
#             The configured logger instance.
#         """
#         self.logger.enable(package_name)
#         self._package = package_name
#         return self.logger

#     def __exit__(
#         self,
#         exc_type: type[BaseException] | None,
#         exc_val: BaseException | None,
#         exc_tb: Any | None,
#     ) -> None:
#         """Exit the context manager.

#         If test is supplying caplog, pytest will manage teardown.

#         Args:
#             exc_type: The type of the exception that was raised, if any.
#             exc_val: The instance of the exception that was raised, if any.
#             exc_tb: The traceback of the exception that was raised, if any.
#         """
#         self.logger.disable(self._package)
#         if not self.using_caplog:
#             try:
#                 self.logger.remove()
#             except ValueError:
#                 pass

</document_content>
</document>
<document index="20">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/aiodbx.py</source>
<document_content>
"""democracy_exe.utils.aiodbx.

Provides asynchronous Dropbox API client functionality.
"""

# SOURCE: https://github.com/ebai101/aiodbx/blob/main/aiodbx.py
from __future__ import annotations

import asyncio
import base64
import json
import os
import pathlib
import sys
import traceback
import uuid

from collections.abc import AsyncGenerator, Awaitable, Callable, Generator
from typing import Any, Dict, List, Optional, TypeVar, Union, cast

import aiofiles
import aiohttp
import bpdb
import structlog


logger = structlog.get_logger(__name__)
from pydantic import SecretStr

from democracy_exe.aio_settings import aiosettings


async def aio_path_exists(path: str | pathlib.Path) -> bool:
    """Async wrapper for path existence check.

    Args:
        path: Path to check

    Returns:
        bool: True if path exists
    """
    return await asyncio.to_thread(os.path.exists, path)

async def aio_path_basename(path: str | pathlib.Path) -> str:
    """Async wrapper for path basename.

    Args:
        path: Path to get basename from

    Returns:
        str: Basename of path
    """
    return await asyncio.to_thread(os.path.basename, path)

async def aio_path_getsize(path: str | pathlib.Path) -> int:
    """Async wrapper for getting file size.

    Args:
        path: Path to get size of

    Returns:
        int: Size of file in bytes
    """
    return await asyncio.to_thread(os.path.getsize, path)


class DropboxAPIError(Exception):
    """Exception raised for Dropbox API errors.

    Args:
        status: HTTP status code
        message: Error message
    """

    def __init__(self, status: int, message: str) -> None:
        self.status = status
        super().__init__(message)


class Request:
    """Wrapper for HTTP requests with retry logic.

    Args:
        request_func: Async request function to call
        url: Request URL
        headers: Request headers
        data: Request data
        retry_count: Number of retries
    """

    def __init__(
        self,
        request_func: Callable[..., Awaitable[aiohttp.ClientResponse]],
        url: str,
        headers: dict[str, str],
        data: str | bytes | dict[str, Any] | None = None,
        retry_count: int = 5
    ) -> None:
        self.request_func = request_func
        self.url = url
        self.headers = headers
        self.data = data
        self.retry_count = retry_count
        self.response: aiohttp.ClientResponse | None = None

    async def _do_request(self) -> aiohttp.ClientResponse:
        """Execute the request with retry logic.

        Returns:
            aiohttp.ClientResponse: The response from the request

        Raises:
            DropboxAPIError: If request fails after retries
        """
        last_exception = None
        for attempt in range(self.retry_count):
            try:
                response = await self.request_func(
                    self.url,
                    headers=self.headers,
                    data=self.data
                )
                if response.status == 429:  # Rate limit
                    await asyncio.sleep(1 * (2 ** attempt))
                    continue
                if response.status >= 400:
                    error_data = await response.text()
                    try:
                        error_json = json.loads(error_data)
                        error_message = error_json.get("error_summary", error_data)
                    except json.JSONDecodeError:
                        error_message = error_data
                    raise DropboxAPIError(response.status, str(error_message))
                return response
            except DropboxAPIError:
                raise
            except Exception as e:
                last_exception = e
                if attempt < self.retry_count - 1:
                    await asyncio.sleep(1 * (2 ** attempt))
                    continue
                break

        raise DropboxAPIError(
            500,
            f"Request failed after {self.retry_count} retries: {last_exception!s}"
        )

    def __await__(self) -> Generator[Any, None, aiohttp.ClientResponse]:
        """Make the class awaitable.

        Returns:
            Generator yielding the response
        """
        return self._do_request().__await__()

    async def __aenter__(self) -> aiohttp.ClientResponse:
        """Enter async context manager.

        Returns:
            aiohttp.ClientResponse: The response from the request
        """
        self.response = await self._do_request()
        return self.response

    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        """Exit async context manager.

        Args:
            exc_type: Exception type if an error occurred
            exc_val: Exception value if an error occurred
            exc_tb: Exception traceback if an error occurred
        """
        if self.response:
            await self.response.release()


async def retry_with_backoff(
    func: Callable[..., Any],
    *args: Any,
    max_retries: int = 3,
    initial_delay: float = 1.0,
    max_delay: float = 10.0,
    backoff_factor: float = 2.0,
    **kwargs: Any
) -> Any:
    """Execute a function with exponential backoff retry logic.

    Args:
        func: Function to execute
        *args: Positional arguments for the function
        max_retries: Maximum number of retry attempts
        initial_delay: Initial delay between retries in seconds
        max_delay: Maximum delay between retries in seconds
        backoff_factor: Factor to multiply delay by after each retry
        **kwargs: Keyword arguments for the function

    Returns:
        Result from the function

    Raises:
        Exception: The last exception encountered after all retries
    """
    delay = initial_delay
    last_exception = None

    for attempt in range(max_retries + 1):
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            last_exception = e
            if attempt == max_retries:
                raise

            logger.warning(f"Operation failed (attempt {attempt + 1}/{max_retries + 1}): {e!s}")
            await asyncio.sleep(delay)
            delay = min(delay * backoff_factor, max_delay)

    raise last_exception if last_exception else RuntimeError("Unexpected retry failure")

class SafeFileHandler:
    """Safe file handler with proper error handling, cleanup, and thread safety.

    This class provides a thread-safe async context manager for file operations
    with comprehensive error handling and resource cleanup. It ensures proper
    cleanup in both success and error cases, even during segfaults or thread
    termination.

    Args:
        path: Path to the file
        mode: File mode
        cleanup_on_error: Whether to delete file on error in write mode
        **kwargs: Additional arguments for aiofiles.open
    """

    def __init__(
        self,
        path: str | pathlib.Path,
        mode: str = "r",
        cleanup_on_error: bool = True,
        **kwargs: Any
    ) -> None:
        self.path = path
        self.mode = mode
        self.cleanup_on_error = cleanup_on_error
        self.kwargs = kwargs
        self.file = None
        self._closed = False
        self._lock = asyncio.Lock()
        self._cleanup_lock = asyncio.Lock()
        self._loop: asyncio.AbstractEventLoop | None = None
        try:
            self._loop = asyncio.get_running_loop()
        except RuntimeError:
            raise RuntimeError("SafeFileHandler must be created from an async context")

    async def _ensure_directory(self) -> None:
        """Ensure the parent directory exists.

        Raises:
            OSError: If directory creation fails
        """
        try:
            directory = os.path.dirname(self.path)
            if directory and not await aio_path_exists(directory):
                await asyncio.to_thread(os.makedirs, directory, exist_ok=True)
        except Exception as e:
            raise OSError(f"Failed to create directory: {e!s}") from e

    async def _cleanup_file(self) -> None:
        """Clean up the file safely."""
        if self.cleanup_on_error and "w" in self.mode and await aio_path_exists(self.path):
            try:
                await asyncio.to_thread(os.remove, self.path)
            except Exception as e:
                logger.error(f"Failed to cleanup file {self.path}: {e!s}")

    async def __aenter__(self) -> Any:
        """Enter the async context manager.

        Returns:
            The opened file object

        Raises:
            OSError: If file operations fail
            RuntimeError: If context manager is reused after closing
        """
        if self._closed:
            raise RuntimeError("Cannot reuse closed SafeFileHandler")

        async with self._lock:
            try:
                if "w" in self.mode:
                    await self._ensure_directory()

                self.file = await aiofiles.open(self.path, self.mode, **self.kwargs)
                return self.file
            except Exception as e:
                await self._cleanup_file()
                raise OSError(f"File operation failed: {e!s}") from e

    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        """Exit the async context manager.

        Ensures proper cleanup of resources in all cases.

        Args:
            exc_type: Exception type if an error occurred
            exc_val: Exception value if an error occurred
            exc_tb: Exception traceback if an error occurred
        """
        async with self._cleanup_lock:
            if self._closed:
                return

            try:
                if self.file:
                    try:
                        await self.file.close()
                    except Exception as e:
                        logger.error(f"Error closing file {self.path}: {e!s}")
                    finally:
                        self.file = None

                if exc_type and self.cleanup_on_error and "w" in self.mode:
                    await self._cleanup_file()
            finally:
                self._closed = True
                self._loop = None  # Clear event loop reference

    async def close(self) -> None:
        """Explicitly close the file handler.

        This method can be called to ensure cleanup if the context manager
        cannot complete normally (e.g., during thread shutdown).
        """
        if not self._closed:
            await self.__aexit__(None, None, None)

def safe_aiofiles_open(
    path: str | pathlib.Path,
    mode: str = "r",
    cleanup_on_error: bool = True,
    **kwargs: Any
) -> SafeFileHandler:
    """Create a safe file handler with proper error handling and cleanup.

    Args:
        path: Path to the file
        mode: File mode
        cleanup_on_error: Whether to delete file on error in write mode
        **kwargs: Additional arguments for aiofiles.open

    Returns:
        SafeFileHandler: A context manager for safe file operations
    """
    return SafeFileHandler(path, mode, cleanup_on_error, **kwargs)


class AsyncDropboxAPI:
    """Async Dropbox API client.

    Args:
        access_token: Dropbox access token
        max_retries: Maximum number of retries for failed requests
        retry_delay: Delay between retries in seconds
    """

    def __init__(
        self,
        access_token: str | None | SecretStr = None,
        max_retries: int = 3,
        retry_delay: float = 1.0
    ) -> None:
        if access_token is None:
            logger.debug("Using aiosettings.dropbox_cerebro_token")
            # self.access_token_secret: SecretStr = cast(SecretStr, aiosettings.dropbox_cerebro_token)
            self.access_token: SecretStr | str = aiosettings.dropbox_cerebro_token.get_secret_value()  # pylint: disable=no-member
        else:
            if isinstance(access_token, str):
                self.access_token: str | SecretStr = SecretStr(access_token)
            else:
                self.access_token: SecretStr | str = access_token

        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.client_session: aiohttp.ClientSession | None = None
        self.upload_session: list[dict[str, Any]] = []
        self._request_semaphore = asyncio.Semaphore(10)  # Limit concurrent requests
        self._upload_session_lock = asyncio.Lock()
        self._cleanup_lock = asyncio.Lock()
        self._closed = False

    async def _cleanup(self) -> None:
        """Clean up resources and close connections."""
        if self._closed:
            return

        async with self._cleanup_lock:
            if self._closed:
                return

            try:
                # Clear upload session
                async with self._upload_session_lock:
                    self.upload_session.clear()

                # Close client session
                if self.client_session and not self.client_session.closed:
                    await self.client_session.close()

                self._closed = True
                logger.debug("Cleanup completed successfully")

            except Exception as e:
                logger.error(f"Error during cleanup: {e!s}")
                raise DropboxAPIError(500, f"Cleanup failed: {e!s}")

    async def __aenter__(self) -> AsyncDropboxAPI:
        """Enter async context manager.

        Returns:
            Self instance
        """
        return self

    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        """Exit async context manager.

        Args:
            exc_type: Exception type if an error occurred
            exc_val: Exception value if an error occurred
            exc_tb: Exception traceback if an error occurred
        """
        await self._cleanup()

    async def _do_request(
        self,
        method: str,
        url: str,
        headers: dict[str, str] | None = None,
        data: str | bytes | dict[str, Any] | None = None,
        json_data: dict[str, Any] | None = None,
        retry_count: int = 0
    ) -> Any:
        """Make an HTTP request to the Dropbox API.

        Args:
            method: HTTP method
            url: Request URL
            headers: Optional request headers
            data: Optional request data
            json_data: Optional JSON data
            retry_count: Current retry attempt

        Returns:
            Response data as JSON

        Raises:
            DropboxAPIError: If request fails after retries
        """
        if not self.client_session:
            logger.debug("Creating new client session")
            self.client_session = aiohttp.ClientSession()

        headers = headers or {}
        if "Authorization" not in headers:
            logger.debug("Adding authorization header")
            headers["Authorization"] = f"Bearer {aiosettings.dropbox_cerebro_token.get_secret_value()}" # pylint: disable=no-member

        logger.debug(f"Making {method} request to {url} (retry {retry_count}/{self.max_retries})")
        # logger.debug(f"Request headers: {headers}")
        if data:
            logger.debug(f"Request data length: {len(data) if isinstance(data, (str, bytes)) else len(str(data))} bytes")
        if json_data:
            logger.debug(f"Request JSON data: {json_data}")

        try:
            async with self._request_semaphore:
                logger.debug("Acquired request semaphore")
                async with self.client_session.request(
                    method,
                    url,
                    headers=headers,
                    data=data,
                    json=json_data
                ) as response:
                    logger.debug(f"Response status: {response.status}")
                    # logger.debug(f"Response headers: {response.headers}")

                    if response.status == 429:  # Rate limit
                        if retry_count < self.max_retries:
                            delay = self.retry_delay * (2 ** retry_count)
                            logger.debug(f"Rate limited, retrying in {delay} seconds")
                            await asyncio.sleep(delay)
                            return await self._do_request(
                                method, url, headers, data,
                                json_data, retry_count + 1
                            )
                        logger.error("Rate limit exceeded after max retries")
                        raise DropboxAPIError(429, "Rate limit exceeded")

                    if not response.ok:
                        error_data = await response.text()
                        logger.error(f"Request failed with status {response.status}: {error_data}")
                        raise DropboxAPIError(
                            response.status,
                            f"Request failed: {response.status} - {error_data}"
                        )

                    if response.content_type == "application/json":
                        response_data = await response.json()
                        logger.debug(f"Received JSON response: {response_data}")
                        return response_data
                    response_text = await response.text()
                    logger.debug(f"Received text response length: {len(response_text)} bytes")
                    return response_text

        except aiohttp.ClientError as e:
            logger.error(f"Client error occurred: {e!s}")
            if retry_count < self.max_retries:
                delay = self.retry_delay * (2 ** retry_count)
                logger.debug(f"Retrying request in {delay} seconds")
                await asyncio.sleep(delay)
                return await self._do_request(
                    method, url, headers, data,
                    json_data, retry_count + 1
                )
            raise DropboxAPIError(500, f"Request failed: {e!s}")

        except Exception as e:
            logger.error(f"Unexpected error occurred: {e!s}")
            logger.error(f"Traceback: {''.join(traceback.format_exception(type(e), e, e.__traceback__))}")
            raise DropboxAPIError(500, f"Unexpected error: {e!s}")

    async def validate(self) -> bool:
        """Validate the API token.

        Returns:
            bool: True if token is valid

        Raises:
            DropboxAPIError: If validation fails
        """
        logger.info("Validating Dropbox API token...")
        nonce = str(uuid.uuid4())
        url = "https://api.dropboxapi.com/2/check/user"
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Content-Type": "application/json",
        }
        data = json.dumps({"query": nonce})

        try:
            response = await self._do_request("POST", url, headers=headers, data=data)
            if response["result"] != nonce:
                logger.error("API token validation failed: Invalid response")
                raise DropboxAPIError(401, "Invalid API token")
            logger.info("API token validated successfully")
            # await logger.complete()
            return True

        except Exception as ex:
            import bpdb
            import rich
            logger.error(f"API token validation failed: {ex}")
            print(f"{ex}")
            exc_type, exc_value, exc_traceback = sys.exc_info()
            print(f"Error Class: {ex.__class__}")
            output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
            print(output)
            print(f"exc_type: {exc_type}")
            print(f"exc_value: {exc_value}")
            traceback.print_tb(exc_traceback)
            # await logger.complete()
            rich.print(f"aiosettings.dev_mode: {aiosettings.dev_mode}")
            if aiosettings.dev_mode:
                bpdb.pm()

            # await logger.complete()
            raise DropboxAPIError(401, f"Token validation failed: {ex}")

        # except Exception as e:
        #     logger.error(f"API token validation failed: {e!s}")
        #     # await logger.complete()
        #     raise DropboxAPIError(401, f"Token validation failed: {e!s}")

    async def download_file(self, dropbox_path: str, local_path: str | None = None) -> str:
        """Download a file from Dropbox.

        Args:
            dropbox_path: Path to file in Dropbox
            local_path: Optional local path to save file to

        Returns:
            str: Path to downloaded file

        Raises:
            DropboxAPIError: If download fails
            OSError: If file operations fail
        """
        if not local_path:
            local_path = os.path.basename(dropbox_path)

        logger.info(f"Downloading file from Dropbox: {dropbox_path}")
        logger.debug(f"Saving to local path: {local_path}")

        url = "https://content.dropboxapi.com/2/files/download"
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Dropbox-API-Arg": json.dumps({"path": dropbox_path}),
        }

        try:
            response = await self._do_request("POST", url, headers=headers)
            async with safe_aiofiles_open(local_path, "wb") as f:
                if isinstance(response, str):
                    await f.write(response.encode())
                else:
                    await f.write(response)
            logger.info(f"File downloaded successfully: {local_path}")
            # await logger.complete()
            return local_path
        except Exception as e:
            logger.error(f"File download failed: {e!s}")
            # await logger.complete()
            raise DropboxAPIError(500, f"Download failed: {e!s}")

    async def download_folder(self, dropbox_path: str, local_path: str | None = None) -> str:
        """Download a folder from Dropbox as a zip file.

        Args:
            dropbox_path: Path to folder in Dropbox
            local_path: Optional local path to save zip file to

        Returns:
            str: Path to downloaded zip file

        Raises:
            DropboxAPIError: If download fails
            OSError: If file operations fail
        """
        if not local_path:
            local_path = os.path.basename(dropbox_path) + ".zip"

        logger.info(f"Downloading folder from Dropbox: {dropbox_path}")
        logger.debug(f"Saving to local path: {local_path}")

        url = "https://content.dropboxapi.com/2/files/download_zip"
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Dropbox-API-Arg": json.dumps({"path": dropbox_path}),
        }

        try:
            response = await self._do_request("POST", url, headers=headers)
            async with safe_aiofiles_open(local_path, "wb") as f:
                if isinstance(response, str):
                    await f.write(response.encode())
                else:
                    await f.write(response)
            logger.info(f"Folder downloaded successfully: {local_path}")
            # await logger.complete()
            return local_path
        except Exception as e:
            logger.error(f"Folder download failed: {e!s}")
            # await logger.complete()
            raise DropboxAPIError(500, f"Download failed: {e!s}")

    async def download_shared_link(self, shared_link: str, local_path: str | None = None) -> str:
        """Download a file from a shared link.

        Args:
            shared_link: Shared link URL
            local_path: Optional local path to save file to

        Returns:
            str: Path to downloaded file

        Raises:
            DropboxAPIError: If download fails
            OSError: If file operations fail
        """
        if not local_path:
            local_path = str(uuid.uuid4())

        url = "https://content.dropboxapi.com/2/sharing/get_shared_link_file"
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Dropbox-API-Arg": json.dumps({"url": shared_link}),
        }

        try:
            response = await self._do_request("POST", url, headers=headers)
            async with safe_aiofiles_open(local_path, "wb") as f:
                if isinstance(response, str):
                    await f.write(response.encode())
                else:
                    await f.write(response)
            # await logger.complete()
            return local_path
        except Exception as e:
            # await logger.complete()
            raise DropboxAPIError(500, f"Download failed: {e!s}")

    async def upload_start(self, local_path: str, dropbox_path: str) -> dict[str, Any]:
        """Start an upload session.

        Args:
            local_path: Path to local file
            dropbox_path: Destination path in Dropbox

        Returns:
            Dict containing upload session information

        Raises:
            DropboxAPIError: If upload fails
            OSError: If file operations fail
        """
        logger.info(f"Starting upload session for file: {local_path}")
        logger.debug(f"Destination path: {dropbox_path}")

        url = "https://content.dropboxapi.com/2/files/upload_session/start"
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Dropbox-API-Arg": json.dumps({"close": True}),
            "Content-Type": "application/octet-stream",
        }

        try:
            async with safe_aiofiles_open(local_path, "rb") as f:
                data = await f.read()
                response = await self._do_request("POST", url, headers=headers, data=data)
                logger.info("Upload session started successfully")
                # await logger.complete()
                return response
        except Exception as e:
            logger.error(f"Failed to start upload session: {e!s}")
            # await logger.complete()
            raise DropboxAPIError(500, f"Upload failed: {e!s}")

    async def upload_finish(self) -> dict[str, Any]:
        """Finish the upload session.

        Returns:
            Dict containing upload completion information

        Raises:
            DropboxAPIError: If upload fails
        """
        if not self.upload_session:
            logger.error("No active upload session to finish")
            # await logger.complete()
            raise DropboxAPIError(400, "No active upload session")

        logger.info("Finishing upload session...")
        logger.debug(f"Number of files in session: {len(self.upload_session)}")

        url = "https://api.dropboxapi.com/2/files/upload_session/finish_batch"
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Content-Type": "application/json",
        }
        data = json.dumps({"entries": self.upload_session})

        try:
            response = await self._do_request("POST", url, headers=headers, data=data)
            session_id = response.get("async_job_id")
            if not session_id:
                logger.error("No async job ID received in response")
                # await logger.complete()
                raise DropboxAPIError(500, "No async job ID in response")

            logger.info("Waiting for upload completion...")
            # Poll for completion
            url = "https://api.dropboxapi.com/2/files/upload_session/finish_batch/check"
            headers = {
                "Authorization": f"Bearer {self.access_token}",
                "Content-Type": "application/json",
            }
            data = json.dumps({"async_job_id": session_id})

            while True:
                response = await self._do_request("POST", url, headers=headers, data=data)
                if response[".tag"] != "in_progress":
                    break
                logger.debug("Upload still in progress, checking again...")
                await asyncio.sleep(1)

            logger.info("Upload session completed successfully")
            # await logger.complete()
            return response
        except Exception as e:
            logger.error(f"Upload session failed: {e!s}")
            # await logger.complete()
            raise DropboxAPIError(500, f"Upload finish failed: {e!s}")
        finally:
            self.upload_session = []

    async def upload_single(
        self,
        local_path: str,
        dropbox_path: str,
        mode: str = "add",
        autorename: bool = False,
        mute: bool = False
    ) -> dict[str, Any]:
        """Upload a single file to Dropbox.

        Args:
            local_path: Path to local file
            dropbox_path: Destination path in Dropbox
            mode: Upload mode (add, overwrite)
            autorename: Whether to rename file if it exists
            mute: Whether to mute notifications

        Returns:
            Dict containing upload metadata

        Raises:
            DropboxAPIError: If upload fails
            OSError: If file operations fail
        """
        logger.info(f"Uploading file to Dropbox: {local_path}")
        logger.debug(f"Destination path: {dropbox_path} (mode: {mode})")

        args = {
            "path": dropbox_path,
            "mode": mode,
            "autorename": autorename,
            "mute": mute,
        }

        url = "https://content.dropboxapi.com/2/files/upload"
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Dropbox-API-Arg": json.dumps(args),
            "Content-Type": "application/octet-stream",
        }

        try:
            async with safe_aiofiles_open(local_path, "rb") as f:
                data = await f.read()
                response = await self._do_request("POST", url, headers=headers, data=data)
                logger.info(f"File uploaded successfully: {dropbox_path}")
                # await logger.complete()
                return response
        except Exception as e:
            logger.error(f"File upload failed: {e!s}")
            # await logger.complete()
            raise DropboxAPIError(500, f"Upload failed: {e!s}")

    async def create_shared_link(self, dropbox_path: str) -> str:
        """Create a shared link for a file.

        Args:
            dropbox_path: Path to file in Dropbox

        Returns:
            str: Shared link URL

        Raises:
            DropboxAPIError: If request fails
        """
        logger.info(f"Creating shared link for: {dropbox_path}")

        url = "https://api.dropboxapi.com/2/sharing/create_shared_link_with_settings"
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Content-Type": "application/json",
        }
        data = json.dumps({"path": dropbox_path})

        try:
            response = await self._do_request("POST", url, headers=headers, data=data)
            logger.info("Shared link created successfully")
            # await logger.complete()
            return response["url"]
        except DropboxAPIError as e:
            if e.status == 409:  # Link already exists
                logger.info("Shared link already exists, retrieving existing link")
                url = "https://api.dropboxapi.com/2/sharing/get_shared_link_metadata"
                response = await self._do_request("POST", url, headers=headers, data=data)
                # await logger.complete()
                return response["url"]
            logger.error(f"Failed to create shared link: {e!s}")
            # await logger.complete()
            raise

    async def get_shared_link_metadata(self, shared_link: str) -> dict[str, Any]:
        """Get metadata for a shared link.

        Args:
            shared_link: Path to file in Dropbox

        Returns:
            Dict containing shared link metadata

        Raises:
            DropboxAPIError: If request fails
        """
        logger.info(f"Getting metadata for shared link: {shared_link}")

        url = "https://api.dropboxapi.com/2/sharing/create_shared_link_with_settings"
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Content-Type": "application/json",
        }
        data = json.dumps({"path": shared_link})

        try:
            response = await self._do_request("POST", url, headers=headers, data=data)
            logger.info("Retrieved shared link metadata successfully")
            # await logger.complete()
            return response
        except DropboxAPIError as e:
            if e.status == 409:  # Link already exists
                logger.info("Using existing shared link metadata")
                url = "https://api.dropboxapi.com/2/sharing/get_shared_link_metadata"
                response = await self._do_request("POST", url, headers=headers, data=data)
                # await logger.complete()
                return response
            logger.error(f"Failed to get shared link metadata: {e!s}")
            # await logger.complete()
            raise

    async def get_connection_metrics(self) -> dict[str, Any]:
        """Get current connection pooling metrics.

        Returns:
            Dictionary containing connection metrics:
                - active_connections: Number of currently active connections
                - acquired_connections: Number of connections currently in use
                - connection_limit: Maximum allowed connections
                - connection_timeouts: Number of connection timeouts
        """
        logger.debug("Getting connection pooling metrics")

        if not self.client_session or self.client_session.closed:
            logger.debug("No active client session")
            return {
                "active_connections": 0,
                "acquired_connections": 0,
                "connection_limit": 0,
                "connection_timeouts": 0
            }

        connector = self.client_session.connector
        if not connector:
            logger.debug("No active connector")
            return {
                "active_connections": 0,
                "acquired_connections": 0,
                "connection_limit": 0,
                "connection_timeouts": 0
            }

        metrics = {
            "active_connections": len(connector._conns),  # type: ignore
            "acquired_connections": len(connector._acquired),  # type: ignore
            "connection_limit": connector._limit,  # type: ignore
            "connection_timeouts": getattr(connector, "_timeouts_count", 0)  # type: ignore
        }
        logger.debug(f"Connection metrics: {metrics}")
        return metrics

    async def _recover_upload_session(self) -> None:
        """Attempt to recover a failed upload session.

        This method tries to salvage any valid uploads from a failed session
        and cleans up any incomplete uploads.

        Raises:
            DropboxAPIError: If session recovery fails
        """
        if not self.upload_session:
            return

        async with self._upload_session_lock:
            try:
                # Get list of valid session IDs
                valid_sessions = []
                for entry in self.upload_session:
                    try:
                        session_id = entry["cursor"]["session_id"]
                        # Try to verify session is still valid
                        url = "https://api.dropboxapi.com/2/files/upload_session/finish_batch/check"
                        headers = {
                            "Authorization": f"Bearer {self.access_token}",
                            "Content-Type": "application/json",
                        }
                        data = json.dumps({"async_job_id": session_id})
                        response = await self._do_request("POST", url, headers=headers, data=data)
                        if response[".tag"] != "in_progress":
                            continue
                        valid_sessions.append(entry)
                    except:
                        continue

                # Update upload session with only valid entries
                self.upload_session = valid_sessions

                if len(valid_sessions) > 0:
                    logger.info(f"Recovered {len(valid_sessions)} valid upload sessions")
                else:
                    logger.warning("No valid upload sessions could be recovered")

            except Exception as e:
                logger.error(f"Failed to recover upload session: {e!s}")
                # Clear the session if recovery fails
                self.upload_session.clear()
                raise DropboxAPIError(500, f"Failed to recover upload session: {e!s}")

    async def dropbox_upload(
        self,
        file_path: str | pathlib.Path,
        dropbox_path: str,
        chunk_size: int = 4 * 1024 * 1024,  # 4MB chunks
        overwrite: bool = True,
        progress_callback: Callable[[int, int], Awaitable[None]] | None = None,
    ) -> str:
        """Upload a file to Dropbox.

        Args:
            file_path: Local path to file
            dropbox_path: Dropbox destination path
            chunk_size: Size of upload chunks in bytes
            overwrite: Whether to overwrite existing file
            progress_callback: Optional callback for upload progress

        Returns:
            str: Shared link to uploaded file

        Raises:
            DropboxAPIError: If upload fails
            OSError: If file operations fail
        """
        logger.info(f"Starting chunked upload to Dropbox: {file_path}")
        logger.debug(f"Destination path: {dropbox_path} (chunk size: {chunk_size/1024/1024:.1f}MB)")

        try:
            async with safe_aiofiles_open(file_path, "rb") as f:
                file_size = await asyncio.to_thread(os.path.getsize, file_path)
                uploaded = 0

                # Start upload session
                logger.info("Initializing upload session...")
                upload_session_start_result = await self._do_request(
                    "POST",
                    "https://content.dropboxapi.com/2/files/upload_session/start",
                    headers={"Content-Type": "application/octet-stream"},
                    data=b"",
                )
                session_id = upload_session_start_result["session_id"]
                logger.debug(f"Upload session initialized with ID: {session_id}")

                # Upload file chunks
                while True:
                    chunk = await f.read(chunk_size)
                    if not chunk:
                        break

                    if uploaded + len(chunk) < file_size:
                        # Append to upload session
                        logger.debug(f"Uploading chunk: {uploaded/1024/1024:.1f}MB / {file_size/1024/1024:.1f}MB")
                        await self._do_request(
                            "POST",
                            "https://content.dropboxapi.com/2/files/upload_session/append_v2",
                            headers={
                                "Content-Type": "application/octet-stream",
                                "Dropbox-API-Arg": json.dumps({
                                    "cursor": {
                                        "session_id": session_id,
                                        "offset": uploaded
                                    }
                                })
                            },
                            data=chunk,
                        )
                    else:
                        # Finish upload session
                        logger.info("Finalizing upload...")
                        mode = "overwrite" if overwrite else "add"
                        finish_args = {
                            "cursor": {
                                "session_id": session_id,
                                "offset": uploaded
                            },
                            "commit": {
                                "path": dropbox_path,
                                "mode": mode
                            }
                        }

                        await self._do_request(
                            "POST",
                            "https://content.dropboxapi.com/2/files/upload_session/finish",
                            headers={
                                "Content-Type": "application/octet-stream",
                                "Dropbox-API-Arg": json.dumps(finish_args)
                            },
                            data=chunk,
                        )

                    uploaded += len(chunk)
                    if progress_callback:
                        await progress_callback(uploaded, file_size)

                # Get shared link
                logger.info("Upload completed, creating shared link...")
                shared_link = await self.get_shared_link_metadata(dropbox_path)
                logger.info("File uploaded and shared successfully")
                # await logger.complete()
                return shared_link["url"]

        except DropboxAPIError:
            logger.error("Upload failed due to Dropbox API error")
            raise
        except Exception as e:
            error_msg = f"Upload failed: {e!s}"
            logger.error(error_msg)
            # await logger.complete()
            raise DropboxAPIError(500, error_msg) from e

</document_content>
</document>
<document index="21">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/aiotweetpik.py</source>
<document_content>
"""cerebro_bot.utils.aiotweetpik"""
# pylint: disable=unused-import
# NOTE: couple sources
# https://github.com/powerfist01/hawk-eyed/blob/f340c6ff814dd3e2a3cac7a30d03b7c07d95d1e4/services/tweet_to_image/tweetpik.py
# https://github.com/bwhli/birdcatcher/blob/a4b33feff4f2d88d5412cd50b11760312bdd4f1d/app/util/Tweet.py
from __future__ import annotations

import asyncio
import json
import logging
import os
import pathlib
import re
import sys
import typing
import weakref

from collections.abc import Coroutine, Iterable
from typing import TYPE_CHECKING, Any, ClassVar, Dict, List, Optional, Tuple, Type, TypeVar, Union
from urllib.parse import quote as _uriquote

import aiofiles
import aiohttp
import structlog


logger = structlog.get_logger(__name__)

import democracy_exe

from democracy_exe.aio_settings import aiosettings


_from_json = json.loads

TWEETPIK_AUTHORIZATION = aiosettings.tweetpik_authorization.get_secret_value()  # pylint: disable=no-member
TWEETPIK_BUCKET_ID = aiosettings.tweetpik_bucket_id

TWEETPIK_DIMENSION_IG_FEED = aiosettings.tweetpik_dimension_ig_feed
TWEETPIK_DIMENSION_IG_STORY = aiosettings.tweetpik_dimension_ig_story
TWEETPIK_TIMEZONE = aiosettings.tweetpik_timezone
TWEETPIK_DISPLAY_LIKES = aiosettings.tweetpik_display_metrics
TWEETPIK_DISPLAY_REPLIES = aiosettings.tweetpik_display_metrics
TWEETPIK_DISPLAY_RETWEETS = aiosettings.tweetpik_display_metrics
TWEETPIK_DISPLAY_VERIFIED = aiosettings.tweetpik_display_verified
TWEETPIK_DISPLAY_SOURCE = aiosettings.tweetpik_display_embeds
TWEETPIK_DISPLAY_TIME = False
TWEETPIK_DISPLAY_MEDIA_IMAGES = aiosettings.tweetpik_display_media_images
TWEETPIK_DISPLAY_LINK_PREVIEW = aiosettings.tweetpik_display_link_preview
# Any number higher than zero. This value is representing a percentage
TWEETPIK_TEXT_WIDTH = "100"
# Any number higher than zero. This value is used in pixels(px) units
TWEETPIK_CANVAS_WIDTH = "510"

TWEETPIK_BACKGROUND_COLOR = "#FFFFFF"  # Change the background color of the tweet screenshot
TWEETPIK_TEXT_PRIMARY_COLOR = (
    "#000000"  # Change the text primary color used for the main text of the tweet and user's name
)
TWEETPIK_TEXT_SECONDARY_COLOR = (
    "#5B7083"  # Change the text secondary used for the secondary info of the tweet like the username
)
TWEETPIK_LINK_COLOR = "#1B95E0"  # Change the link colors used for the links, hashtags and mentions
TWEETPIK_VERIFIED_ICON = "#1B95E0"  # Change the verified icon color


def _to_json(obj: Any) -> str:
    return json.dumps(obj, separators=(",", ":"), ensure_ascii=True)


if TYPE_CHECKING:
    from aiohttp import ClientResponse

    try:
        from requests import Response

        _ResponseType = Union[ClientResponse, Response]
    except ModuleNotFoundError:
        _ResponseType = ClientResponse

    Snowflake = Union[str, int]
    SnowflakeList = list[Snowflake]

    from types import TracebackType

    T = TypeVar("T")
    BE = TypeVar("BE", bound=BaseException)
    MU = TypeVar("MU", bound="MaybeUnlock")
    Response = Coroutine[Any, Any, T]


class _MissingSentinel:
    def __eq__(self, other):
        return False

    def __bool__(self):
        return False

    def __repr__(self):
        return "..."


MISSING: Any = _MissingSentinel()


def get_tweet_id(tweet_url: str) -> str:
    return re.findall(r"[http?s//]?twitter\.com\/.*\/status\/(\d+)", tweet_url)[0]


def build_tweetpik_download_url(tweetId: str) -> str:
    """Building the URL
    The URL is predictable, so you don't have to worry about storing it. You just need to make sure you generated it before using it. The URL will always consist of your bucket ID and the tweet ID. https://ik.imagekit.io/tweetpik/323251495115948625/tweetId

    Returns:
        str: Url of the image we plan to download
    """
    return f"https://ik.imagekit.io/tweetpik/{TWEETPIK_BUCKET_ID}/{tweetId}"


class TweetpikAPIError(Exception):
    """
    Exception for errors thrown by the API. Contains the HTTP status code and the returned error message.
    """

    def __init__(self, status: int, message: str | dict):
        self.status = status
        self.message = message
        super().__init__(self.message)

    def __str__(self):
        if not isinstance(self.message, str):
            return f"{self.status} {self.message}"
        try:
            self.message = json.loads(self.message)
            return f'{self.status} {self.message["error_summary"]}'
        except Exception:
            return f"{self.status} {self.message}"


class TweetpikException(Exception):
    """Base exception class for tweetpik

    Ideally speaking, this could be caught to handle any exceptions raised from this library.
    """


class ClientException(TweetpikException):
    """Exception that's raised when an operation in the :class:`Client` fails.

    These are usually for exceptions that happened due to user input.
    """


class NoMoreItems(TweetpikException):
    """Exception that is raised when an async iteration operation has no more items."""


class GatewayNotFound(TweetpikException):
    """An exception that is raised when the gateway for Tweetpik could not be found"""

    def __init__(self):
        message = "The gateway to connect to discord was not found."
        super().__init__(message)


def _flatten_error_dict(d: dict[str, Any], key: str = "") -> dict[str, str]:
    items: list[tuple[str, str]] = []
    for k, v in d.items():
        new_key = f"{key}.{k}" if key else k

        if isinstance(v, dict):
            try:
                _errors: list[dict[str, Any]] = v["_errors"]
            except KeyError:
                items.extend(_flatten_error_dict(v, new_key).items())
            else:
                items.append((new_key, " ".join(x.get("message", "") for x in _errors)))
        else:
            items.append((new_key, v))

    return dict(items)


class HTTPException(TweetpikException):
    """Exception that's raised when an HTTP request operation fails.

    Attributes
    ------------
    response: :class:`aiohttp.ClientResponse`
        The response of the failed HTTP request. This is an
        instance of :class:`aiohttp.ClientResponse`. In some cases
        this could also be a :class:`requests.Response`.

    text: :class:`str`
        The text of the error. Could be an empty string.
    status: :class:`int`
        The status code of the HTTP request.
    code: :class:`int`
        The Tweetpik specific error code for the failure.
    """

    def __init__(self, response: _ResponseType, message: str | dict[str, Any] | None):
        self.response: _ResponseType = response
        self.status: int = response.status  # type: ignore
        self.code: int
        self.text: str
        if isinstance(message, dict):
            self.code = message.get("code", 0)
            base = message.get("message", "")
            if errors := message.get("errors"):
                errors = _flatten_error_dict(errors)
                helpful = "\n".join("In %s: %s" % t for t in errors.items())
                self.text = base + "\n" + helpful
            else:
                self.text = base
        else:
            self.text = message or ""
            self.code = 0

        fmt = "{0.status} {0.reason} (error code: {1})"
        if len(self.text):
            fmt += ": {2}"

        super().__init__(fmt.format(self.response, self.code, self.text))


class Forbidden(HTTPException):
    """Exception that's raised for when status code 403 occurs.

    Subclass of :exc:`HTTPException`
    """


class NotFound(HTTPException):
    """Exception that's raised for when status code 404 occurs.

    Subclass of :exc:`HTTPException`
    """


class TweetpikServerError(HTTPException):
    """Exception that's raised for when a 500 range status code occurs.

    Subclass of :exc:`HTTPException`.

    .. versionadded:: 1.5
    """


class InvalidData(ClientException):
    """Exception that's raised when the library encounters unknown
    or invalid data from Tweetpik.
    """


class InvalidArgument(ClientException):
    """Exception that's raised when an argument to a function
    is invalid some way (e.g. wrong value or wrong type).

    This could be considered the analogous of ``ValueError`` and
    ``TypeError`` except inherited from :exc:`ClientException` and thus
    :exc:`TweetpikException`.
    """


class ConnectionClosed(ClientException):
    """Exception that's raised when the gateway connection is
    closed for reasons that could not be handled internally.

    Attributes
    -----------
    code: :class:`int`
        The close code of the websocket.
    reason: :class:`str`
        The reason provided for the closure.
    shard_id: Optional[:class:`int`]
        The shard ID that got closed if applicable.
    """

    def __init__(self, socket: ClientResponse, *, code: int | None = None):
        # This exception is just the same exception except
        # reconfigured to subclass ClientException for users
        self.code: int = code or socket.close_code or -1
        # aiohttp doesn't seem to consistently provide close reason
        self.reason: str = ""
        super().__init__(f"HTTP Request closed with {self.code}")


# SOURCE: discord.py
async def json_or_text(response: aiohttp.ClientResponse) -> dict[str, Any] | str:
    text = await response.text(encoding="utf-8")
    try:
        if response.headers["content-type"] == "application/json":
            return _from_json(text)
    except KeyError:
        # Thanks Cloudflare
        pass

    return text


# SOURCE: discord.py
class TweetpikRoute:
    BASE: ClassVar[str] = "https://tweetpik.com/api"

    def __init__(self, method: str, path: str, **parameters: Any) -> None:
        self.path: str = path
        self.method: str = method
        url = self.BASE + self.path
        if parameters:
            url = url.format_map({k: _uriquote(v) if isinstance(v, str) else v for k, v in parameters.items()})
        self.url: str = url
        self.bucket_id: str = TWEETPIK_BUCKET_ID

    @property
    def bucket(self) -> str:
        # the bucket is just method + path w/ major parameters
        return f"{self.bucket_id}"


# SOURCE: discord.py
class MaybeUnlock:
    def __init__(self, lock: asyncio.Lock) -> None:
        self.lock: asyncio.Lock = lock
        self._unlock: bool = True

    def __enter__(self: MU) -> MU:
        return self

    def defer(self) -> None:
        self._unlock = False

    def __exit__(
        self,
        exc_type: type[BE] | None,
        exc: BE | None,
        traceback: TracebackType | None,
    ) -> None:
        if self._unlock:
            self.lock.release()


# SOURCE: discord.py
class TweetpikHTTPClient:
    """Represents an HTTP client sending HTTP requests to the Tweetpik API."""

    def __init__(
        self,
        connector: aiohttp.BaseConnector | None = None,
        *,
        proxy: str | None = None,
        proxy_auth: aiohttp.BasicAuth | None = None,
        loop: asyncio.AbstractEventLoop | None = None,
        unsync_clock: bool = True,
    ) -> None:
        self.loop: asyncio.AbstractEventLoop = asyncio.get_event_loop() if loop is None else loop
        self.connector = connector
        self.__session: aiohttp.ClientSession = aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit_per_host=50))
        self._locks: weakref.WeakValueDictionary = weakref.WeakValueDictionary()
        self._global_over: asyncio.Event = asyncio.Event()
        self._global_over.set()
        self.token: str | None = None
        self.bot_token: bool = False
        self.proxy: str | None = proxy
        self.proxy_auth: aiohttp.BasicAuth | None = proxy_auth
        self.use_clock: bool = not unsync_clock

        user_agent = "TweetpikHTTPClient (democracy-exe {0}) Python/{1[0]}.{1[1]} aiohttp/{2}"
        self.user_agent: str = user_agent.format(democracy_exe.__version__, sys.version_info, aiohttp.__version__)

    async def request(
        self,
        route: TweetpikRoute,
        *,
        # files: Optional[Sequence[File]] = None,
        form: Iterable[dict[str, Any]] | None = None,
        **kwargs: Any,
    ) -> Any:
        print(f"{form}")

        bucket = route.bucket
        method = route.method
        url = route.url

        lock = self._locks.get(bucket)
        if lock is None:
            lock = asyncio.Lock()
            if bucket is not None:
                self._locks[bucket] = lock

        # header creation
        headers: dict[str, str] = {
            "User-Agent": self.user_agent,
            "Authorization": TWEETPIK_AUTHORIZATION,
            "Content-Type": "application/json",
        }

        kwargs["data"] = _to_json(kwargs.pop("json"))

        kwargs["headers"] = headers

        # Proxy support
        if self.proxy is not None:
            kwargs["proxy"] = self.proxy
        if self.proxy_auth is not None:
            kwargs["proxy_auth"] = self.proxy_auth

        if not self._global_over.is_set():
            # wait until the global lock is complete
            await self._global_over.wait()

        response: aiohttp.ClientResponse | None = None
        data: dict[str, Any] | str | None = None
        await lock.acquire()
        with MaybeUnlock(lock) as maybe_lock:
            for tries in range(5):
                try:
                    async with self.__session.request(method, url, **kwargs) as response:
                        logger.debug(f"{method} {url} with {kwargs.get('data')} has returned {response.status}")

                        # even errors have text involved in them so this is safe to call
                        data = await json_or_text(response)

                        logger.debug("HERE IS THE DATA WE GET BACK FROM THE API CALL BELOVED")
                        logger.debug(data)

                        # the request was successful so just return the text/json
                        if 300 > response.status >= 200:
                            logger.debug(f"{method} {url} has received {data}")
                            return data

                        # we are being rate limited
                        if response.status == 429:
                            if not response.headers.get("Via") or isinstance(data, str):
                                # Banned by Cloudflare more than likely.
                                raise HTTPException(response, data)

                            fmt = 'We are being rate limited. Retrying in %.2f seconds. Handled under the bucket "%s"'

                            # sleep a bit
                            retry_after: float = data["retry_after"]
                            logger.warning(fmt, retry_after, bucket)

                            continue

                        # we've received a 500, 502, or 504, unconditional retry
                        if response.status in {500, 502, 504}:
                            await asyncio.sleep(1 + tries * 2)
                            continue

                        # the usual error cases
                        if response.status == 403:
                            raise Forbidden(response, data)
                        elif response.status == 404:
                            raise NotFound(response, data)
                        elif response.status >= 500:
                            raise TweetpikServerError(response, data)
                        else:
                            raise HTTPException(response, data)

                # This is handling exceptions from the request
                except OSError as e:
                    # Connection reset by peer
                    if tries < 4 and e.errno in (54, 10054):
                        await asyncio.sleep(1 + tries * 2)
                        continue
                    raise

            if response is not None:
                # We've run out of retries, raise.
                if response.status >= 500:
                    raise TweetpikServerError(response, data)

                raise HTTPException(response, data)

            raise RuntimeError("Unreachable code in HTTP handling")

    async def get_from_cdn(self, url: str) -> bytes:
        async with self.__session.get(url) as resp:
            if resp.status == 200:
                return await resp.read()
            elif resp.status == 404:
                raise NotFound(resp, "asset not found")
            elif resp.status == 403:
                raise Forbidden(resp, "cannot retrieve asset")
            else:
                raise HTTPException(resp, "failed to get asset")

    async def close(self) -> None:
        if self.__session:
            await self.__session.close()

    def images(
        self,
        tweet_url: str | None,
        *,
        dimension_ig_feed: str | None = TWEETPIK_DIMENSION_IG_FEED,
        dimension_ig_story: str | None = TWEETPIK_DIMENSION_IG_STORY,
        timezone: str | None = TWEETPIK_TIMEZONE,
        display_likes: str | None = TWEETPIK_DISPLAY_LIKES,
        display_replies: str | None = TWEETPIK_DISPLAY_REPLIES,
        display_retweets: str | None = TWEETPIK_DISPLAY_RETWEETS,
        display_verified: str | None = TWEETPIK_DISPLAY_VERIFIED,
        display_source: str | None = TWEETPIK_DISPLAY_SOURCE,
        display_time: str | None = TWEETPIK_DISPLAY_TIME,
        display_media_images: str | None = TWEETPIK_DISPLAY_MEDIA_IMAGES,
        display_link_preview: str | None = TWEETPIK_DISPLAY_LINK_PREVIEW,
        text_width: str | None = TWEETPIK_TEXT_WIDTH,
        canvas_width: str | None = TWEETPIK_CANVAS_WIDTH,
        background_color: str | None = TWEETPIK_BACKGROUND_COLOR,
        text_primary_color: str | None = TWEETPIK_TEXT_PRIMARY_COLOR,
        text_secondary_color: str | None = TWEETPIK_TEXT_SECONDARY_COLOR,
        link_color: str | None = TWEETPIK_LINK_COLOR,
        verified_icon: str | None = TWEETPIK_VERIFIED_ICON,
    ) -> Any:
        r = TweetpikRoute("POST", "/images", tweet_url=tweet_url)
        payload = {}

        if tweet_url:
            payload["tweetId"] = get_tweet_id(tweet_url)

        if dimension_ig_feed:
            payload["dimension_ig_feed"] = dimension_ig_feed
        if dimension_ig_story:
            payload["dimension_ig_story"] = dimension_ig_story
        if timezone:
            payload["timezone"] = timezone
        if display_likes:
            payload["display_likes"] = display_likes
        if display_replies:
            payload["display_replies"] = display_replies
        if display_retweets:
            payload["display_retweets"] = display_retweets
        if display_verified:
            payload["display_verified"] = display_verified
        if display_source:
            payload["display_source"] = display_source
        if display_time:
            payload["display_time"] = display_time
        if display_media_images:
            payload["display_media_images"] = display_media_images
        if display_link_preview:
            payload["display_link_preview"] = display_link_preview
        if text_width:
            payload["text_width"] = text_width
        if canvas_width:
            payload["canvas_width"] = canvas_width
        if background_color:
            payload["background_color"] = background_color
        if text_primary_color:
            payload["text_primary_color"] = text_primary_color
        if text_secondary_color:
            payload["text_secondary_color"] = text_secondary_color
        if link_color:
            payload["link_color"] = link_color
        if verified_icon:
            payload["verified_icon"] = verified_icon

        logger.debug("payload debuggggggggggggggggggggggggggg")
        logger.debug(payload)

        return self.request(r, json=payload)

    async def aimages(
        self,
        tweet_url: str | None,
        *,
        dimension_ig_feed: str | None = TWEETPIK_DIMENSION_IG_FEED,
        dimension_ig_story: str | None = TWEETPIK_DIMENSION_IG_STORY,
        timezone: str | None = TWEETPIK_TIMEZONE,
        display_likes: str | None = TWEETPIK_DISPLAY_LIKES,
        display_replies: str | None = TWEETPIK_DISPLAY_REPLIES,
        display_retweets: str | None = TWEETPIK_DISPLAY_RETWEETS,
        display_verified: str | None = TWEETPIK_DISPLAY_VERIFIED,
        display_source: str | None = TWEETPIK_DISPLAY_SOURCE,
        display_time: str | None = TWEETPIK_DISPLAY_TIME,
        display_media_images: str | None = TWEETPIK_DISPLAY_MEDIA_IMAGES,
        display_link_preview: str | None = TWEETPIK_DISPLAY_LINK_PREVIEW,
        text_width: str | None = TWEETPIK_TEXT_WIDTH,
        canvas_width: str | None = TWEETPIK_CANVAS_WIDTH,
        background_color: str | None = TWEETPIK_BACKGROUND_COLOR,
        text_primary_color: str | None = TWEETPIK_TEXT_PRIMARY_COLOR,
        text_secondary_color: str | None = TWEETPIK_TEXT_SECONDARY_COLOR,
        link_color: str | None = TWEETPIK_LINK_COLOR,
        verified_icon: str | None = TWEETPIK_VERIFIED_ICON,
    ) -> Any:
        r = TweetpikRoute("POST", "/images", tweet_url=tweet_url)
        payload = {}

        if tweet_url:
            payload["tweetId"] = get_tweet_id(tweet_url)

        if dimension_ig_feed:
            payload["dimension_ig_feed"] = dimension_ig_feed
        if dimension_ig_story:
            payload["dimension_ig_story"] = dimension_ig_story
        if timezone:
            payload["timezone"] = timezone
        if display_likes:
            payload["display_likes"] = display_likes
        if display_replies:
            payload["display_replies"] = display_replies
        if display_retweets:
            payload["display_retweets"] = display_retweets
        if display_verified:
            payload["display_verified"] = display_verified
        if display_source:
            payload["display_source"] = display_source
        if display_time:
            payload["display_time"] = display_time
        if display_media_images:
            payload["display_media_images"] = display_media_images
        if display_link_preview:
            payload["display_link_preview"] = display_link_preview
        if text_width:
            payload["text_width"] = text_width
        if canvas_width:
            payload["canvas_width"] = canvas_width
        if background_color:
            payload["background_color"] = background_color
        if text_primary_color:
            payload["text_primary_color"] = text_primary_color
        if text_secondary_color:
            payload["text_secondary_color"] = text_secondary_color
        if link_color:
            payload["link_color"] = link_color
        if verified_icon:
            payload["verified_icon"] = verified_icon

        logger.debug("payload debuggggggggggggggggggggggggggg")
        logger.debug(payload)
        data = await self.request(r, json=payload)
        await self.close()

        return data


# TODO: implement multi download https://stackoverflow.com/questions/64282309/aiohttp-download-large-list-of-pdf-files


async def async_download_file(data: dict, dl_dir="./"):
    async with aiohttp.ClientSession() as session:
        url: str = data["url"]
        username: str = data["tweet"]["username"]
        p = pathlib.Path(url)
        p_dl_dir = pathlib.Path(dl_dir)
        full_path_dl_dir = f"{p_dl_dir.absolute()}"
        logger.debug(f"Downloading {url} to {full_path_dl_dir}/{p.name}")
        async with session.get(url) as resp:
            content = await resp.read()

            # Check everything went well
            if resp.status != 200:
                logger.error(f"Download failed: {resp.status}")
                return

            async with aiofiles.open(f"{full_path_dl_dir}/{p.name}", mode="+wb") as f:
                await f.write(content)

</document_content>
</document>
<document index="22">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/architecture.py</source>
<document_content>
# # https://github.com/joeyballentine/ESRGAN-Bot/commit/82c06f7612f7781696185afb665d6c21e9249895

</document_content>
</document>
<document index="23">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/async_.py</source>
<document_content>
"""Asyncio utilities with improved thread safety and resource management."""
from __future__ import annotations

import asyncio
import concurrent.futures
import functools
import threading
import time
import typing
import weakref

from asyncio import Semaphore, coroutines, ensure_future, gather, get_running_loop
from asyncio.events import AbstractEventLoop
from collections.abc import Awaitable, Callable, Coroutine
from concurrent.futures import ThreadPoolExecutor
from traceback import extract_stack
from typing import Any, List, Optional, TypeVar

import structlog

from codetiming import Timer


logger = structlog.get_logger(__name__)


_SHUTDOWN_RUN_CALLBACK_THREADSAFE = "_shutdown_run_callback_threadsafe"
_thread_pools: weakref.WeakSet = weakref.WeakSet()
_global_cleanup_lock = threading.Lock()
_global_shutdown = False

T = TypeVar("T")


def register_thread_pool(pool: ThreadPoolExecutor) -> None:
    """Register a thread pool for proper cleanup during shutdown.

    Args:
        pool: ThreadPoolExecutor to register
    """
    with _global_cleanup_lock:
        if not _global_shutdown:
            _thread_pools.add(pool)


def cleanup_thread_pools() -> None:
    """Clean up all registered thread pools."""
    global _global_shutdown  # pylint: disable=global-statement

    with _global_cleanup_lock:
        _global_shutdown = True
        for pool in _thread_pools:
            try:
                pool.shutdown(wait=True, cancel_futures=True)
            except Exception as e:  # pylint: disable=broad-except
                logger.error("Error shutting down thread pool", error=str(e))


class AsyncContextManager:
    """Base class for async context managers with proper cleanup."""

    async def __aenter__(self) -> Any:
        """Enter the async context.

        Returns:
            Self instance
        """
        return self

    async def __aexit__(self, exc_type: type | None, exc_val: Exception | None, exc_tb: Any) -> None:
        """Exit the async context.

        Args:
            exc_type: Exception type if an error occurred
            exc_val: Exception value if an error occurred
            exc_tb: Exception traceback if an error occurred
        """
        pass


class ThreadSafeEvent:
    """A thread-safe event wrapper for coordinating between threads and coroutines.

    This class provides a thread-safe wrapper around asyncio.Event that can be used
    to coordinate between threads and coroutines. The event loop is stored at creation
    time and used for all operations to ensure thread safety.
    """

    def __init__(self) -> None:
        """Initialize the event with the current event loop."""
        self._event = asyncio.Event()
        self._lock = threading.Lock()
        self._is_set = False
        self._closed = False
        try:
            self._loop = asyncio.get_running_loop()
        except RuntimeError:
            raise RuntimeError("ThreadSafeEvent must be created from an async context")

    def set(self) -> None:
        """Set the event in a thread-safe manner."""
        with self._lock:
            if self._closed:
                return
            self._is_set = True
            if self._event and not self._event.is_set():
                try:
                    self._loop.call_soon_threadsafe(self._event.set)
                except Exception as e:  # pylint: disable=broad-except
                    logger.error("Error setting event", error=str(e))

    def clear(self) -> None:
        """Clear the event in a thread-safe manner."""
        with self._lock:
            if self._closed:
                return
            self._is_set = False
            if self._event and self._event.is_set():
                try:
                    self._loop.call_soon_threadsafe(self._event.clear)
                except Exception as e:  # pylint: disable=broad-except
                    logger.error("Error clearing event", error=str(e))

    async def wait(self) -> None:
        """Wait for the event to be set.

        Raises:
            RuntimeError: If the event is closed
        """
        if self._closed:
            raise RuntimeError("Event is closed")
        if not self._is_set:
            await self._event.wait()

    def close(self) -> None:
        """Close the event and cleanup resources."""
        with self._lock:
            self._closed = True
            self._event = None
            self._loop = None  # type: ignore


class AsyncSemaphore(AsyncContextManager):
    """Thread-safe semaphore that can be used across async and sync code."""

    def __init__(self, value: int = 1) -> None:
        """Initialize the semaphore.

        Args:
            value: Initial semaphore value

        Raises:
            ValueError: If value is less than 1
        """
        super().__init__()
        if value < 1:
            raise ValueError("Semaphore initial value must be >= 1")
        self._value = value  # Store initial value
        self._semaphore = Semaphore(value)
        self._lock = threading.Lock()
        self._count = value  # Current available count
        self._waiters: list[asyncio.Future] = []
        self._closed = False
        try:
            self._loop = asyncio.get_running_loop()
        except RuntimeError:
            raise RuntimeError("AsyncSemaphore must be created from an async context")

    async def acquire(self) -> bool:
        """Acquire the semaphore.

        Returns:
            bool: True if acquired, False if closed

        Raises:
            RuntimeError: If semaphore is closed
        """
        if self._closed:
            raise RuntimeError("Semaphore is closed")

        # First acquire the underlying semaphore
        await self._semaphore.acquire()

        with self._lock:
            if self._count > 0:
                self._count -= 1
                return True

            # Create a waiter before releasing the lock
            waiter: asyncio.Future = asyncio.Future()
            self._waiters.append(waiter)

        try:
            # Wait for our turn
            await waiter
            self._count -= 1  # Decrement count after being woken up
            return True
        except Exception:  # pylint: disable=broad-except
            with self._lock:
                if waiter in self._waiters:
                    self._waiters.remove(waiter)
                self._semaphore.release()  # Release semaphore if we failed
            raise

    def release(self) -> None:
        """Release the semaphore.

        Raises:
            RuntimeError: If semaphore is closed
        """
        if self._closed:
            raise RuntimeError("Semaphore is closed")

        with self._lock:
            # Release the underlying semaphore first
            try:
                self._loop.call_soon_threadsafe(self._semaphore.release)
            except Exception as e:
                logger.error("Error releasing semaphore", error=str(e))
                raise

            # Update count and wake waiters
            self._count += 1
            if self._waiters:
                waiter = self._waiters.pop(0)
                if not waiter.done():
                    self._loop.call_soon_threadsafe(waiter.set_result, None)

    def close(self) -> None:
        """Close the semaphore and cleanup resources."""
        with self._lock:
            self._closed = True
            # Wake up all waiters with an error
            for waiter in self._waiters:
                if not waiter.done():
                    self._loop.call_soon_threadsafe(
                        waiter.set_exception, RuntimeError("Semaphore is closed")
                    )
            self._waiters.clear()
            # Release all held resources
            while self._count < self._value:
                self._loop.call_soon_threadsafe(self._semaphore.release)
                self._count += 1

    async def __aenter__(self) -> AsyncSemaphore:
        """Enter async context and acquire semaphore.

        Returns:
            Self instance

        Raises:
            RuntimeError: If semaphore is closed
        """
        await self.acquire()
        return self

    async def __aexit__(
        self, exc_type: type | None, exc_val: Exception | None, exc_tb: Any
    ) -> None:
        """Exit async context and release semaphore.

        Args:
            exc_type: Exception type if an error occurred
            exc_val: Exception value if an error occurred
            exc_tb: Exception traceback if an error occurred
        """
        self.release()


class ThreadPoolManager:
    """Manager for thread pools with proper lifecycle management."""

    def __init__(self) -> None:
        self._pools: weakref.WeakSet = weakref.WeakSet()
        self._lock = threading.Lock()
        self._closed = False

    def create_pool(self, *args, **kwargs) -> ThreadPoolExecutor:
        """Create and register a new thread pool.

        Args:
            *args: Positional arguments for ThreadPoolExecutor
            **kwargs: Keyword arguments for ThreadPoolExecutor

        Returns:
            ThreadPoolExecutor: New thread pool instance

        Raises:
            RuntimeError: If manager is closed
        """
        if self._closed:
            raise RuntimeError("ThreadPoolManager is closed")

        pool = ThreadPoolExecutor(*args, **kwargs)
        with self._lock:
            self._pools.add(pool)
        return pool

    def shutdown_all(self, wait: bool = True) -> None:
        """Shutdown all managed thread pools.

        Args:
            wait: Whether to wait for thread pools to finish
        """
        with self._lock:
            self._closed = True
            for pool in self._pools:
                try:
                    pool.shutdown(wait=wait, cancel_futures=True)
                except Exception as e:  # pylint: disable=broad-except
                    logger.error("Error shutting down thread pool", error=str(e))
            self._pools.clear()


# Global thread pool manager
thread_pool_manager = ThreadPoolManager()


async def gather_with_concurrency(limit: int, *tasks: Any, return_exceptions: bool = False) -> Any:
    """
    Wrap asyncio.gather to limit the number of concurrent tasks with improved error handling.

    Args:
        limit: Maximum number of concurrent tasks
        *tasks: Tasks to gather
        return_exceptions: Whether to return exceptions rather than raising them

    Returns:
        Results from the gathered tasks

    Raises:
        Exception: If a task fails and return_exceptions is False
        ValueError: If limit is less than 1
    """
    if limit < 1:
        raise ValueError("Concurrency limit must be >= 1")

    semaphore = AsyncSemaphore(limit)

    async def sem_task(task: Awaitable[Any]) -> Any:
        """Run a task with semaphore protection."""
        try:
            async with semaphore:
                return await task
        except Exception as e:  # pylint: disable=broad-except
            if return_exceptions:
                return e
            raise
        finally:
            # Ensure semaphore is released even if task is cancelled
            if not semaphore._closed:  # pylint: disable=protected-access
                try:
                    semaphore.release()
                except RuntimeError:
                    pass

    try:
        return await gather(*(sem_task(task) for task in tasks), return_exceptions=return_exceptions)
    except Exception as e:  # pylint: disable=broad-except
        logger.error("Error in gathered tasks", error=str(e))
        raise
    finally:
        semaphore.close()


def to_async(func: typing.Callable) -> typing.Callable:
    """
    Turn a sync function to async function using event loop with proper error handling.

    Args:
        func: Synchronous function to convert

    Returns:
        Async version of the function
    """

    @functools.wraps(func)
    async def run(*args, loop=None, executor=None, **kwargs):
        if loop is None:
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

        pfunc = functools.partial(func, *args, **kwargs)
        try:
            return await loop.run_in_executor(executor, pfunc)
        except Exception as e:  # pylint: disable=broad-except
            logger.error(
                "Error in async execution",
                function=func.__name__,
                error=str(e),
                args=args,
                kwargs=kwargs,
            )
            raise

    return run


def to_async_thread(fn):
    """
    Turn a sync function to async function using threads with proper cleanup.

    Args:
        fn: Synchronous function to convert

    Returns:
        Async version of the function
    """
    pool = thread_pool_manager.create_pool()

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        try:
            future = pool.submit(fn, *args, **kwargs)
            return asyncio.wrap_future(future)
        except Exception as e:  # pylint: disable=broad-except
            logger.error(
                "Error submitting to thread pool",
                function=fn.__name__,
                error=str(e),
                args=args,
                kwargs=kwargs,
            )
            raise

    return wrapper


def to_sync(fn):
    """
    Turn an async function to sync function with proper error handling.

    Args:
        fn: Async function to convert

    Returns:
        Sync version of the function
    """

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        res = fn(*args, **kwargs)
        if asyncio.iscoroutine(res):
            try:
                loop = asyncio.get_event_loop()
                return loop.run_until_complete(res)
            except Exception as e:  # pylint: disable=broad-except
                logger.error(
                    "Error in sync execution",
                    function=fn.__name__,
                    error=str(e),
                    args=args,
                    kwargs=kwargs,
                )
                raise
        return res

    return wrapper


def force_async(fn):
    """
    Turn a sync function to async function using threads with proper resource management.

    Args:
        fn: Synchronous function to convert

    Returns:
        Async version of the function
    """
    pool = thread_pool_manager.create_pool()

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        try:
            future = pool.submit(fn, *args, **kwargs)
            return asyncio.wrap_future(future)
        except Exception as e:  # pylint: disable=broad-except
            logger.error(
                "Error in forced async execution",
                function=fn.__name__,
                error=str(e),
                args=args,
                kwargs=kwargs,
            )
            raise

    return wrapper


def force_sync(fn):
    """
    Turn an async function to sync function with proper error handling.

    Args:
        fn: Async function to convert

    Returns:
        Sync version of the function
    """

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        res = fn(*args, **kwargs)
        if asyncio.iscoroutine(res):
            try:
                loop = asyncio.get_running_loop()
                return loop.run_until_complete(res)
            except Exception as e:  # pylint: disable=broad-except
                logger.error(
                    "Error in forced sync execution",
                    function=fn.__name__,
                    error=str(e),
                    args=args,
                    kwargs=kwargs,
                )
                raise
        return res

    return wrapper


def fire_coroutine_threadsafe(coro: Coroutine, loop: AbstractEventLoop) -> asyncio.Future:
    """
    Submit a coroutine object to a given event loop with improved safety.

    This method provides a way to run a coroutine on a different event loop
    and retrieve its result through the returned Future object.

    Args:
        coro: Coroutine to run
        loop: Event loop to run the coroutine in

    Returns:
        asyncio.Future: A Future object that will contain the result of the coroutine

    Raises:
        RuntimeError: If called from within the event loop
        TypeError: If coro is not a coroutine object
    """
    ident = loop.__dict__.get("_thread_ident")
    if ident is not None and ident == threading.get_ident():
        raise RuntimeError("Cannot be called from within the event loop")

    if not coroutines.iscoroutine(coro):
        raise TypeError(f"A coroutine object is required: {coro}")

    done_event = ThreadSafeEvent()
    future_ref: list[asyncio.Future | None] = [None]  # Use a list to store future reference
    future_ready = threading.Event()

    def callback() -> None:
        """Handle the firing of a coroutine with proper error handling."""
        try:
            future = ensure_future(coro, loop=loop)
            future_ref[0] = future  # Store future reference
            future.add_done_callback(lambda _: done_event.set())
            future_ready.set()  # Signal that future is ready

            def _on_cancel(fut: asyncio.Future) -> None:
                """Handle future cancellation."""
                if not fut.cancelled():
                    return
                # Get the task and cancel it
                task = asyncio.tasks._get_current_task(loop)  # type: ignore
                if task is not None:
                    task.cancel()

            future.add_done_callback(_on_cancel)
        except Exception as e:  # pylint: disable=broad-except
            logger.error("Error in coroutine", error=str(e))
            done_event.set()
            future_ready.set()

    try:
        loop.call_soon_threadsafe(callback)
        # Wait for the future to be created
        if not future_ready.wait(timeout=0.1):
            logger.warning("Timeout waiting for future to be created")
        return future_ref[0] if future_ref[0] is not None else asyncio.Future(loop=loop)
    except Exception as e:  # pylint: disable=broad-except
        logger.error("Error scheduling coroutine", error=str(e))
        raise


def run_callback_threadsafe(
    loop: AbstractEventLoop, callback: Callable[..., T], *args: Any
) -> concurrent.futures.Future[T]:  # pylint: disable=unsubscriptable-object
    """
    Submit a callback object to a given event loop with improved safety.

    Return a concurrent.futures.Future to access the result.

    Args:
        loop: Event loop to run the callback in
        callback: Function to call
        *args: Arguments to pass to the callback

    Returns:
        Future containing the callback result

    Raises:
        RuntimeError: If called from within the event loop or during shutdown
    """
    ident = loop.__dict__.get("_thread_ident")
    if ident is not None and ident == threading.get_ident():
        raise RuntimeError("Cannot be called from within the event loop")

    future: concurrent.futures.Future = concurrent.futures.Future()
    done_event = ThreadSafeEvent()

    def run_callback() -> None:
        """Run callback and store result with proper error handling."""
        try:
            result = callback(*args)
            future.set_result(result)
        except Exception as exc:  # pylint: disable=broad-except
            if future.set_running_or_notify_cancel():
                future.set_exception(exc)
            else:
                logger.warning("Exception on lost future", exc_info=True)
        finally:
            done_event.set()

    try:
        loop.call_soon_threadsafe(run_callback)
    except Exception as e:  # pylint: disable=broad-except
        logger.error("Error scheduling callback", error=str(e))
        future.set_exception(e)
        return future

    if hasattr(loop, _SHUTDOWN_RUN_CALLBACK_THREADSAFE):
        raise RuntimeError("The event loop is in the process of shutting down")

    return future


def check_loop() -> None:
    """
    Warn if called inside the event loop and provide guidance.

    Raises:
        RuntimeError: If I/O is detected inside the event loop
    """
    try:
        get_running_loop()
        in_loop = True
    except RuntimeError:
        in_loop = False

    if not in_loop:
        return

    found_frame = None

    for frame in reversed(extract_stack()):
        for path in ("custom_components/", "homeassistant/components/"):
            try:
                index = frame.filename.index(path)
                found_frame = frame
                break
            except ValueError:
                continue

        if found_frame is not None:
            break

    # Did not source from integration? Hard error.
    if found_frame is None:
        raise RuntimeError(
            "Detected I/O inside the event loop. This is causing stability issues. Please report issue"
        )

    start = index + len(path)
    end = found_frame.filename.index("/", start)

    integration = found_frame.filename[start:end]

    if path == "custom_components/":
        extra = " to the custom component author"
    else:
        extra = ""

    logger.warning(
        "Detected I/O inside the event loop. This is causing stability issues. Please report issue%s for %s doing I/O at %s, line %s: %s",
        extra,
        integration,
        found_frame.filename[index:],
        found_frame.lineno,
        found_frame.line.strip(),
    )

    raise RuntimeError(
        f"I/O must be done in the executor; Use `await loop.run_in_executor()` "
        f"at {found_frame.filename[index:]}, line {found_frame.lineno}: {found_frame.line.strip()}"
    )


def protect_loop(func: Callable) -> Callable:
    """
    Protect function from running in event loop.

    Args:
        func: Function to protect

    Returns:
        Protected function that cannot run in the event loop
    """

    @functools.wraps(func)
    def protected_loop_func(*args, **kwargs):  # type: ignore
        check_loop()
        return func(*args, **kwargs)

    return protected_loop_func


def async_timed():
    """Decorator for timing async functions with proper cleanup."""

    def wrapper(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapped(*args, **kwargs) -> Any:
            logger.debug(f"Starting {func.__name__}", args=args, kwargs=kwargs)
            start = time.time()
            try:
                return await func(*args, **kwargs)
            finally:
                end = time.time()
                total = end - start
                logger.debug(
                    f"Finished {func.__name__}",
                    duration=f"{total:.4f}s",
                    args=args,
                    kwargs=kwargs,
                )

        return wrapped

    return wrapper


def async_timer():
    """Decorator for timing async functions using Timer with proper cleanup."""

    def wrapper(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapped(*args, **kwargs) -> Any:
            timer = Timer(text=f"Task {func.__name__} elapsed time: {{:.1f}}")
            logger.debug(f"Starting {func.__name__}", args=args, kwargs=kwargs)
            timer.start()
            try:
                return await func(*args, **kwargs)
            finally:
                timer.stop()
                logger.debug(
                    f"Finished {func.__name__}",
                    duration=timer.last,
                    args=args,
                    kwargs=kwargs,
                )

        return wrapped

    return wrapper


def shutdown_run_callback_threadsafe(loop: AbstractEventLoop) -> None:
    """
    Call when run_callback_threadsafe should prevent creating new futures.

    We must finish all callbacks before the executor is shutdown
    or we can end up in a deadlock state where:

    `executor.result()` is waiting for its `._condition`
    and the executor shutdown is trying to `.join()` the
    executor thread.

    This function is considered irreversible and should only ever
    be called when Home Assistant is going to shutdown and
    python is going to exit.

    Args:
        loop: Event loop to mark as shutting down
    """
    setattr(loop, _SHUTDOWN_RUN_CALLBACK_THREADSAFE, True)
    cleanup_thread_pools()


# Register cleanup on module exit
import atexit


atexit.register(cleanup_thread_pools)

</document_content>
</document>
<document index="24">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/base.py</source>
<document_content>
"""Base utility functions for the democracy_exe package."""

from __future__ import annotations

import collections
import copy
import math

from typing import TYPE_CHECKING, Any, Union

import structlog


logger = structlog.get_logger(__name__)


if TYPE_CHECKING:
    import types

SEPARATOR_CHARACTER_DEFAULT = "-"
SEPARATOR_LENGTH_DEFAULT = 40


def get_sys_module() -> types.ModuleType:
    """
    Get the 'sys' module.

    Returns:
        types.ModuleType: The 'sys' module.
    """
    import sys  # pylint:disable=reimported,import-outside-toplevel

    return sys


def get_itertools_module() -> types.ModuleType:
    """
    Get the 'itertools' module.

    Returns:
        types.ModuleType: The 'itertools' module.
    """
    import itertools  # pylint:disable=reimported,import-outside-toplevel

    return itertools


# SOURCE: https://www.safaribooksonline.com/library/view/python-cookbook/0596001673/ch14s08.html
def check_whoami() -> tuple[str, int, str]:
    """
    Get information about the caller of the current function.

    By calling sys._getframe(1), you can get the function name, line number,
    and filename of the caller of the current function.

    Returns:
        Tuple[str, int, str]: A tuple containing the function name, line number,
            and filename of the caller.
    """
    sys = get_sys_module()
    this_function_name, this_line_number, this_filename = (
        sys._getframe(1).f_code.co_name,
        sys._getframe(1).f_lineno,
        sys._getframe(1).f_code.co_filename,
    )
    # logger.debug(f"{this_function_name}() @ {this_filename}:{this_line_number}")
    return this_function_name, this_line_number, this_filename


# SOURCE: https://www.oreilly.com/library/view/python-cookbook/0596001673/ch14s08.html
def check_callersname() -> tuple[str, int, str]:
    """
    Get information about the caller of the caller of the current function.

    Returns:
        Tuple[str, int, str]: A tuple containing the function name, line number,
            and filename of the caller's caller.
    """
    sys = get_sys_module()
    this_function_name, this_line_number, this_filename = (
        sys._getframe(2).f_code.co_name,
        sys._getframe(2).f_lineno,
        sys._getframe(2).f_code.co_filename,
    )
    # logger.debug(f"{this_function_name}() @ {this_filename}:{this_line_number}")
    return this_function_name, this_line_number, this_filename
    # return sys._getframe(2).f_code.co_name


def print_line_seperator(
    value: str,
    length: int = SEPARATOR_LENGTH_DEFAULT,
    char: str = SEPARATOR_CHARACTER_DEFAULT,
) -> None:
    """
    Print a line separator with the given value centered.

    Args:
        value (str): The value to be centered in the separator.
        length (int, optional): The total length of the separator.
            Defaults to SEPARATOR_LENGTH_DEFAULT.
        char (str, optional): The character to be used for the separator.
            Defaults to SEPARATOR_CHARACTER_DEFAULT.
    """
    output = value

    if len(value) < length:
        #   Update length based on insert length, less a space for margin.
        length -= len(value) + 2
        #   Halve the length and floor left side.
        left = math.floor(length / 2)
        right = left
        #   If odd number, add dropped remainder to right side.
        if length % 2 != 0:
            right += 1

        #   Surround insert with separators.
        output = f"{char * left} {value} {char * right}"

    print_output(output)


def print_output(*args, sep: str = " ", end: str = "\n") -> None:
    """
    Print the given arguments with the specified separator and end string.

    Args:
        *args: The arguments to be printed.
        sep (str, optional): The separator between the arguments.
            Defaults to " ".
        end (str, optional): The end string to be printed after the arguments.
            Defaults to "\\n".
    """
    print(*args, sep=sep, end=end)


# SOURCE: https://gist.github.com/89465127/5776892
def create_dict_from_filter(d: dict[Any, Any], white_list: list[Any]) -> dict[Any, Any]:
    """
    Create a new dictionary by filtering the given dictionary based on a white list of keys.

    Args:
        d (Dict[Any, Any]): The dictionary to be filtered.
        white_list (List[Any]): The list of keys to be included in the new dictionary.

    Returns:
        Dict[Any, Any]: A new dictionary containing only the keys from the white list.
    """
    return {k: v for k, v in filter(lambda t: t[0] in white_list, d.items())}


# NAME: Python filter nested dict given list of key names
# https://stackoverflow.com/questions/23230947/python-filter-nested-dict-given-list-of-key-names
def fltr(node: dict[Any, Any] | list[Any], whitelist: list[Any]) -> dict[Any, Any] | list[Any] | None:
    """
    Filter a nested dictionary or list based on a white list of keys.

    This function returns a new object rather than modifying the old one and
    handles filtering on non-leaf nodes.

    Args:
        node (Union[Dict[Any, Any], List[Any]]): The dictionary or list to be filtered.
        whitelist (List[Any]): The list of keys to be included in the filtered result.

    Returns:
        Union[Dict[Any, Any], List[Any], None]: The filtered dictionary, list, or None if the result is empty.

    Example:
        >>> x = {"a": 1, "b": {"c": 2, "d": 3}, "e": [{"f": 4, "g": 5}, {"h": 6}]}
        >>> fltr(x, ["a", "c", "f"])
        {'a': 1, 'b': {'c': 2}, 'e': [{'f': 4}]}
    """
    if isinstance(node, dict):
        retVal = {}
        for key in node:
            if key in whitelist:
                retVal[key] = copy.deepcopy(node[key])
            elif isinstance(node[key], list) or isinstance(node[key], dict):
                child = fltr(node[key], whitelist)
                if child:
                    retVal[key] = child
        if retVal:
            return retVal
        else:
            return None
    elif isinstance(node, list):
        retVal = []
        for entry in node:
            child = fltr(entry, whitelist)
            if child:
                retVal.append(child)
        if retVal:
            return retVal
        else:
            return None


# from unittest import TestCase
# SOURCE: https://gist.github.com/angstwad/bf22d1822c38a92ec0a9
def dict_merge(dct: dict[Any, Any], merge_dct: dict[Any, Any], add_keys: bool = True) -> dict[Any, Any]:
    """
    Recursively merge two dictionaries.

    Inspired by :meth:``dict.update()``, instead of updating only top-level keys,
    dict_merge recurses down into dicts nested to an arbitrary depth, updating keys.
    The ``merge_dct`` is merged into ``dct``.

    This version will return a copy of the dictionary and leave the original
    arguments untouched.

    The optional argument ``add_keys``, determines whether keys which are
    present in ``merge_dict`` but not ``dct`` should be included in the new dict.

    Args:
        dct (Dict[Any, Any]): The dictionary onto which the merge is executed.
        merge_dct (Dict[Any, Any]): The dictionary merged into dct.
        add_keys (bool, optional): Whether to add new keys from merge_dct that
            are not present in dct. Defaults to True.

    Returns:
        Dict[Any, Any]: The merged dictionary.
    """
    dct = dct.copy()
    if not add_keys:
        merge_dct = {k: merge_dct[k] for k in set(dct).intersection(set(merge_dct))}

    for k, v in merge_dct.items():
        if k in dct and isinstance(dct[k], dict) and isinstance(merge_dct[k], collections.abc.Mapping):
            dct[k] = dict_merge(dct[k], merge_dct[k], add_keys=add_keys)
        else:
            dct[k] = merge_dct[k]

    return dct


##########################################################################
# old
##########################################################################

# from __future__ import annotations

# import collections
# import copy
# import math
# from typing import Any, Dict, List, Tuple, Union



# SEPARATOR_CHARACTER_DEFAULT = "-"
# SEPARATOR_LENGTH_DEFAULT = 40


# def get_sys_module() -> types.ModuleType:
#     import sys  # pylint:disable=reimported,import-outside-toplevel

#     return sys


# def get_itertools_module() -> types.ModuleType:
#     import itertools  # pylint:disable=reimported,import-outside-toplevel

#     return itertools


# # SOURCE: https://www.safaribooksonline.com/library/view/python-cookbook/0596001673/ch14s08.html
# def check_whoami() -> Tuple[str, int, str]:
#     """By calling sys._getframe(1), you can get this information for the caller of the current function."""
#     sys = get_sys_module()
#     this_function_name, this_line_number, this_filename = (
#         sys._getframe(1).f_code.co_name,
#         sys._getframe(1).f_lineno,
#         sys._getframe(1).f_code.co_filename,
#     )
#     logger.debug(f"{this_function_name}() @ {this_filename}:{this_line_number}")
#     return this_function_name, this_line_number, this_filename


# # SOURCE: https://www.oreilly.com/library/view/python-cookbook/0596001673/ch14s08.html
# def check_callersname() -> Tuple[str, int, str]:
#     sys = get_sys_module()
#     this_function_name, this_line_number, this_filename = (
#         sys._getframe(2).f_code.co_name,
#         sys._getframe(2).f_lineno,
#         sys._getframe(2).f_code.co_filename,
#     )
#     logger.debug(f"{this_function_name}() @ {this_filename}:{this_line_number}")
#     return this_function_name, this_line_number, this_filename
#     # return sys._getframe(2).f_code.co_name


# def print_line_seperator(
#     value: str,
#     length: int = SEPARATOR_LENGTH_DEFAULT,
#     char: str = SEPARATOR_CHARACTER_DEFAULT,
# ) -> None:
#     output = value

#     if len(value) < length:
#         #   Update length based on insert length, less a space for margin.
#         length -= len(value) + 2
#         #   Halve the length and floor left side.
#         left = math.floor(length / 2)
#         right = left
#         #   If odd number, add dropped remainder to right side.
#         if length % 2 != 0:
#             right += 1

#         #   Surround insert with separators.
#         output = f"{char * left} {value} {char * right}"

#     print_output(output)


# def print_output(*args, sep: str = " ", end: str = "\n") -> None:
#     print(*args, sep=sep, end=end)


# # SOURCE: https://gist.github.com/89465127/5776892
# def create_dict_from_filter(d: Dict[Any, Any], white_list: List[Any]) -> Dict[Any, Any]:
#     """Filter by key"""
#     return {k: v for k, v in filter(lambda t: t[0] in white_list, d.items())}


# # NAME: Python filter nested dict given list of key names
# # https://stackoverflow.com/questions/23230947/python-filter-nested-dict-given-list-of-key-names
# def fltr(node: Union[Dict[Any, Any], List[Any]], whitelist: List[Any]) -> Union[Dict[Any, Any], List[Any], None]:
#     """
#     returns a new object rather than modifying the old one (and handles filtering on non-leaf nodes).
#     Example Usage: `fltr(x, ['dropdown_value', 'nm_field', 'url_app', 'dt_reg'])`
#     """
#     if isinstance(node, dict):
#         retVal = {}
#         for key in node:
#             if key in whitelist:
#                 retVal[key] = copy.deepcopy(node[key])
#             elif isinstance(node[key], list) or isinstance(node[key], dict):
#                 child = fltr(node[key], whitelist)
#                 if child:
#                     retVal[key] = child
#         if retVal:
#             return retVal
#         else:
#             return None
#     elif isinstance(node, list):
#         retVal = []
#         for entry in node:
#             child = fltr(entry, whitelist)
#             if child:
#                 retVal.append(child)
#         if retVal:
#             return retVal
#         else:
#             return None


# # from unittest import TestCase
# # SOURCE: https://gist.github.com/angstwad/bf22d1822c38a92ec0a9
# def dict_merge(dct: Dict[Any, Any], merge_dct: Dict[Any, Any], add_keys: bool = True) -> Dict[Any, Any]:
#     """Recursive dict merge. Inspired by :meth:``dict.update()``, instead of
#     updating only top-level keys, dict_merge recurses down into dicts nested
#     to an arbitrary depth, updating keys. The ``merge_dct`` is merged into
#     ``dct``.

#     This version will return a copy of the dictionary and leave the original
#     arguments untouched.

#     The optional argument ``add_keys``, determines whether keys which are
#     present in ``merge_dict`` but not ``dct`` should be included in the
#     new dict.

#     Args:
#         dct (dict) onto which the merge is executed
#         merge_dct (dict): dct merged into dct
#         add_keys (bool): whether to add new keys

#     Returns:
#         dict: updated dict
#     """
#     dct = dct.copy()
#     if not add_keys:
#         merge_dct = {k: merge_dct[k] for k in set(dct).intersection(set(merge_dct))}

#     for k, v in merge_dct.items():
#         if k in dct and isinstance(dct[k], dict) and isinstance(merge_dct[k], collections.abc.Mapping):
#             dct[k] = dict_merge(dct[k], merge_dct[k], add_keys=add_keys)
#         else:
#             dct[k] = merge_dct[k]

#     return dct

</document_content>
</document>
<document index="25">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/base_context.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
"""Base context implementation for Discord commands.

This module contains the base context class used for Discord command handling.
"""
from __future__ import annotations

import io

from collections.abc import Callable, Iterable
from typing import TYPE_CHECKING, Any, Generic, Optional, Protocol, TypeVar, Union

import discord

from discord.ext import commands


if TYPE_CHECKING:
    from types import TracebackType

    from aiohttp import ClientSession
    from redis.asyncio import ConnectionPool as RedisConnectionPool

T = TypeVar("T")

class ConnectionContextManager(Protocol):
    """Protocol defining the connection context manager interface."""

    async def __aenter__(self) -> RedisConnectionPool: ...

    async def __aexit__(
        self,
        exc_type: type[BaseException] | None,
        exc_value: BaseException | None,
        traceback: TracebackType | None,
    ) -> None: ...


class ConfirmationView(discord.ui.View):
    """View for confirmation dialogs."""

    def __init__(self, *, timeout: float, author_id: int, delete_after: bool) -> None:
        """Initialize the confirmation view.

        Args:
            timeout: How long to wait before timing out
            author_id: ID of the user who can interact with this view
            delete_after: Whether to delete the message after confirmation
        """
        super().__init__(timeout=timeout)
        self.value: bool | None = None
        self.delete_after: bool = delete_after
        self.author_id: int = author_id
        self.message: discord.Message | None = None

    async def interaction_check(self, interaction: discord.Interaction) -> bool:
        """Check if the interaction is from the authorized user.

        Args:
            interaction: The interaction to check

        Returns:
            bool: Whether the interaction is valid
        """
        if interaction.user and interaction.user.id == self.author_id:
            return True
        await interaction.response.send_message("This confirmation dialog is not for you.", ephemeral=True)
        return False

    async def on_timeout(self) -> None:
        """Handle view timeout."""
        if self.delete_after and self.message:
            await self.message.delete()

    @discord.ui.button(label="Confirm", style=discord.ButtonStyle.green)
    async def confirm(self, interaction: discord.Interaction, button: discord.ui.Button):
        """Handle confirmation button click.

        Args:
            interaction: The button interaction
            button: The button that was clicked
        """
        self.value = True
        await interaction.response.defer()
        if self.delete_after:
            await interaction.delete_original_response()
        self.stop()

    @discord.ui.button(label="Cancel", style=discord.ButtonStyle.red)
    async def cancel(self, interaction: discord.Interaction, button: discord.ui.Button):
        """Handle cancellation button click.

        Args:
            interaction: The button interaction
            button: The button that was clicked
        """
        self.value = False
        await interaction.response.defer()
        if self.delete_after:
            await interaction.delete_original_response()
        self.stop()


class DisambiguatorView(discord.ui.View, Generic[T]):
    """View for disambiguation selection menus."""

    message: discord.Message
    selected: T

    def __init__(self, ctx: BaseContext, data: list[T], entry: Callable[[T], Any]):
        """Initialize the disambiguator view.

        Args:
            ctx: The command context
            data: List of items to disambiguate
            entry: Function to convert items to select options
        """
        super().__init__()
        self.ctx: BaseContext = ctx
        self.data: list[T] = data

        options = []
        for i, x in enumerate(data):
            opt = entry(x)
            if not isinstance(opt, discord.SelectOption):
                opt = discord.SelectOption(label=str(opt))
            opt.value = str(i)
            options.append(opt)

        select = discord.ui.Select(options=options)
        select.callback = self.on_select_submit
        self.select = select
        self.add_item(select)

    async def interaction_check(self, interaction: discord.Interaction) -> bool:
        """Check if the interaction is from the authorized user.

        Args:
            interaction: The interaction to check

        Returns:
            bool: Whether the interaction is valid
        """
        if interaction.user.id != self.ctx.author.id:
            await interaction.response.send_message("This select menu is not meant for you, sorry.", ephemeral=True)
            return False
        return True

    async def on_select_submit(self, interaction: discord.Interaction):
        """Handle select menu submission.

        Args:
            interaction: The select menu interaction
        """
        index = int(self.select.values[0])
        self.selected = self.data[index]
        await interaction.response.defer()
        if not self.message.flags.ephemeral:
            await self.message.delete()
        self.stop()


class BaseContext(commands.Context):
    """Base context class for Discord commands."""

    channel: discord.VoiceChannel | discord.TextChannel | discord.Thread | discord.DMChannel
    prefix: str
    command: commands.Command[Any, ..., Any]

    def __init__(self, **kwargs):
        """Initialize the base context.

        Args:
            **kwargs: Keyword arguments to pass to the parent class
        """
        super().__init__(**kwargs)
        self.pool: RedisConnectionPool | None = None

    async def entry_to_code(self, entries: Iterable[tuple[str, str]]) -> None:
        """Convert entries to a code block and send it.

        Args:
            entries: Iterable of name-value pairs to format
        """
        width = max(len(a) for a, b in entries)
        output = ["```"]
        output.extend(f"{name:<{width}}: {entry}" for name, entry in entries)
        output.append("```")
        await self.send("\n".join(output))

    async def indented_entry_to_code(self, entries: Iterable[tuple[str, str]]) -> None:
        """Convert entries to an indented code block and send it.

        Args:
            entries: Iterable of name-value pairs to format
        """
        width = max(len(a) for a, b in entries)
        output = ["```"]
        output.extend(f"\u200b{name:>{width}}: {entry}" for name, entry in entries)
        output.append("```")
        await self.send("\n".join(output))

    def __repr__(self) -> str:
        """Get string representation of the context.

        Returns:
            str: String representation
        """
        return "<Context>"

    @property
    def session(self) -> ClientSession:
        """Get the client session.

        Returns:
            ClientSession: The client session
        """
        return self.bot.session

    @discord.utils.cached_property
    def replied_reference(self) -> discord.MessageReference | None:
        """Get the message reference for a reply.

        Returns:
            Optional[MessageReference]: The message reference if this is a reply
        """
        ref = self.message.reference
        if ref and isinstance(ref.resolved, discord.Message):
            return ref.resolved.to_reference()
        return None

    @discord.utils.cached_property
    def replied_message(self) -> discord.Message | None:
        """Get the message being replied to.

        Returns:
            Optional[Message]: The message if this is a reply
        """
        ref = self.message.reference
        if ref and isinstance(ref.resolved, discord.Message):
            return ref.resolved
        return None

    async def disambiguate(self, matches: list[T], entry: Callable[[T], Any], *, ephemeral: bool = False) -> T:
        """Disambiguate between multiple matches.

        Args:
            matches: List of items to disambiguate between
            entry: Function to convert items to select options
            ephemeral: Whether the disambiguation message should be ephemeral

        Returns:
            T: The selected item

        Raises:
            ValueError: If there are no matches or too many matches
        """
        if not matches:
            raise ValueError("No results found.")

        if len(matches) == 1:
            return matches[0]

        if len(matches) > 25:
            raise ValueError("Too many results... sorry.")

        view = DisambiguatorView(self, matches, entry)
        view.message = await self.send(
            "There are too many matches... Which one did you mean?",
            view=view,
            ephemeral=ephemeral,
        )
        await view.wait()
        return view.selected

    async def prompt(
        self,
        message: str,
        *,
        timeout: float = 60.0,
        delete_after: bool = True,
        author_id: int | None = None,
    ) -> bool | None:
        """An interactive reaction confirmation dialog.

        Args:
            message: The message to show along with the prompt
            timeout: How long to wait before returning
            delete_after: Whether to delete the confirmation message after we're done
            author_id: The member who should respond to the prompt. Defaults to the author of the
                Context's message

        Returns:
            Optional[bool]: True if explicit confirm, False if explicit deny, None if deny due to timeout
        """
        author_id = author_id or self.author.id
        view = ConfirmationView(
            timeout=timeout,
            delete_after=delete_after,
            author_id=author_id,
        )
        view.message = await self.send(message, view=view, ephemeral=delete_after)
        await view.wait()
        return view.value

    def tick(self, opt: bool | None, label: str | None = None) -> str:
        """Get a tick emoji based on a boolean value.

        Args:
            opt: The boolean value to convert to an emoji
            label: Optional label to append after the emoji

        Returns:
            str: The emoji string
        """
        lookup = {
            True: "<:greenTick:330090705336664065>",
            False: "<:redTick:330090723011592193>",
            None: "<:greyTick:563231201280917524>",
        }
        emoji = lookup.get(opt, "<:redTick:330090723011592193>")
        return f"{emoji}: {label}" if label is not None else emoji

    @property
    def db(self) -> RedisConnectionPool:
        """Get the database connection pool.

        Returns:
            RedisConnectionPool: The database connection pool
        """
        return self.pool

    async def show_help(self, command: Any = None) -> None:
        """Show the help command for the specified command if given.

        If no command is given, then it'll show help for the current command.

        Args:
            command: The command to show help for
        """
        cmd = self.bot.get_command("help")
        if cmd is None:
            return

        if command is None:
            command = self.command.qualified_name if self.command else None

        await self.invoke(cmd, command=command)

</document_content>
</document>
<document index="26">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/block.py</source>
<document_content>

</document_content>
</document>
<document index="27">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/bot_context.py</source>
<document_content>
"""Bot-specific context implementation.

This module contains the bot-specific context class that extends the base context.
"""
from __future__ import annotations

from typing import TYPE_CHECKING, Any

import discord

from discord.ext import commands

from democracy_exe.chatbot.core.types import BotProtocol
from democracy_exe.utils.base_context import BaseContext


if TYPE_CHECKING:
    from redis.asyncio import ConnectionPool as RedisConnectionPool

class Context(BaseContext):
    """Bot-specific context class for Discord commands."""

    bot: BotProtocol

    def __init__(self, **kwargs: Any) -> None:
        """Initialize the bot context.

        Args:
            **kwargs: Keyword arguments to pass to the parent class
        """
        super().__init__(**kwargs)
        self.pool: RedisConnectionPool | None = self.bot.pool

</document_content>
</document>
<document index="28">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/chat_formatting.py</source>
<document_content>
# pylint: disable=no-member
# pylint: disable=possibly-used-before-assignment
# pyright: reportImportCycles=false
# pyright: reportAttributeAccessIssue=false
# mypy: disable-error-code="index"
# mypy: disable-error-code="no-redef"

"""democracy_exe.utils.chat_formatting"""

from __future__ import annotations

import datetime
import itertools
import textwrap

from collections.abc import Iterator, Sequence
from io import BytesIO
from typing import Optional, SupportsInt, Union

import discord

from babel.lists import format_list as babel_list
from babel.numbers import format_decimal


def error(text: str) -> str:
    """
    Get text prefixed with an error emoji.

    Parameters
    ----------
    text : str
        The text to be prefixed.

    Returns
    -------
    str
        The new message.

    """
    return f"\N{NO ENTRY SIGN} {text}"


def warning(text: str) -> str:
    """
    Get text prefixed with a warning emoji.

    Parameters
    ----------
    text : str
        The text to be prefixed.

    Returns
    -------
    str
        The new message.

    """
    return f"\N{WARNING SIGN}\N{VARIATION SELECTOR-16} {text}"


def info(text: str) -> str:
    """
    Get text prefixed with an info emoji.

    Parameters
    ----------
    text : str
        The text to be prefixed.

    Returns
    -------
    str
        The new message.

    """
    return f"\N{INFORMATION SOURCE}\N{VARIATION SELECTOR-16} {text}"


def success(text: str) -> str:
    """
    Get text prefixed with a success emoji.

    Parameters
    ----------
    text : str
        The text to be prefixed.

    Returns
    -------
    str
        The new message.

    """
    return f"\N{WHITE HEAVY CHECK MARK} {text}"


def question(text: str) -> str:
    """
    Get text prefixed with a question emoji.

    Parameters
    ----------
    text : str
        The text to be prefixed.

    Returns
    -------
    str
        The new message.

    """
    return f"\N{BLACK QUESTION MARK ORNAMENT}\N{VARIATION SELECTOR-16} {text}"


def bold(text: str, escape_formatting: bool = True) -> str:
    """
    Get the given text in bold.

    Note: By default, this function will escape ``text`` prior to emboldening.

    Parameters
    ----------
    text : str
        The text to be marked up.
    escape_formatting : `bool`, optional
        Set to :code:`False` to not escape markdown formatting in the text.

    Returns
    -------
    str
        The marked up text.

    """
    return f"**{escape(text, formatting=escape_formatting)}**"


def box(text: str, lang: str = "") -> str:
    """
    Get the given text in a code block.

    Parameters
    ----------
    text : str
        The text to be marked up.
    lang : `str`, optional
        The syntax highlighting language for the codeblock.

    Returns
    -------
    str
        The marked up text.

    """
    return f"```{lang}\n{text}\n```"


def inline(text: str) -> str:
    """
    Get the given text as inline code.

    Parameters
    ----------
    text : str
        The text to be marked up.

    Returns
    -------
    str
        The marked up text.

    """
    return f"``{text}``" if "`" in text else f"`{text}`"


def italics(text: str, escape_formatting: bool = True) -> str:
    """
    Get the given text in italics.

    Note: By default, this function will escape ``text`` prior to italicising.

    Parameters
    ----------
    text : str
        The text to be marked up.
    escape_formatting : `bool`, optional
        Set to :code:`False` to not escape markdown formatting in the text.

    Returns
    -------
    str
        The marked up text.

    """
    return f"*{escape(text, formatting=escape_formatting)}*"


def spoiler(text: str, escape_formatting: bool = True) -> str:
    """
    Get the given text as a spoiler.

    Note: By default, this function will escape ``text`` prior to making the text a spoiler.

    Parameters
    ----------
    text : str
        The text to be marked up.
    escape_formatting : `bool`, optional
        Set to :code:`False` to not escape markdown formatting in the text.

    Returns
    -------
    str
        The marked up text.

    """
    return f"||{escape(text, formatting=escape_formatting)}||"


def bordered(*columns: Sequence[str], ascii_border: bool = False) -> str:
    """
    Get two blocks of text inside borders.

    Note
    ----
    This will only work with a monospaced font.

    Parameters
    ----------
    *columns : `sequence` of `str`
        The columns of text, each being a list of lines in that column.
    ascii_border : bool
        Whether or not the border should be pure ASCII.

    Returns
    -------
    str
        The bordered text.

    """
    borders = {
        "TL": "+" if ascii_border else "",  # Top-left
        "TR": "+" if ascii_border else "",  # Top-right
        "BL": "+" if ascii_border else "",  # Bottom-left
        "BR": "+" if ascii_border else "",  # Bottom-right
        "HZ": "-" if ascii_border else "",  # Horizontal
        "VT": "|" if ascii_border else "",  # Vertical
    }

    sep = " " * 4  # Separator between boxes
    widths = tuple(max(len(row) for row in column) + 9 for column in columns)  # width of each col
    colsdone = [False] * len(columns)  # whether or not each column is done
    lines = [sep.join("{TL}" + "{HZ}" * width + "{TR}" for width in widths)]

    for line in itertools.zip_longest(*columns):
        row = []
        for colidx, column in enumerate(line):
            width = widths[colidx]
            done = colsdone[colidx]
            if column is None:
                if not done:
                    # bottom border of column
                    column = "{HZ}" * width
                    row.append("{BL}" + column + "{BR}")
                    colsdone[colidx] = True  # mark column as done
                else:
                    # leave empty
                    row.append(" " * (width + 2))
            else:
                column += " " * (width - len(column))  # append padded spaces
                row.append("{VT}" + column + "{VT}")

        lines.append(sep.join(row))

    final_row = []
    for width, done in zip(widths, colsdone, strict=False):
        if not done:
            final_row.append("{BL}" + "{HZ}" * width + "{BR}")
        else:
            final_row.append(" " * (width + 2))
    lines.append(sep.join(final_row))

    return "\n".join(lines).format(**borders)


def pagify(
    text: str,
    delims: Sequence[str] = None,
    *,
    priority: bool = False,
    escape_mass_mentions: bool = True,
    shorten_by: int = 8,
    page_length: int = 2000,
) -> Iterator[str]:
    """
    Generate multiple pages from the given text.

    Note
    ----
    This does not respect code blocks or inline code.

    Parameters
    ----------
    text : str
        The content to pagify and send.
    delims : `sequence` of `str`, optional
        Characters where page breaks will occur. If no delimiters are found
        in a page, the page will break after ``page_length`` characters.
        By default this only contains the newline.

    Other Parameters
    ----------------
    priority : `bool`
        Set to :code:`True` to choose the page break delimiter based on the
        order of ``delims``. Otherwise, the page will always break at the
        last possible delimiter.
    escape_mass_mentions : `bool`
        If :code:`True`, any mass mentions (here or everyone) will be
        silenced.
    shorten_by : `int`
        How much to shorten each page by. Defaults to 8.
    page_length : `int`
        The maximum length of each page. Defaults to 2000.

    Yields
    ------
    `str`
        Pages of the given text.

    """
    if delims is None:
        delims = ["\n"]
    in_text = text
    page_length -= shorten_by
    while len(in_text) > page_length:
        this_page_len = page_length
        if escape_mass_mentions:
            this_page_len -= in_text.count("@here", 0, page_length) + in_text.count("@everyone", 0, page_length)
        closest_delim = (in_text.rfind(d, 1, this_page_len) for d in delims)
        if priority:
            closest_delim = next((x for x in closest_delim if x > 0), -1)
        else:
            closest_delim = max(closest_delim)
        closest_delim = closest_delim if closest_delim != -1 else this_page_len
        if escape_mass_mentions:
            to_send = escape(in_text[:closest_delim], mass_mentions=True)
        else:
            to_send = in_text[:closest_delim]
        if len(to_send.strip()) > 0:
            yield to_send
        in_text = in_text[closest_delim:]

    if len(in_text.strip()) > 0:
        if escape_mass_mentions:
            yield escape(in_text, mass_mentions=True)
        else:
            yield in_text


def strikethrough(text: str, escape_formatting: bool = True) -> str:
    """
    Get the given text with a strikethrough.

    Note: By default, this function will escape ``text`` prior to applying a strikethrough.

    Parameters
    ----------
    text : str
        The text to be marked up.
    escape_formatting : `bool`, optional
        Set to :code:`False` to not escape markdown formatting in the text.

    Returns
    -------
    str
        The marked up text.

    """
    return f"~~{escape(text, formatting=escape_formatting)}~~"


def underline(text: str, escape_formatting: bool = True) -> str:
    """
    Get the given text with an underline.

    Note: By default, this function will escape ``text`` prior to underlining.

    Parameters
    ----------
    text : str
        The text to be marked up.
    escape_formatting : `bool`, optional
        Set to :code:`False` to not escape markdown formatting in the text.

    Returns
    -------
    str
        The marked up text.

    """
    return f"__{escape(text, formatting=escape_formatting)}__"


def quote(text: str) -> str:
    """
    Quotes the given text.

    Parameters
    ----------
    text : str
        The text to be marked up.

    Returns
    -------
    str
        The marked up text.

    """
    return textwrap.indent(text, "> ", lambda l: True)


def escape(text: str, *, mass_mentions: bool = False, formatting: bool = False) -> str:
    """
    Get text with all mass mentions or markdown escaped.

    Parameters
    ----------
    text : str
        The text to be escaped.
    mass_mentions : `bool`, optional
        Set to :code:`True` to escape mass mentions in the text.
    formatting : `bool`, optional
        Set to :code:`True` to escape any markdown formatting in the text.

    Returns
    -------
    str
        The escaped text.

    """
    if mass_mentions:
        text = text.replace("@everyone", "@\u200beveryone")
        text = text.replace("@here", "@\u200bhere")
    if formatting:
        text = discord.utils.escape_markdown(text)
    return text


def humanize_list(items: Sequence[str], *, locale: str | None = None, style: str = "standard") -> str:
    """
    Get comma-separated list, with the last element joined with *and*.

    Parameters
    ----------
    items : Sequence[str]
        The items of the list to join together.
    locale : Optional[str]
        The locale to convert, if not specified it defaults to the bot's locale.
    style : str
        The style to format the list with.

        Note: Not all styles are necessarily available in all locales,
        see documentation of `babel.lists.format_list` for more details.

        standard
            A typical 'and' list for arbitrary placeholders.
            eg. "January, February, and March"
        standard-short
             A short version of a 'and' list, suitable for use with short or
             abbreviated placeholder values.
             eg. "Jan., Feb., and Mar."
        or
            A typical 'or' list for arbitrary placeholders.
            eg. "January, February, or March"
        or-short
            A short version of an 'or' list.
            eg. "Jan., Feb., or Mar."
        unit
            A list suitable for wide units.
            eg. "3 feet, 7 inches"
        unit-short
            A list suitable for short units
            eg. "3 ft, 7 in"
        unit-narrow
            A list suitable for narrow units, where space on the screen is very limited.
            eg. "3 7"

    Raises
    ------
    ValueError
        The locale does not support the specified style.

    Examples
    --------
    .. testsetup::

        from democracy_exe.core.utils.chat_formatting import humanize_list

    .. doctest::

        >>> humanize_list(["One", "Two", "Three"])
        'One, Two, and Three'
        >>> humanize_list(["One"])
        'One'
        >>> humanize_list(["omena", "peruna", "aplari"], style="or", locale="fi")
        'omena, peruna tai aplari'

    """
    return babel_list(items, style=style, locale="en-US")


def format_perms_list(perms: discord.Permissions) -> str:
    """
    Format a list of permission names.

    This will return a humanized list of the names of all enabled
    permissions in the provided `discord.Permissions` object.

    Parameters
    ----------
    perms : discord.Permissions
        The permissions object with the requested permissions to list
        enabled.

    Returns
    -------
    str
        The humanized list.

    """
    perm_names: list[str] = []
    for perm, value in perms:
        if value is True:
            perm_name = '"' + perm.replace("_", " ").title() + '"'
            perm_names.append(perm_name)
    return humanize_list(perm_names).replace("Guild", "Server")


def humanize_timedelta(
    *,
    timedelta: datetime.timedelta | None = None,
    seconds: SupportsInt | None = None,
) -> str:
    """
    Get a locale aware human timedelta representation.

    This works with either a timedelta object or a number of seconds.

    Fractional values will be omitted, and values less than 1 second
    an empty string.

    Parameters
    ----------
    timedelta: Optional[datetime.timedelta]
        A timedelta object
    seconds: Optional[SupportsInt]
        A number of seconds

    Returns
    -------
    str
        A locale aware representation of the timedelta or seconds.

    Raises
    ------
    ValueError
        The function was called with neither a number of seconds nor a timedelta object

    """
    try:
        obj = seconds if seconds is not None else timedelta.total_seconds()
    except AttributeError as e:
        raise ValueError("You must provide either a timedelta or a number of seconds") from e

    seconds = int(obj)
    periods = [
        (("year"), ("years"), 60 * 60 * 24 * 365),
        (("month"), ("months"), 60 * 60 * 24 * 30),
        (("day"), ("days"), 60 * 60 * 24),
        (("hour"), ("hours"), 60 * 60),
        (("minute"), ("minutes"), 60),
        (("second"), ("seconds"), 1),
    ]

    strings = []
    for period_name, plural_period_name, period_seconds in periods:
        if seconds >= period_seconds:
            period_value, seconds = divmod(seconds, period_seconds)
            if period_value == 0:
                continue
            unit = plural_period_name if period_value > 1 else period_name
            strings.append(f"{period_value} {unit}")

    return ", ".join(strings)


def humanize_number(val: int | float, override_locale=None) -> str:
    """
    Convert an int or float to a str with digit separators based on bot locale

    Parameters
    ----------
    val : Union[int, float]
        The int/float to be formatted.
    override_locale: Optional[str]
        A value to override bot's regional format.

    Returns
    -------
    str
        locale aware formatted number.

    """
    return format_decimal(val, locale="en-US")


def text_to_file(
    text: str,
    filename: str = "file.txt",
    *,
    spoiler: bool = False,
    encoding: str = "utf-8",
):
    """
    Prepares text to be sent as a file on Discord, without character limit.

    This writes text into a bytes object that can be used for the ``file`` or ``files`` parameters
    of :meth:`discord.abc.Messageable.send`.

    Parameters
    ----------
    text: str
        The text to put in your file.
    filename: str
        The name of the file sent. Defaults to ``file.txt``.
    spoiler: bool
        Whether the attachment is a spoiler. Defaults to ``False``.

    Returns
    -------
    discord.File
        The file containing your text.

    """
    file = BytesIO(text.encode(encoding))
    return discord.File(file, filename, spoiler=spoiler)

</document_content>
</document>
<document index="29">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/checks.py</source>
<document_content>
"""Utility methods relating to general object checks."""

from __future__ import annotations

from typing import Any


def ifnone(val: Any, default: Any):
    """Return the given value if it is not None, else return the default."""
    return val if val is not None else default

</document_content>
</document>
<document index="30">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/collections_io.py</source>
<document_content>
"""Utility functions to import/export collections using CSV postgres files."""

from __future__ import annotations

from os import path

# from democracy_exe.ai import collections, connection


# def export_collection_data(
#     folder_path: str,
#     collection: str,
# ):
#     """Export collection data using `psql`"""
#     if not collections.collection_name_exists(collection):
#         raise ValueError(f"Collection '{collection}' was not found")

#     csv_file_collection = f"{folder_path}/{collection}-collection.csv"
#     csv_file_embedding = f"{folder_path}/{collection}-embedding.csv"
#     if path.exists(csv_file_collection):
#         raise ValueError(f"CSV file {csv_file_collection} already exists, exiting")
#     if path.exists(csv_file_embedding):
#         raise ValueError(f"CSV file {csv_file_embedding} already exists, exiting")

#     select = f"SELECT * FROM langchain_pg_collection WHERE name = '{collection}'"
#     copy = f"\\copy ({select}) TO '{csv_file_collection}' WITH DELIMITER ',' CSV HEADER"
#     res = connection.psql_command(cmd=copy)
#     if not res:
#         return

#     print(f"Collection record '{collection}' exported to '{csv_file_collection}'")

#     sub = f"SELECT uuid from langchain_pg_collection WHERE name = '{collection}'"
#     select = f"SELECT * FROM langchain_pg_embedding WHERE collection_id = ({sub})"
#     copy = f"\\copy ({select}) TO '{csv_file_embedding}' WITH DELIMITER ',' CSV HEADER"
#     res = connection.psql_command(cmd=copy)
#     if not res:
#         return

#     print(f"Collection embedding for '{collection}' exported to '{csv_file_embedding}'")


# def import_collection_data(
#     folder_path: str,
#     collection: str,
# ):
#     """Import collection data using `psql`"""
#     if collections.collection_name_exists(collection):
#         raise ValueError(f"Collection '{collection}' already exists, exiting")

#     csv_file_collection = f"{folder_path}/{collection}-collection.csv"
#     csv_file_embedding = f"{folder_path}/{collection}-embedding.csv"
#     print(f"Importing from {csv_file_collection} and {csv_file_embedding}")
#     if not path.exists(csv_file_collection):
#         raise ValueError(f"CSV file {csv_file_collection} not found, exiting")
#     if not path.exists(csv_file_embedding):
#         raise ValueError(f"CSV file {csv_file_embedding} not found, exiting")

#     table = "langchain_pg_collection"
#     copy = f"\\copy {table} FROM '{csv_file_collection}' WITH DELIMITER ',' CSV HEADER"
#     res = connection.psql_command(cmd=copy)
#     if not res:
#         return

#     table = "langchain_pg_embedding"
#     copy = f"\\copy {table} FROM '{csv_file_embedding}' WITH DELIMITER ',' CSV HEADER"
#     res = connection.psql_command(cmd=copy)
#     if not res:
#         return

#     print(f"Collection '{collection}' imported")

</document_content>
</document>
<document index="31">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/config.py</source>
<document_content>

</document_content>
</document>
<document index="32">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/context.py</source>
<document_content>
# pylint: disable=no-member
# pylint: disable=possibly-used-before-assignment
# pyright: reportImportCycles=false
# pyright: reportAttributeAccessIssue=false
# mypy: disable-error-code="index"
# mypy: disable-error-code="no-redef"

"""Context module re-exports.

This module re-exports the Context class from bot_context to maintain backward compatibility.
"""
from __future__ import annotations

from democracy_exe.utils.bot_context import Context


__all__ = ["Context"]

</document_content>
</document>
<document index="33">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/dropbox_.py</source>
<document_content>
"""democracy_exe.utils.dropbox_"""
# pylint: disable=unused-import
# pylint: disable=no-value-for-parameter
# pyright: reportAttributeAccessIssue=false
# pylint: disable=no-member
from __future__ import annotations

import asyncio
import contextlib
import datetime
import logging
import os
import pathlib
import sys
import time
import traceback
import unicodedata

from collections import defaultdict
from collections.abc import Generator
from typing import Any, Dict, List, Optional, Tuple, Union, cast

import aiofiles
import dropbox
import requests
import rich
import six
import structlog

from dropbox import DropboxOAuth2FlowNoRedirect, create_session
from dropbox.dropbox_client import BadInputException
from dropbox.exceptions import ApiError, AuthError
from dropbox.files import FileMetadata, FolderMetadata, WriteMode
from dropbox.users import Account


logger = structlog.get_logger(__name__)
from pydantic import Field, SecretStr

from democracy_exe.aio_settings import aiosettings


async def get_dropbox_session() -> requests.Session:
    """Get a new Dropbox session asynchronously.

    Returns:
        requests.Session: A configured session for Dropbox API requests.
    """
    return await asyncio.to_thread(create_session)


async def get_dropbox_client(oauth2_access_token: str | None = None) -> dropbox.Dropbox | None:  # pylint: disable=no-member
    """Create and initialize a Dropbox client.

    Args:
        oauth2_access_token: Optional OAuth2 access token. If not provided, uses token from settings.

    Returns:
        dropbox.Dropbox | None: An authenticated Dropbox client or None if initialization fails.

    Raises:
        BadInputException: If the provided token is invalid.
        AuthError: If authentication fails.
        Exception: For other unexpected errors.
    """
    dbx = None
    try:
        if oauth2_access_token is None:
            # token_secret: SecretStr = cast(SecretStr, aiosettings.dropbox_cerebro_token)
            oauth2_access_token = aiosettings.dropbox_cerebro_token.get_secret_value()  # pylint: disable=no-member
        dbx = dropbox.Dropbox(oauth2_access_token=oauth2_access_token)
        # Run potentially blocking operation in thread pool
        await asyncio.to_thread(dbx.users_get_current_account)
        logger.info("Connected to Dropbox successfully")
    except BadInputException as ex:
        logger.error(f"Error Class: {ex.__class__!s}")
        output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
        logger.warning(output)
        logger.error(f"exc_type: {sys.exc_info()[0]}")
        logger.error(f"exc_value: {sys.exc_info()[1]}")
        traceback.print_tb(sys.exc_info()[2])
        raise
    except AuthError:
        sys.exit("ERROR: Invalid access token; try re-generating an access token from the app console on the web.")
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return None

    return dbx


async def list_files_in_remote_folder(dbx: dropbox.Dropbox) -> None:
    """List all files in a remote Dropbox folder asynchronously.

    Args:
        dbx: An authenticated Dropbox client.

    Raises:
        Exception: If listing files fails.
    """
    logger.info("Listing files in remote folder")
    try:
        folder_path = aiosettings.default_dropbox_folder
        # Run potentially blocking operation in thread pool
        files = await asyncio.to_thread(
            lambda: dbx.files_list_folder(folder_path, recursive=True).entries
        )
        rich.print("------------Listing Files in Folder------------ ")

        for file in files:
            rich.print(file.name)
        logger.info("Successfully listed files")
        # await logger.complete()

    except Exception as ex:
        logger.error(f"Error Class: {ex.__class__!s}")
        output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
        logger.warning(output)
        logger.error(f"exc_type: {sys.exc_info()[0]}")
        logger.error(f"exc_value: {sys.exc_info()[1]}")
        traceback.print_tb(sys.exc_info()[2])
        # await logger.complete()
        raise


async def download_img(dbx: dropbox.Dropbox, short_file_name: str) -> None:
    """Download an image file from Dropbox asynchronously.

    Args:
        dbx: An authenticated Dropbox client.
        short_file_name: The name of the file to download.

    Raises:
        Exception: If downloading or writing the file fails.
    """
    try:
        # Run potentially blocking download in thread pool
        metadata, res = await asyncio.to_thread(
            dbx.files_download,
            path=f"{aiosettings.default_dropbox_folder}/{short_file_name}"
        )

        async with aiofiles.open(f"{short_file_name}", "wb") as f:
            await f.write(res.content)

    except Exception as ex:
        logger.error(f"Error Class: {ex.__class__!s}")
        output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
        logger.warning(output)
        logger.error(f"exc_type: {sys.exc_info()[0]}")
        logger.error(f"exc_value: {sys.exc_info()[1]}")
        traceback.print_tb(sys.exc_info()[2])
        raise


@contextlib.contextmanager
def stopwatch(message: str) -> Generator[None, None, None]:
    """Context manager to print how long a block of code took.

    Args:
        message: Message to display with the elapsed time.

    Yields:
        Generator[None, None, None]: A context manager that times code execution.
    """
    t0 = time.time()
    try:
        yield
    finally:
        t1 = time.time()
        print(f"Total elapsed time for {message}: {t1 - t0:.3f}")


async def list_folder(dbx: dropbox.Dropbox, folder: str, subfolder: str) -> dict[str, FileMetadata | FolderMetadata]:
    """List a folder asynchronously.

    Args:
        dbx: An authenticated Dropbox client.
        folder: The root folder to list.
        subfolder: The subfolder within the root folder.

    Returns:
        Dict[str, Union[FileMetadata, FolderMetadata]]: A dictionary mapping filenames to their metadata.

    Raises:
        dropbox.exceptions.ApiError: If folder listing fails.
    """
    logger.info(f"Listing folder: /{folder}/{subfolder}")
    path = f'/{folder}/{subfolder.replace(os.path.sep, "/")}'
    while "//" in path:
        path = path.replace("//", "/")
    path = path.rstrip("/")
    try:
        with stopwatch("list_folder"):
            res = await asyncio.to_thread(dbx.files_list_folder, path)
        logger.info("Successfully listed folder contents")
        # await logger.complete()
        return {entry.name: entry for entry in res.entries}
    except dropbox.exceptions.ApiError as err:
        logger.error(f"Folder listing failed for {path}: {err}")
        # await logger.complete()
        return {}


async def download(
    dbx: dropbox.Dropbox,
    folder: str,
    subfolder: str,
    name: str
) -> bytes | None:
    """Download a file asynchronously.

    Args:
        dbx: An authenticated Dropbox client.
        folder: The root folder containing the file.
        subfolder: The subfolder within the root folder.
        name: The name of the file to download.

    Returns:
        Optional[bytes]: The file contents as bytes, or None if download fails.

    Raises:
        dropbox.exceptions.HttpError: If HTTP request fails.
    """
    logger.info(f"Downloading file: {name} from /{folder}/{subfolder}")
    path = f'/{folder}/{subfolder.replace(os.path.sep, "/")}/{name}'
    while "//" in path:
        path = path.replace("//", "/")
    with stopwatch("download"):
        try:
            md, res = await asyncio.to_thread(dbx.files_download, path)
            data = res.content
            logger.info(f"Successfully downloaded {len(data)} bytes")
            # await logger.complete()
            return data
        except dropbox.exceptions.HttpError as err:
            logger.error(f"HTTP error during download: {err}")
            # await logger.complete()
            return None


async def upload(
    dbx: dropbox.Dropbox,
    fullname: str,
    folder: str,
    subfolder: str,
    name: str,
    overwrite: bool = False
) -> FileMetadata | None:
    """Upload a file asynchronously.

    Args:
        dbx: An authenticated Dropbox client.
        fullname: The full local path to the file.
        folder: The root folder to upload to.
        subfolder: The subfolder within the root folder.
        name: The name to give the file in Dropbox.
        overwrite: Whether to overwrite existing files. Defaults to False.

    Returns:
        Optional[FileMetadata]: The uploaded file's metadata, or None if upload fails.

    Raises:
        OSError: If file operations fail.
        dropbox.exceptions.ApiError: If upload fails.
    """
    path = f'/{folder}/{subfolder.replace(os.path.sep, "/")}/{name}'
    while "//" in path:
        path = path.replace("//", "/")
    mode = WriteMode.overwrite if overwrite else WriteMode.add
    mtime = await asyncio.to_thread(os.path.getmtime, fullname)

    async with aiofiles.open(fullname, "rb") as f:
        data = await f.read()

    with stopwatch("upload %d bytes" % len(data)):
        try:
            res = await asyncio.to_thread(
                dbx.files_upload,
                data,
                path,
                mode,
                client_modified=datetime.datetime(*time.gmtime(mtime)[:6]),
                mute=True,
            )
            return res
        except dropbox.exceptions.ApiError as err:
            logger.error(f"API error: {err}")
            return None


async def iter_dir_and_upload(dbx: dropbox.Dropbox, remote_folder: str, local_folder: str) -> None:
    """Iterate through a local directory and upload files to Dropbox asynchronously.

    Args:
        dbx: An authenticated Dropbox client.
        remote_folder: The remote folder path in Dropbox.
        local_folder: The local folder path to upload from.

    Raises:
        Exception: If directory iteration or upload fails.
    """
    try:
        # Run os.walk in thread pool since it's blocking
        walk_results = await asyncio.to_thread(lambda: list(os.walk(local_folder)))

        for root, _dirs, files in walk_results:
            for filename in files:
                local_path = os.path.join(root, filename)
                relative_path = os.path.relpath(local_path, local_folder)
                dropbox_path = os.path.join(remote_folder, relative_path)

                async with aiofiles.open(local_path, 'rb') as f:
                    file_data = await f.read()
                    await asyncio.to_thread(
                        dbx.files_upload,
                        file_data,
                        dropbox_path,
                        mode=WriteMode('overwrite')
                    )

    except Exception as ex:
        logger.error(f"Error Class: {ex.__class__!s}")
        output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
        logger.warning(output)
        logger.error(f"exc_type: {sys.exc_info()[0]}")
        logger.error(f"exc_value: {sys.exc_info()[1]}")
        traceback.print_tb(sys.exc_info()[2])
        raise


async def co_upload_to_dropbox(
    dbx: dropbox.Dropbox,
    path_to_local_file: str,
    path_to_remote_dir: str | None = None
) -> None:
    """Upload a file to Dropbox asynchronously.

    Args:
        dbx: An authenticated Dropbox client.
        path_to_local_file: The local file path to upload.
        path_to_remote_dir: Optional remote directory path. If not provided, uses default.

    Raises:
        Exception: If file upload fails.
    """
    try:
        if path_to_remote_dir is None:
            path_to_remote_dir = aiosettings.default_dropbox_folder

        file_name = os.path.basename(path_to_local_file)
        dropbox_path = f"{path_to_remote_dir}/{file_name}"

        async with aiofiles.open(path_to_local_file, mode='rb') as f:
            contents = await f.read()
            dbx.files_upload(contents, dropbox_path, mode=WriteMode('overwrite'))

    except Exception as ex:
        logger.error(f"Error Class: {ex.__class__!s}")
        output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
        logger.warning(output)
        logger.error(f"exc_type: {sys.exc_info()[0]}")
        logger.error(f"exc_value: {sys.exc_info()[1]}")
        traceback.print_tb(sys.exc_info()[2])
        raise


def select_revision(
    dbx: dropbox.Dropbox,
    filename: str | None = None,
    path_to_remote_file_or_dir: str | None = None
) -> str:
    """Select a revision of a file in Dropbox.

    Args:
        dbx: An authenticated Dropbox client.
        filename: Optional filename to select revision for.
        path_to_remote_file_or_dir: Optional remote path. If not provided, uses default.

    Returns:
        str: The selected revision path.

    Raises:
        Exception: If revision selection fails.
    """
    try:
        if path_to_remote_file_or_dir is None:
            path_to_remote_file_or_dir = aiosettings.default_dropbox_folder

        if filename is None:
            return path_to_remote_file_or_dir

        return f"{path_to_remote_file_or_dir}/{filename}"

    except Exception as ex:
        logger.error(f"Error Class: {ex.__class__!s}")
        output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
        logger.warning(output)
        logger.error(f"exc_type: {sys.exc_info()[0]}")
        logger.error(f"exc_value: {sys.exc_info()[1]}")
        traceback.print_tb(sys.exc_info()[2])
        raise


def cli_oauth() -> None:
    """Walk through a basic oauth flow using the existing long-lived token type.
    Populate your app key and app secret in order to run this locally.

    Raises:
        SystemExit: If OAuth flow fails.
    """
    # app_key_secret: SecretStr = cast(SecretStr, aiosettings.dropbox_cerebro_app_key)
    # app_secret_secret: SecretStr = cast(SecretStr, aiosettings.dropbox_cerebro_app_secret)

    auth_flow = DropboxOAuth2FlowNoRedirect(
        aiosettings.dropbox_cerebro_app_key.get_secret_value(),
        aiosettings.dropbox_cerebro_app_secret.get_secret_value()
    )

    authorize_url = auth_flow.start()
    print(f"1. Go to: {authorize_url}")
    print('2. Click "Allow" (you might have to log in first).')
    print("3. Copy the authorization code.")
    auth_code = input("Enter the authorization code here: ").strip()

    try:
        oauth_result = auth_flow.finish(auth_code)
        with dropbox.Dropbox(oauth2_access_token=oauth_result.access_token) as dbx:
            dbx.users_get_current_account()
            rich.print("Successfully set up client!")
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


class AsyncDropBox:
    """Asynchronous Dropbox client implementation."""

    def __init__(self) -> None:
        """Initialize AsyncDropBox with credentials from settings."""
        # DISABLED: This gets around pyright/pylance but exposes the secret values to the global namespace.
        # token_secret: SecretStr = cast(SecretStr, aiosettings.dropbox_cerebro_token)
        # app_key_secret: SecretStr = cast(SecretStr, aiosettings.dropbox_cerebro_app_key)
        # app_secret_secret: SecretStr = cast(SecretStr, aiosettings.dropbox_cerebro_app_secret)
        # refresh_token_secret: SecretStr = cast(SecretStr, aiosettings.dropbox_cerebro_token)

        # self.dropbox_access_token: str = token_secret.get_secret_value()
        # self.app_key: str = app_key_secret.get_secret_value()
        # self.app_secret: str = app_secret_secret.get_secret_value()
        # self.dropbox_refresh_token: str = refresh_token_secret.get_secret_value()

        # token_secret: SecretStr = cast(SecretStr, aiosettings.dropbox_cerebro_token)
        # app_key_secret: SecretStr = cast(SecretStr, aiosettings.dropbox_cerebro_app_key)
        # app_secret_secret: SecretStr = cast(SecretStr, aiosettings.dropbox_cerebro_app_secret)
        # refresh_token_secret: SecretStr = cast(SecretStr, aiosettings.dropbox_cerebro_token)

        self.dropbox_access_token: str = aiosettings.dropbox_cerebro_token.get_secret_value()  # pylint: disable=no-member
        self.app_key: str = aiosettings.dropbox_cerebro_app_key.get_secret_value()  # pylint: disable=no-member
        self.app_secret: str = aiosettings.dropbox_cerebro_app_secret.get_secret_value()  # pylint: disable=no-member
        self.dropbox_refresh_token: str = aiosettings.dropbox_cerebro_token.get_secret_value()  # pylint: disable=no-member

        self.client = self.auth()

    async def __aenter__(self) -> AsyncDropBox:
        """Async context manager entry.

        Returns:
            AsyncDropBox: The instance itself.
        """
        return self

    async def __aexit__(self, *args: Any) -> None:
        """Async context manager exit.

        Args:
            *args: Variable length argument list.
        """
        self.client.close()

    def auth(self) -> dropbox.Dropbox:
        """Authenticate with Dropbox.

        Authentication is done via OAuth2. You can generate token for yourself in the App Console.
        See https://blogs.dropbox.com/developers/2014/05/generate-an-access-token-for-your-own-account/
        Authentication step initially is done with ACCESS_TOKEN, as it is short lived it will expire soon.
        Therefore better to have Refresh token.

        Returns:
            dropbox.Dropbox: An authenticated Dropbox client.
        """
        return (
            dropbox.Dropbox(
                oauth2_access_token=self.dropbox_access_token,
                oauth2_refresh_token=self.dropbox_refresh_token,
                app_key=self.app_key,
                app_secret=self.app_secret,
            )
            if self.dropbox_refresh_token
            else dropbox.Dropbox(self.dropbox_access_token)
        )

    async def account_info(self) -> dict[str, Any]:
        """Get account information of the current user.

        Returns:
            Dict[str, Any]: A dictionary containing account information.
        """
        temp = defaultdict(dict)
        result = self.client.users_get_current_account()

        temp["abbreviated_name"] = result.name.abbreviated_name
        temp["display_name"] = result.name.display_name
        temp["familiar_name"] = result.name.familiar_name
        temp["given_name"] = result.name.given_name
        temp["surname"] = result.name.surname

        temp["account_id"] = result.account_id
        temp["country"] = result.country
        temp["disabled"] = result.disabled
        temp["email"] = result.email
        temp["email_verified"] = result.email_verified
        temp["is_paired"] = result.is_paired
        temp["locale"] = result.locale

        temp["profile_photo_url"] = result.profile_photo_url
        temp["referral_link"] = result.referral_link
        temp["team"] = result.team
        temp["team_member_id"] = result.team_member_id
        return temp

    async def list_files(
        self,
        path: str,
        recursive: bool = False,
        include_media_info: bool = False,
        include_deleted: bool = False,
        include_has_explicit_shared_members: bool = False,
        include_mounted_folders: bool = True,
        limit: int | None = None,
        shared_link: str | None = None,
        include_property_groups: list[str] | None = None,
        include_non_downloadable_files: bool = True,
    ) -> dict[str, str]:
        """List files in a Dropbox folder.

        Args:
            path (str): Path to list files from.
            recursive (bool, optional): Whether to list recursively. Defaults to False.
            include_media_info (bool, optional): Include media info. Defaults to False.
            include_deleted (bool, optional): Include deleted files. Defaults to False.
            include_has_explicit_shared_members (bool, optional): Include explicit shared members. Defaults to False.
            include_mounted_folders (bool, optional): Include mounted folders. Defaults to True.
            limit (Optional[int], optional): Maximum number of results. Defaults to None.
            shared_link (Optional[str], optional): Shared link to use. Defaults to None.
            include_property_groups (Optional[List[str]], optional): Property groups to include. Defaults to None.
            include_non_downloadable_files (bool, optional): Include non-downloadable files. Defaults to True.

        Returns:
            Dict[str, str]: A dictionary mapping file paths to their shared links.
        """
        response = self.client.files_list_folder(
            path=path,
            recursive=recursive,
            include_media_info=include_media_info,
            include_deleted=include_deleted,
            include_has_explicit_shared_members=include_has_explicit_shared_members,
            include_mounted_folders=include_mounted_folders,
            limit=limit,
            shared_link=shared_link,
            include_property_groups=include_property_groups,
            include_non_downloadable_files=include_non_downloadable_files,
        )
        temp: dict[str, str] = {}

        try:
            for file in response.entries:
                link = self.client.sharing_create_shared_link(file.path_display)
                path = link.url.replace("0", "1")
                temp[file.path_display] = path
            return temp
        except Exception as er:
            print(er)
            return temp

    async def upload_file(self, file_from: str, file_to: str) -> None:
        """Upload a file to Dropbox.

        Args:
            file_from (str): Local file path.
            file_to (str): Remote file path.
        """
        with open(file_from, "rb") as f:
            self.client.files_upload(f.read(), file_to, mode=WriteMode("overwrite"))

    async def save_file_localy(self, file_path: str, filename: str) -> None:
        """Save a file locally from Dropbox.

        Args:
            file_path (str): Remote file path.
            filename (str): Remote filename.
        """
        metadata, res = self.client.files_download(file_path + filename)
        with open(metadata.name, "wb") as f:
            f.write(res.content)

    async def get_link_of_file(self, file_path: str, filename: str, dowload: bool = False) -> dict[str, str]:
        """Get a shared link for a file.

        Args:
            file_path (str): Remote file path.
            filename (str): Remote filename.
            dowload (bool, optional): Whether to get a download link. Defaults to False.

        Returns:
            Dict[str, str]: A dictionary containing the file URL.
        """
        path = self.client.sharing_create_shared_link(file_path + filename)
        if dowload:
            path = path.url.replace("0", "1")
        return {"file": path.url}


async def co_upload_to_dropbox2(path_to_local_file: str, path_to_remote_dir: str = "/") -> None:
    """Upload a file to Dropbox using the async client.

    Args:
        path_to_local_file (str): Path to the local file to upload.
        path_to_remote_dir (str, optional): Remote directory to upload to. Defaults to "/".

    Raises:
        AssertionError: If the local file doesn't exist or is not a file.
        ApiError: If there's an error during upload, including insufficient space.
    """
    localfile_pathobj = pathlib.Path(f"{path_to_local_file}").absolute()
    try:
        assert localfile_pathobj.exists()
        assert localfile_pathobj.is_file()
    except Exception as ex:
        print(ex)
        exc_type, exc_value, exc_traceback = sys.exc_info()
        logger.error(f"Error Class: {ex.__class__!s}")
        output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
        logger.warning(output)
        logger.error(f"exc_type: {exc_type}")
        logger.error(f"exc_value: {exc_value}")
        traceback.print_tb(exc_traceback)
        raise

    _localfile = f"{localfile_pathobj}"
    _backuppath = f"{path_to_remote_dir}/{path_to_local_file}"

    try:
        async with AsyncDropBox() as drop:
            await drop.upload_file(file_from=_localfile, file_to=_backuppath)
    except ApiError as err:
        if err.error.is_path() and err.error.get_path().reason.is_insufficient_space():
            logger.error("ERROR: Cannot back up; insufficient space.")
        elif err.user_message_text:
            rich.print(err.user_message_text)

        else:
            rich.print(err)

</document_content>
</document>
<document index="34">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/duration_str.py</source>
<document_content>
"""democracy_exe.utils.duration_str"""
# Copyright 2022 Cisco Systems, Inc. and/or its affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Adapted from durationpy
# https://github.com/icholy/durationpy/
from __future__ import annotations

import re

from datetime import timedelta
from typing import Optional


_nanosecond_size = 1
_microsecond_size = 1000 * _nanosecond_size
_millisecond_size = 1000 * _microsecond_size
_second_size = 1000 * _millisecond_size
_minute_size = 60 * _second_size
_hour_size = 60 * _minute_size
_day_size = 24 * _hour_size
_week_size = 7 * _day_size
_month_size = 30 * _day_size
_year_size = 365 * _day_size

units = {
    "ns": _nanosecond_size,
    "us": _microsecond_size,
    "s": _microsecond_size,
    "s": _microsecond_size,
    "ms": _millisecond_size,
    "s": _second_size,
    "m": _minute_size,
    "h": _hour_size,
    "d": _day_size,
    "w": _week_size,
    "mm": _month_size,
    "y": _year_size,
}


def microseconds_from_duration_str(duration: str) -> float:
    """Parse a duration string into a microseconds float value."""
    if duration in {"0", "+0", "-0"}:
        return 0

    pattern = re.compile(r"([\d\.]+)([a-z]+)")
    matches = pattern.findall(duration)
    if not len(matches):
        raise ValueError(f"Invalid duration '{duration}'")

    total: float = 0
    sign = -1 if duration[0] == "-" else 1

    for value, unit in matches:
        if unit not in units:
            raise ValueError(f"Unknown unit '{unit}' in duration '{duration}'")
        try:
            total += float(value) * units[unit]
        except Exception as e:
            raise ValueError(f"Invalid value '{value}' in duration '{duration}'") from e

    return sign * (total / _microsecond_size)


def timedelta_from_duration_str(duration: str) -> timedelta:
    """
    Parse a Golang duration string into a Python timedelta value.

    Raises a ValueError if the string cannot be parsed.

    A duration string is a possibly signed sequence of decimal numbers,
    each with optional fraction and a unit suffix, such as "300ms", "-1.5h" or "2h45m".

    Valid units are :
        ns - nanoseconds
        us - microseconds
        ms - millisecond
        s  - second
        m  - minute
        h  - hour
        w  - week
        mm - month
        y  - year
    """
    return timedelta(microseconds=microseconds_from_duration_str(duration))


def timedelta_to_duration_str(delta: timedelta, extended: bool = False) -> str:
    """
    Return a Golang duration string representation of a timedelta value.

    A duration string is a possibly signed sequence of decimal numbers,
    each with optional fraction and a unit suffix, such as "300ms", "-1.5h" or "2h45m".

    Components of the returned string are:
        ns - nanoseconds
        us - microseconds
        ms - millisecond
        s  - second
        m  - minute
        h  - hour
        w  - week
        mm - month
        y  - year
    """
    total_seconds = delta.total_seconds()
    sign = "-" if total_seconds < 0 else ""
    nanoseconds = abs(total_seconds * _second_size)

    if total_seconds < 1:
        result_str = _to_str_small(nanoseconds, extended)
    else:
        result_str = _to_str_large(nanoseconds, extended)

    return f"{sign}{result_str}"


def _to_str_small(nanoseconds: float | None, extended: bool = False) -> str:
    result_str = ""

    if not nanoseconds:
        return "0"

    if milliseconds := int(nanoseconds / _millisecond_size):
        nanoseconds -= _millisecond_size * milliseconds
        result_str += f"{milliseconds:g}ms"

    if microseconds := int(nanoseconds / _microsecond_size):
        nanoseconds -= _microsecond_size * microseconds
        result_str += f"{microseconds:g}us"

    if nanoseconds:
        result_str += f"{nanoseconds:g}ns"

    return result_str


def _to_str_large(nanoseconds: float, extended: bool = False) -> str:
    result_str = ""

    if extended:
        if years := int(nanoseconds / _year_size):
            nanoseconds -= _year_size * years
            result_str += f"{years:g}y"

        if months := int(nanoseconds / _month_size):
            nanoseconds -= _month_size * months
            result_str += f"{months:g}mm"

        if days := int(nanoseconds / _day_size):
            nanoseconds -= _day_size * days
            result_str += f"{days:g}d"

    if hours := int(nanoseconds / _hour_size):
        nanoseconds -= _hour_size * hours
        result_str += f"{hours:g}h"

    if minutes := int(nanoseconds / _minute_size):
        nanoseconds -= _minute_size * minutes
        result_str += f"{minutes:g}m"

    if seconds := nanoseconds / float(_second_size):
        nanoseconds -= _second_size * seconds
        result_str += f"{seconds:g}s"

    return result_str

</document_content>
</document>
<document index="35">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/dynamic.py</source>
<document_content>
"""Utility methods relating to dynamically generating objects."""

from __future__ import annotations

import importlib


def dynamic_instantiation(module_name: str, class_name: str) -> object:
    """Dynamically instantiates a class from a module."""
    module = importlib.import_module(module_name)
    class_ = getattr(module, class_name)
    return class_()


def instantiate_target(target: str):
    """
    Instantiates a target object from a string.

    The target string should be in the same format as expected from Hydra targets. I.e. 'module_name.class_name'.

    Args:
    ----
        target: A string representing a target object.

    Example::

        from mltemplate.utils import instantiate_target

        target = "mltemplate.data.mnist.MNISTDataModule"
        mnist = instantiate_target(target)

        print(type(mnist))  # <class 'mltemplate.data.mnist.MNISTDataModule'>

    """
    module_name, class_name = target.rsplit(".", 1)
    return dynamic_instantiation(module_name, class_name)

</document_content>
</document>
<document index="36">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/env.py</source>
<document_content>
from __future__ import annotations

import os


def environ_get(key, default=None):
    retval = os.environ.get(key, default=default)

    if key not in os.environ:
        print(f"environ_get: Env Var not defined! Using default! Attempted={key}, default={default}")

    return retval


def environ_append(key, value, separator=" ", force=False):
    old_value = os.environ.get(key)
    if old_value is not None:
        value = old_value + separator + value
    os.environ[key] = value


def environ_prepend(key, value, separator=" ", force=False):
    old_value = os.environ.get(key)
    if old_value is not None:
        value = value + separator + old_value
    os.environ[key] = value


def environ_remove(key, value, separator=":", force=False):
    old_value = os.environ.get(key)
    if old_value is not None:
        old_value_split = old_value.split(separator)
        value_split = [x for x in old_value_split if x != value]
        value = separator.join(value_split)
    os.environ[key] = value


def environ_set(key, value):
    os.environ[key] = value


def path_append(value):
    if os.path.exists(value):
        environ_append("PATH", value, ":")


def path_prepend(value, force=False):
    if os.path.exists(value):
        environ_prepend("PATH", value, ":", force)

</document_content>
</document>
<document index="37">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/events.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
# pylint: disable=possibly-used-before-assignment,used-before-assignment
"""democracy_exe.utils.events"""

from __future__ import annotations

from typing import TYPE_CHECKING

import discord
import structlog


logger = structlog.get_logger(__name__)

from democracy_exe.utils.file_functions import glob_file_by_extension


if TYPE_CHECKING:
    pass


def css_syntax_highlight(text: str):
    return f"```css\n{text}\n```"


def ig_type(typename: str):
    """
    Take instagram metadata payload typenames and convert them into readable IG post type values, eg Albums, Reels, or Image Post.

    Args:
    ----
        typename (str): _description_

    Returns:
    -------
        _type_: _description_

    """
    if typename == "GraphSidecar":
        return "Album"
    elif typename == "GraphImage":
        return "Image Post"
    elif typename == "GraphVideo":
        return "Reel"


def aio_create_thumbnail_attachment(tmpdirname: str, recursive: bool = False):
    #######################################################
    # add event to system channel
    #######################################################
    jpg_file_list = glob_file_by_extension(f"{tmpdirname}", extension="*.jpg", recursive=recursive)

    jpg_file = f"{jpg_file_list[0]}"
    logger.debug(f"jpg_file = {jpg_file}")
    print(f"jpg_file = {jpg_file}")

    jpg_attachment = discord.File(jpg_file)
    attachment_url = f"attachment://{jpg_attachment}"
    return attachment_url, jpg_attachment


# async def aio_download_event(
#     ctx: Context,
#     tmpdirname: str,
#     cmd_metadata: cmd_factory.CmdSerializer,
#     is_dropbox_upload: bool = False,
#     recursive: bool = False,
# ):
#     #######################################################
#     # add event to system channel
#     #######################################################
#     json_file_list = glob_file_by_extension(f"{tmpdirname}", extension="*.json", recursive=recursive)

#     json_file = f"{json_file_list[0]}"
#     logger.debug(f"json_file = {json_file}")
#     print(f"json_file = {json_file}")

#     try:
#         json_data = await run_aio_json_loads(json_file)
#     except Exception as ex:
#         await ctx.send(embed=discord.Embed(description="Could not open json metadata file"))
#         print(ex)
#         exc_type, exc_value, exc_traceback = sys.exc_info()
#         logger.error(f"Error Class: {str(ex.__class__)}")
#         output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
#         await ctx.send(embed=discord.Embed(description=f"{output}"))
#         logger.warning(output)
#         logger.error(f"exc_type: {exc_type}")
#         logger.error(f"exc_value: {exc_value}")
#         traceback.print_tb(exc_traceback)

#     # current_message: Message
#     current_message: Message = ctx.message  # pyright: ignore[reportAttributeAccessIssue]
#     current_channel: discord.TextChannel
#     current_channel = ctx.channel
#     current_guild: discord.Guild
#     current_guild = current_channel.guild

#     if "youtu.be" in f"{cmd_metadata.uri}" or "youtube" in f"{cmd_metadata.uri}":
#         try:
#             # 1. Get guild
#             # ctx.guild.id
#             # = await guild_factory.Guild(id=guild.id)

#             ##########################################
#             full_description = json_data["description"]
#             description = f"{full_description[:75]}.." if len(full_description) > 75 else full_description
#             embed_event = discord.Embed(
#                 title=f"Downloaded: '{json_data['fulltitle']}' in channel #{current_channel.name}",
#                 url=f"{current_message.jump_url}",  # pyright: ignore[reportAttributeAccessIssue]
#                 description=css_syntax_highlight(description),
#                 color=discord.Color.blue(),
#             )
#             # set author
#             embed_event.set_author(
#                 name=json_data["channel"],
#                 url=json_data["uploader_url"],
#                 icon_url=json_data["thumbnail"],
#             )

#             # set thumbnail
#             embed_event.set_thumbnail(url=json_data["thumbnail"])
#             embed_event.set_image(url=json_data["thumbnail"])

#             embed_event.add_field(name="Url", value=f"{cmd_metadata.uri}", inline=False)
#             embed_event.add_field(
#                 name="View Count",
#                 value=css_syntax_highlight(json_data["view_count"]),
#                 inline=True,
#             )
#             embed_event.add_field(
#                 name="Duration in seconds",
#                 value=css_syntax_highlight(json_data["duration"]),
#                 inline=True,
#             )
#             embed_event.set_footer(text=f'Is dropbox upload? "{is_dropbox_upload}"')
#             ##########################################
#             # rich.inspect(current_guild, methods=True)
#             if current_guild.system_channel is not None:
#                 # to_send = 'Welcome {0.mention} to {1.name}!'.format(member, current_guild)
#                 await current_guild.system_channel.send(embed=embed_event)
#         except Exception as ex:
#             await ctx.send(embed=discord.Embed(description="Could not send download event to general"))
#             print(ex)
#             exc_type, exc_value, exc_traceback = sys.exc_info()
#             logger.error(f"Error Class: {str(ex.__class__)}")
#             output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
#             await ctx.send(embed=discord.Embed(description=f"{output}"))
#             logger.warning(output)
#             logger.error(f"exc_type: {exc_type}")
#             logger.error(f"exc_value: {exc_value}")
#             traceback.print_tb(exc_traceback)

#     elif "instagram" in f"{cmd_metadata.uri}":
#         # 1. first grab the first media file we can find since IG can techincally download multiple media types
#         file_to_upload = get_all_media_files_to_upload(f"{tmpdirname}")

#         media_file_api = pathlib.Path(f"{file_to_upload[0]}")
#         # eg 280546359_2873025409665148_6148590927180637067_n.mp4.json
#         json_metadata_fname = f"{media_file_api.name}.json"

#         # For instagram we want to use the metadata json file instead of the info.json
#         json_file_list = glob_file_by_extension(f"{tmpdirname}", extension=json_metadata_fname, recursive=recursive)

#         json_file = f"{json_file_list[0]}"
#         logger.debug(f"json_file = {json_file}")
#         print(f"json_file = {json_file}")

#         json_data = await run_aio_json_loads(json_file)

#         if "display_url" in json_data:
#             json_data_description = json_data["description"]
#             json_data_title = json_data["description"]
#             json_data_uploader_id = json_data["username"]
#             json_data_uploader_url = f"https://instagram.com/{json_data_uploader_id}"

#             dest_override = f"{tmpdirname}/{json_data['filename']}.jpg"
#             thumbnail, _ = await downloader.download_and_save(json_data["display_url"], dest_override)
#             rich.print(f"WE HAVE THE DISPLAY URL thumbnail => {thumbnail}")
#             rich.print(f"WE HAVE THE DISPLAY URL dest_override => {dest_override}")

#             # since we are not using recursive, it should just find the first jpg in directory which will be at tmpdirname/image.jpg
#             attachment_url, jpg_attachment = aio_create_thumbnail_attachment(f"{tmpdirname}")

#             rich.print(f"WE HAVE THE DISPLAY URL attachment_url => {attachment_url}")
#             rich.print(f"WE HAVE THE DISPLAY URL jpg_attachment => {jpg_attachment}")

#             json_data_icon_url = attachment_url
#             json_data_thumbnail = attachment_url
#             json_data_like_count = json_data["likes"] if "likes" in json_data else "n/a"
#         else:
#             await ctx.send(embed=discord.Embed(description="Key 'display_url' is not in dictonary 'json_data'"))

#         try:
#             ##########################################
#             full_description = json_data_description
#             description = f"{full_description[:75]}.." if len(full_description) > 75 else full_description
#             embed_event = discord.Embed(
#                 title=f"Downloaded: '{description}' in channel #{current_channel.name}",
#                 url=f"{current_message.jump_url}",  # pyright: ignore[reportAttributeAccessIssue]
#                 description=css_syntax_highlight(description),
#                 color=discord.Color.blue(),
#             )
#             # set author
#             embed_event.set_author(
#                 name=json_data_uploader_id,
#                 url=json_data_uploader_url,
#                 icon_url=attachment_url,
#             )

#             # set thumbnail
#             embed_event.set_thumbnail(url=attachment_url)
#             embed_event.set_image(url=attachment_url)

#             embed_event.add_field(name="Url", value=f"{cmd_metadata.uri}", inline=False)
#             embed_event.add_field(
#                 name="Likes",
#                 value=css_syntax_highlight(json_data_like_count),
#                 inline=True,
#             )
#             # embed_event.add_field(
#             #     name="Type",
#             #     value=css_syntax_highlight(ig_type(json_data["typename"])),
#             #     inline=True,
#             # )
#             embed_event.set_footer(text=f'Is dropbox upload? "{is_dropbox_upload}"')
#             ##########################################
#             if current_guild.system_channel is not None:
#                 await current_guild.system_channel.send(file=jpg_attachment, embed=embed_event)
#         except Exception as ex:
#             await ctx.send(embed=discord.Embed(description="Could not send download event to general"))
#             print(ex)
#             exc_type, exc_value, exc_traceback = sys.exc_info()
#             logger.error(f"Error Class: {str(ex.__class__)}")
#             output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
#             await ctx.send(embed=discord.Embed(description=f"{output}"))
#             logger.warning(output)
#             logger.error(f"exc_type: {exc_type}")
#             logger.error(f"exc_value: {exc_value}")
#             traceback.print_tb(exc_traceback)

#     elif "twitter" in f"{cmd_metadata.uri}":
#         attachment_url = None
#         jpg_attachment = None

#         rich.print(json_data)

#         # This means we used yt-dlp
#         if "description" in json_data:
#             json_data_description = json_data["description"]
#             json_data_title = json_data["title"]
#             json_data_uploader_id = json_data["uploader_id"]
#             json_data_uploader_url = json_data["uploader_url"]
#             json_data_icon_url = json_data["thumbnail"]
#             json_data_thumbnail = json_data["thumbnail"]
#             json_data_like_count = json_data["like_count"]
#             json_data_repost_count = json_data["repost_count"]
#             json_data_comment_count = json_data["comment_count"]
#         # this means we are using gallery-dl to download a tweet
#         elif "content" in json_data:
#             json_data_description = json_data["content"]
#             json_data_title = json_data["content"]
#             json_data_uploader_id = json_data["author"]["name"]
#             json_data_uploader_url = f"https://twitter.com/{json_data_uploader_id}"
#             json_data_icon_url = json_data["author"]["profile_image"]
#             attachment_url, jpg_attachment = aio_create_thumbnail_attachment(f"{tmpdirname}", recursive=True)
#             json_data_thumbnail = attachment_url
#             json_data_like_count = json_data["favorite_count"]
#             json_data_repost_count = json_data["retweet_count"]
#             json_data_comment_count = json_data["reply_count"]

#         try:
#             # 1. Get guild
#             # ctx.guild.id
#             # = await guild_factory.Guild(id=guild.id)

#             ##########################################
#             full_description = json_data_description
#             description = f"{full_description[:75]}.." if len(full_description) > 75 else full_description
#             embed_event = discord.Embed(
#                 title=f"Downloaded: '{json_data_title}' in channel #{current_channel.name}",
#                 url=f"{current_message.jump_url}",  # pyright: ignore[reportAttributeAccessIssue]
#                 description=css_syntax_highlight(description),
#                 color=discord.Color.blue(),
#             )
#             # set author
#             embed_event.set_author(
#                 name=json_data_uploader_id,
#                 url=json_data_uploader_url,
#                 icon_url=json_data_icon_url,
#             )

#             # set thumbnail
#             embed_event.set_thumbnail(url=json_data_thumbnail)
#             embed_event.set_image(url=json_data_thumbnail)

#             embed_event.add_field(name="Url", value=f"{cmd_metadata.uri}", inline=False)
#             embed_event.add_field(
#                 name="Like Count",
#                 value=css_syntax_highlight(json_data_like_count),
#                 inline=True,
#             )
#             embed_event.add_field(
#                 name="Retweets",
#                 value=css_syntax_highlight(json_data_repost_count),
#                 inline=True,
#             )
#             embed_event.add_field(
#                 name="Comments",
#                 value=css_syntax_highlight(json_data_comment_count),
#                 inline=True,
#             )
#             embed_event.set_footer(text=f'Is dropbox upload? "{is_dropbox_upload}"')
#             if current_guild.system_channel is not None:
#                 if json_data_thumbnail is attachment_url:
#                     await current_guild.system_channel.send(file=jpg_attachment, embed=embed_event)
#                 else:
#                     await current_guild.system_channel.send(embed=embed_event)
#         except Exception as ex:
#             await ctx.send(embed=discord.Embed(description="Could not send download event to general"))
#             print(ex)
#             exc_type, exc_value, exc_traceback = sys.exc_info()
#             logger.error(f"Error Class: {str(ex.__class__)}")
#             output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
#             await ctx.send(embed=discord.Embed(description=f"{output}"))
#             logger.warning(output)
#             logger.error(f"exc_type: {exc_type}")
#             logger.error(f"exc_value: {exc_value}")
#             traceback.print_tb(exc_traceback)
#     elif "reddit" in f"{cmd_metadata.uri}" or "redd.it" in f"{cmd_metadata.uri}":
#         # rich.print(json_data)

#         try:
#             # 1. Get guild
#             # ctx.guild.id
#             # = await guild_factory.Guild(id=guild.id)

#             ##########################################
#             full_description = json_data["title"]
#             if "nsfw" in json_data["thumbnail"]:
#                 thumbnail = json_data["preview"]["images"][0]["source"]["url"]
#             else:
#                 thumbnail = json_data["thumbnail"]

#             description = f"{full_description[:75]}.." if len(full_description) > 75 else full_description
#             embed_event = discord.Embed(
#                 title=f"Downloaded: '{json_data['title']}' in channel #{current_channel.name}",
#                 url=f"{current_message.jump_url}",  # pyright: ignore[reportAttributeAccessIssue]
#                 description=css_syntax_highlight(description),
#                 color=discord.Color.blue(),
#             )
#             # set author
#             embed_event.set_author(
#                 name=json_data["author"],
#                 url=f"https://www.reddit.com/user/{json_data['author']}/",
#                 icon_url=thumbnail,
#             )

#             # set thumbnail
#             embed_event.set_thumbnail(url=thumbnail)
#             embed_event.set_image(url=thumbnail)

#             embed_event.add_field(name="Url", value=f"{cmd_metadata.uri}", inline=False)
#             embed_event.add_field(
#                 name="Upvotes",
#                 value=css_syntax_highlight(json_data["score"]),
#                 inline=True,
#             )
#             embed_event.add_field(
#                 name="Comments",
#                 value=css_syntax_highlight(json_data["num_comments"]),
#                 inline=True,
#             )
#             embed_event.add_field(
#                 name="Subreddit",
#                 value=css_syntax_highlight(json_data["subreddit"]),
#                 inline=True,
#             )
#             embed_event.set_footer(text=f'Is dropbox upload? "{is_dropbox_upload}"')
#             ##########################################
#             # rich.inspect(current_guild, methods=True)
#             if current_guild.system_channel is not None:
#                 await current_guild.system_channel.send(embed=embed_event)
#         except Exception as ex:
#             await ctx.send(embed=discord.Embed(description="Could not send download event to general"))
#             print(ex)
#             exc_type, exc_value, exc_traceback = sys.exc_info()
#             logger.error(f"Error Class: {str(ex.__class__)}")
#             output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
#             await ctx.send(embed=discord.Embed(description=f"{output}"))
#             logger.warning(output)
#             logger.error(f"exc_type: {exc_type}")
#             logger.error(f"exc_value: {exc_value}")
#             traceback.print_tb(exc_traceback)

#     elif "tiktok" in f"{cmd_metadata.uri}":
#         # NOTE: Tiktok doesn't have a public thumbnail url, so we need to create an attachment and upload it
#         attachment_url, jpg_attachment = aio_create_thumbnail_attachment(f"{tmpdirname}")

#         try:
#             # 1. Get guild
#             # ctx.guild.id
#             # = await guild_factory.Guild(id=guild.id)

#             ##########################################
#             full_description = json_data["description"]
#             description = f"{full_description[:75]}.." if len(full_description) > 75 else full_description
#             embed_event = discord.Embed(
#                 title=f"Downloaded: '{json_data['title']}' in channel #{current_channel.name}",
#                 url=f"{current_message.jump_url}",  # pyright: ignore[reportAttributeAccessIssue]
#                 description=css_syntax_highlight(description),
#                 color=discord.Color.blue(),
#             )
#             # set author
#             embed_event.set_author(
#                 name=json_data["uploader"],
#                 url=f"https://tiktok.com/@{json_data['uploader']}",
#                 icon_url=attachment_url,
#             )

#             # set thumbnail
#             embed_event.set_thumbnail(url=attachment_url)
#             embed_event.set_image(url=attachment_url)

#             embed_event.add_field(name="Url", value=f"{cmd_metadata.uri}", inline=False)
#             embed_event.add_field(
#                 name="View Count",
#                 value=css_syntax_highlight(json_data["view_count"]),
#                 inline=True,
#             )
#             embed_event.add_field(
#                 name="Like Count",
#                 value=css_syntax_highlight(json_data["like_count"]),
#                 inline=True,
#             )
#             embed_event.set_footer(text=f'Is dropbox upload? "{is_dropbox_upload}"')
#             ##########################################
#             # rich.inspect(current_guild, methods=True)
#             if current_guild.system_channel is not None:
#                 # to_send = 'Welcome {0.mention} to {1.name}!'.format(member, current_guild)
#                 await current_guild.system_channel.send(file=jpg_attachment, embed=embed_event)
#         except Exception as ex:
#             await ctx.send(embed=discord.Embed(description="Could not send download event to general"))
#             print(ex)
#             exc_type, exc_value, exc_traceback = sys.exc_info()
#             logger.error(f"Error Class: {str(ex.__class__)}")
#             output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
#             await ctx.send(embed=discord.Embed(description=f"{output}"))
#             logger.warning(output)
#             logger.error(f"exc_type: {exc_type}")
#             logger.error(f"exc_value: {exc_value}")
#             traceback.print_tb(exc_traceback)

#     else:
#         if current_guild.system_channel is not None:
#             await current_guild.system_channel.send(
#                 embed=discord.Embed(
#                     description=f"```css\nSorry, **aio_download_event** isn't configured to deal with these urls yet: {cmd_metadata.uri}\n```"
#                 )
#             )

#         rich.print(json_data)

</document_content>
</document>
<document index="38">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/file_functions.py</source>
<document_content>
"""democracy_exe.file_functions"""
# pyright: reportGeneralTypeIssues=false
# pyright: reportOperatorIssue=false
# pyright: reportOptionalIterable=false

from __future__ import annotations

import glob
import json
import os
import os.path
import pathlib
import string
import sys

from typing import TYPE_CHECKING, List, Optional

import aiofiles
import pandas as pd
import rich
import structlog


logger = structlog.get_logger(__name__)
from rich.console import Console
from rich.table import Table

from democracy_exe.constants import (
    FIFTY_THOUSAND,
    FIVE_HUNDRED_THOUSAND,
    MAX_BYTES_UPLOAD_DISCORD,
    ONE_HUNDRED_THOUSAND,
    ONE_MILLION,
    TEN_THOUSAND,
    THIRTY_THOUSAND,
    TWENTY_THOUSAND,
)


if TYPE_CHECKING:
    from pandas import DataFrame


PY2 = sys.version_info[0] == 2
PY3 = sys.version_info[0] == 3

# https://gist.github.com/wassname/1393c4a57cfcbf03641dbc31886123b8
# python convert string to safe filename
VALID_FILENAME_CHARS = f"-_.() {string.ascii_letters}{string.digits}"
CHAR_LIMIT = 255

JSON_EXTENSIONS = [".json", ".JSON"]
VIDEO_EXTENSIONS = [".mp4", ".mov", ".MP4", ".MOV"]
AUDIO_EXTENSIONS = [".mp3", ".MP3"]
GIF_EXTENSIONS = [".gif", ".GIF"]
MKV_EXTENSIONS = [".mkv", ".MKV"]
M3U8_EXTENSIONS = [".m3u8", ".M3U8"]
WEBM_EXTENSIONS = [".webm", ".WEBM"]
IMAGE_EXTENSIONS = [".png", ".jpeg", ".jpg", ".gif", ".PNG", ".JPEG", ".JPG", ".GIF"]
TORCH_MODEL_EXTENSIONS = [".pth", ".PTH"]
PDF_EXTENSIONS = [".pdf", ".PDF"]
TXT_EXTENSIONS = [".txt", ".TXT"]


async def aio_read_jsonfile(jsonfile: str) -> dict:
    """
    Read a JSON file asynchronously.

    Args:
    ----
        jsonfile (str): Path to the JSON file.

    Returns:
    -------
        dict: Parsed JSON data.

    """
    print(f" [aio_read_jsonfile] jsonfile -> {jsonfile}")
    async with aiofiles.open(jsonfile, encoding="utf-8") as f:
        contents = await f.read()
    json_data = json.loads(contents)
    print(f" [aio_read_jsonfile] json_data -> {json_data}")
    return json_data


async def aio_json_loads(uri: str) -> dict:
    """
    Load JSON data from a file asynchronously.

    Args:
    ----
        uri (str): Path to the JSON file.

    Returns:
    -------
        dict: Parsed JSON data.

    """
    return json.loads(await (await aiofiles.open(uri)).read())


async def run_aio_json_loads(uri: str) -> dict:
    """
    Run the aio_json_loads function.

    Args:
    ----
        uri (str): Path to the JSON file.

    Returns:
    -------
        dict: Parsed JSON data.

    """
    return await aio_json_loads(uri=uri)


# SOURCE: https://stackoverflow.com/questions/168409/how-do-you-get-a-directory-listing-sorted-by-creation-date-in-python
def sort_dir_by_mtime(dirpath: str) -> list[pathlib.Path]:
    """
    Sort directory contents by modification time.

    Args:
    ----
        dirpath (str): Path to the directory.

    Returns:
    -------
        list[pathlib.Path]: List of sorted paths.

    """
    return sorted(pathlib.Path(dirpath).iterdir(), key=os.path.getmtime)


# SOURCE: https://stackoverflow.com/questions/168409/how-do-you-get-a-directory-listing-sorted-by-creation-date-in-python
def sort_dir_by_ctime(dirpath: str) -> list[pathlib.Path]:
    """
    Sort directory contents by creation time.

    Args:
    ----
        dirpath (str): Path to the directory.

    Returns:
    -------
        list[pathlib.Path]: List of sorted paths.

    """
    return sorted(pathlib.Path(dirpath).iterdir(), key=os.path.getctime)


def get_all_media_files_to_upload(tmpdirname: str) -> list[str]:
    """
    Get all media files to upload from a directory.

    Args:
    ----
        tmpdirname (str): Path to the temporary directory.

    Returns:
    -------
        list[str]: List of media file paths.

    """
    # top level function that grabs all media files
    tree_list = tree(pathlib.Path(f"{tmpdirname}"))
    rich.print(tree_list)

    file_to_upload_list = [f"{p}" for p in tree_list]
    logger.debug(f"get_all_media_files_to_upload -> file_to_upload_list = {file_to_upload_list}")
    rich.print(file_to_upload_list)

    return filter_media(file_to_upload_list)


def filter_pth(working_dir: list[str]) -> list[str]:
    """
    Filter .pth files from a directory.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of .pth file paths.

    """
    return [
        f
        for f in working_dir
        if (pathlib.Path(f"{f}").is_file()) and pathlib.Path(f"{f}").suffix.lower() in TORCH_MODEL_EXTENSIONS
    ]


def filter_json(working_dir: list[str]) -> list[str]:
    """
    Filter JSON files from a directory.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of JSON file paths.

    """
    return [
        f
        for f in working_dir
        if (pathlib.Path(f"{f}").is_file()) and pathlib.Path(f"{f}").suffix.lower() in JSON_EXTENSIONS
    ]


def rename_without_cachebuster(working_dir: list[str]) -> list[str]:
    """
    Rename files to remove cache buster query parameters.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of renamed file paths.

    """
    working_dir_only = []
    for f in working_dir:
        if ("?updatedAt" in f"{f}") and (pathlib.Path(f"{f}").is_file()):
            orig = pathlib.Path(f"{f}").absolute()
            # tweetpik now adds cache buster, lets work around it
            without_cb = f"{orig}".split("?updatedAt")[0]
            orig.rename(f"{without_cb}")
            working_dir_only.append(f"{without_cb}")
    return working_dir_only


def filter_videos(working_dir: list[str]) -> list[str]:
    """
    Filter video files from a directory.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of video file paths.

    """
    return [
        f
        for f in working_dir
        if (pathlib.Path(f"{f}").is_file()) and pathlib.Path(f"{f}").suffix.lower() in VIDEO_EXTENSIONS
    ]


def filter_audio(working_dir: list[str]) -> list[str]:
    """
    Filter audio files from a directory.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of audio file paths.

    """
    return [
        f
        for f in working_dir
        if (pathlib.Path(f"{f}").is_file()) and pathlib.Path(f"{f}").suffix.lower() in AUDIO_EXTENSIONS
    ]


def filter_gif(working_dir: list[str]) -> list[str]:
    """
    Filter GIF files from a directory.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of GIF file paths.

    """
    return [
        f
        for f in working_dir
        if (pathlib.Path(f"{f}").is_file()) and pathlib.Path(f"{f}").suffix.lower() in GIF_EXTENSIONS
    ]


def filter_mkv(working_dir: list[str]) -> list[str]:
    """
    Filter MKV files from a directory.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of MKV file paths.

    """
    return [
        f
        for f in working_dir
        if (pathlib.Path(f"{f}").is_file()) and pathlib.Path(f"{f}").suffix.lower() in MKV_EXTENSIONS
    ]


def filter_m3u8(working_dir: list[str]) -> list[str]:
    """
    Filter M3U8 files from a directory.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of M3U8 file paths.

    """
    return [
        f
        for f in working_dir
        if (pathlib.Path(f"{f}").is_file()) and pathlib.Path(f"{f}").suffix.lower() in M3U8_EXTENSIONS
    ]


def filter_webm(working_dir: list[str]) -> list[str]:
    """
    Filter WEBM files from a directory.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of WEBM file paths.

    """
    return [
        f
        for f in working_dir
        if (pathlib.Path(f"{f}").is_file()) and pathlib.Path(f"{f}").suffix.lower() in WEBM_EXTENSIONS
    ]


def filter_images(working_dir: list[str]) -> list[str]:
    """
    Filter image files from a directory.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of image file paths.

    """
    return [
        f
        for f in working_dir
        if (pathlib.Path(f"{f}").is_file()) and pathlib.Path(f"{f}").suffix.lower() in IMAGE_EXTENSIONS
    ]


def filter_pdfs(working_dir: list[str]) -> list[pathlib.PosixPath]:
    """
    Filter PDF files from a directory.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of PDF file paths.

    """
    return [
        f
        for f in working_dir
        if (pathlib.Path(f"{f}").is_file()) and pathlib.Path(f"{f}").suffix.lower() in PDF_EXTENSIONS
    ]


def filter_txts(working_dir: list[str]) -> list[pathlib.PosixPath]:
    """
    Filter TXT files from a directory.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of TXT file paths.

    """
    return [
        f
        for f in working_dir
        if (pathlib.Path(f"{f}").is_file()) and pathlib.Path(f"{f}").suffix.lower() in TXT_EXTENSIONS
    ]


def filter_media(working_dir: list[str]) -> list[str]:
    """
    Filter image and video files from a directory.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of image and video file paths.

    """
    imgs = filter_images(working_dir)
    videos = filter_videos(working_dir)
    return imgs + videos


def filter_pdf(working_dir: list[str]) -> list[pathlib.PosixPath]:
    """
    Filter PDF files from a directory.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of PDF file paths.

    """
    return filter_pdfs(working_dir)


def get_dataframe_from_csv(filename: str, return_parent_folder_name: bool = False) -> DataFrame | tuple[DataFrame, str]:
    """
    Open a CSV file and return a DataFrame.

    Args:
    ----
        filename (str): Path to the CSV file.
        return_parent_folder_name (bool, optional): Whether to return the parent folder name. Defaults to False.

    Returns:
    -------
        DataFrame | tuple[DataFrame, str]: DataFrame or tuple of DataFrame and parent folder name.

    """
    """Open csv files and return a dataframe from pandas

    Args:
        filename (str): path to file
    """
    src: pathlib.Path = pathlib.Path(f"{filename}").resolve()
    df: DataFrame = pd.read_csv(f"{src}")

    # import bpdb
    # bpdb.set_trace()

    return (df, f"{src.parent.stem}") if return_parent_folder_name else df


def sort_dataframe(df: DataFrame, columns: list[str] | None = None, ascending: tuple[bool, ...] = ()) -> DataFrame:
    """
    Sort a DataFrame by specified columns.

    Args:
    ----
        df (DataFrame): DataFrame to sort.
        columns (list[str], optional): Columns to sort by. Defaults to None.
        ascending (tuple[bool, ...], optional): Sort order for each column. Defaults to ().

    Returns:
    -------
        DataFrame: Sorted DataFrame.

    """
    """Return dataframe sorted via columns

    Args:
        df (DataFrame): existing dataframe
        columns (list, optional): [description]. Defaults to []. Eg. ["Total Followers", "Total Likes", "Total Comments", "ERDay", "ERpost"]
        ascending (Tuple, optional): [description]. Defaults to (). Eg. (False, False, False, False, False)

    Returns:
        DataFrame: [description]
    """
    if columns is None:
        columns = []  # type: ignore
    df = df.sort_values(by=columns, ascending=list(ascending))
    return df


def rich_format_followers(val: int) -> str:
    """
    Format follower count with rich text.

    Args:
    ----
        val (int): Follower count.

    Returns:
    -------
        str: Formatted follower count.

    """
    """Given a arbritary int, return a 'rich' string formatting

    Args:
        val (int): eg. followers = 4366347347457

    Returns:
        str: [description] eg. "[bold bright_yellow]4366347347457[/bold bright_yellow]"
    """

    if val > ONE_MILLION:
        return f"[bold bright_yellow]{val}[/bold bright_yellow]"
    elif FIVE_HUNDRED_THOUSAND < val < ONE_MILLION:
        return f"[bold dark_orange]{val}[/bold dark_orange]"
    elif ONE_HUNDRED_THOUSAND < val < FIVE_HUNDRED_THOUSAND:
        return f"[bold orange_red1]{val}[/bold orange_red1]"

    elif FIFTY_THOUSAND < val < ONE_HUNDRED_THOUSAND:
        return f"[bold dodger_blue2]{val}[/bold dodger_blue2]"
    elif THIRTY_THOUSAND < val < FIFTY_THOUSAND:
        return f"[bold purple3]{val}[/bold purple3]"
    elif TWENTY_THOUSAND < val < THIRTY_THOUSAND:
        return f"[bold rosy_brown]{val}[/bold rosy_brown]"
    elif TEN_THOUSAND < val < TWENTY_THOUSAND:
        return f"[bold green]{val}[/bold green]"
    else:
        return f"[bold bright_white]{val}[/bold bright_white]"


def rich_likes_or_comments(val: int) -> str:
    """
    Format likes or comments count with rich text.

    Args:
    ----
        val (int): Likes or comments count.

    Returns:
    -------
        str: Formatted likes or comments count.

    """
    """Given a arbritary int, return a 'rich' string formatting

    Args:
        val (int): eg. followers = 4366347347457

    Returns:
        str: [description] eg. "[bold bright_yellow]4366347347457[/bold bright_yellow]"
    """

    if val <= TEN_THOUSAND:
        return f"[bold bright_yellow]{val}[/bold bright_yellow]"
    elif FIFTY_THOUSAND < val < ONE_HUNDRED_THOUSAND:
        return f"[bold dodger_blue2]{val}[/bold dodger_blue2]"
    elif THIRTY_THOUSAND < val < FIFTY_THOUSAND:
        return f"[bold purple3]{val}[/bold purple3]"
    elif TWENTY_THOUSAND < val < THIRTY_THOUSAND:
        return f"[bold rosy_brown]{val}[/bold rosy_brown]"
    elif TEN_THOUSAND < val < TWENTY_THOUSAND:
        return f"[bold green]{val}[/bold green]"
    else:
        return f"[bold bright_white]{val}[/bold bright_white]"


def rich_display_meme_pull_list(df: DataFrame) -> None:
    """
    Display meme pull list in a rich table format.

    Args:
    ----
        df (DataFrame): DataFrame containing meme pull list data.

    """
    console = Console()

    table = Table(show_header=True, header_style="bold magenta")

    table.add_column("Account")
    table.add_column("Social")
    table.add_column("Total Followers")
    table.add_column("Total Likes")
    table.add_column("Total Comments")
    table.add_column("Total Posts")
    table.add_column("Start Date")
    table.add_column("End Date")
    table.add_column("ERDay")
    table.add_column("ERpost")
    table.add_column("Average Likes")
    table.add_column("Average Comments")
    table.add_column("Links")

    for _index, row in df.iterrows():
        account = f"[bold blue]{row['Account']}[/bold blue]"
        social = f"[bold]{row['Social']}[/bold]"
        total_followers = rich_format_followers(row["Total Followers"])
        total_likes = f"[bold]{row['Total Likes']}[/bold]"
        total_comments = f"[bold]{row['Total Comments']}[/bold]"
        total_posts = f"[bold]{row['Total Posts']}[/bold]"
        start_date = f"[bold]{row['Start Date']}[/bold]"
        end_date = f"[bold]{row['End Date']}[/bold]"
        erday = f"[bold]{row['ERDay']}[/bold]"
        erpost = f"[bold]{row['ERpost']}[/bold]"
        average_likes = f"[bold]{row['Average Likes']}[/bold]"
        average_comments = f"[bold]{row['Average Comments']}[/bold]"
        links = f"[bold]{row['Links']}[/bold]"

        table.add_row(
            account,
            social,
            total_followers,
            total_likes,
            total_comments,
            total_posts,
            start_date,
            end_date,
            erday,
            erpost,
            average_likes,
            average_comments,
            links,
        )

    console.print(table)


def rich_display_popstars_analytics(df: DataFrame) -> None:
    """
    Display popstars analytics in a rich table format.

    Args:
    ----
        df (DataFrame): DataFrame containing popstars analytics data.

    """
    console = Console()

    table = Table(show_header=True, header_style="bold magenta")

    table.add_column("Social")
    table.add_column("Author")
    table.add_column("Url")
    table.add_column("Likes")
    table.add_column("Comments")
    table.add_column("ER")
    table.add_column("Text")
    table.add_column("Date")
    table.add_column("Media 1")

    for _index, row in df.iterrows():
        social = f"[bold]{row['Social']}[/bold]"
        author = f"[bold]{row['Author']}[/bold]"
        url = f"[bold]{row['Url']}[/bold]"
        likes = f"[bold]{rich_likes_or_comments(row['Likes'])}[/bold]"
        comments = f"[bold]{rich_likes_or_comments(row['Comments'])}[/bold]"
        er = f"[bold]{row['ER']}[/bold]"
        text = f"[bold]{row['Text']}[/bold]"
        date = f"[bold]{row['Date']}[/bold]"
        media = f"[bold]{row['Media 1']}[/bold]"

        table.add_row(social, author, url, likes, comments, er, text, date, media)

    console.print(table)



def glob_file_by_extension(working_dir: str, extension: str = "*.mp4", recursive: bool = False) -> list[str]:
    """
    Find files by extension using glob.

    Args:
    ----
        working_dir (str): Directory to search in.
        extension (str, optional): File extension to search for. Defaults to "*.mp4".
        recursive (bool, optional): Whether to search recursively. Defaults to False.

    Returns:
    -------
        list[str]: List of file paths.

    """
    print(f"Searching dir -> {working_dir}/{extension}")

    if recursive:
        # NOTE: When recursive is set True "**"" followed by path separator('./**/') will match any files or directories.
        expression = f"{working_dir}/**/{extension}"
    else:
        expression = f"{working_dir}/{extension}"
    return glob.glob(expression, recursive=recursive)


def print_and_append(dir_listing: list[str], tree_str: str, silent: bool = False) -> None:
    """
    Print and append directory listing.

    Args:
    ----
        dir_listing (list[str]): List to append to.
        tree_str (str): String to print and append.
        silent (bool, optional): Whether to suppress printing. Defaults to False.

    """
    if not silent:
        print(tree_str)
    dir_listing.append(tree_str)


def tree(directory: str | pathlib.Path, silent: bool = False) -> list[pathlib.Path]:
    """
    Generate a tree structure of a directory.

    Args:
    ----
        directory (pathlib.Path): Path to the directory.
        silent (bool, optional): Whether to suppress printing. Defaults to False.

    Returns:
    -------
        list[pathlib.Path]: List of file paths in the directory.

    """
    logger.debug(f"directory -> {directory}")
    if isinstance(directory, str):
        directory = fix_path(directory)
        logger.debug(f"directory -> {directory}")
        directory = pathlib.Path(directory)
        logger.debug(f"directory -> {directory}")
    try:
        assert directory.is_dir()
    except:
        raise OSError(f"{directory} is not a directory.")

    # from ffmpeg_tools import fileobject
    file_system: list[pathlib.Path]
    file_system = []
    _tree = []
    print_and_append(_tree, f"+ {directory}", silent=silent)
    for path in sorted(directory.rglob("*")):
        file_system.append(pathlib.Path(f"{path.resolve()}"))
        try:
            depth = len(path.resolve().relative_to(directory.resolve()).parts)
        except ValueError:
            continue
        spacer = "    " * depth
        print_and_append(_tree, f"{spacer}+ {path.name}", silent=silent)

    return sorted(file_system, key=os.path.getmtime)


# SOURCE: https://python.hotexamples.com/site/file?hash=0xda3708e60cd1ddb3012abd7dba205f48214aee7366f452e93807887c6a04db42&fullName=spring_cleaning.py&project=pambot/SpringCleaning
def format_size(a_file: int) -> str:
    """
    Format file size in human-readable format.

    Args:
    ----
        a_file (int): File size in bytes.

    Returns:
    -------
        str: Formatted file size.

    """
    if a_file >= 1024**3:
        return f"{a_file / float(1024**3):.2f} GB"
    elif a_file >= 1024**2:
        return f"{a_file / float(1024**2):.2f} MB"
    elif a_file >= 1024:
        return f"{a_file / float(1024):.2f} KB"
    else:
        return f"{a_file:.0f} B"


async def aiowrite_file(data: str, dl_dir: str = "./", fname: str = "", ext: str = "") -> None:
    """
    Write data to a file asynchronously.

    Args:
    ----
        data (str): Data to write.
        dl_dir (str, optional): Directory to write to. Defaults to "./".
        fname (str, optional): File name. Defaults to "".
        ext (str, optional): File extension. Defaults to "".

    """
    p_dl_dir = pathlib.Path(dl_dir)
    full_path_dl_dir = f"{p_dl_dir.absolute()}"
    p_new = pathlib.Path(f"{full_path_dl_dir}/{fname}.{ext}")
    logger.debug(f"Writing to {p_new.absolute()}")
    async with aiofiles.open(p_new.absolute(), mode="w") as f:
        await f.write(data)
    # await logger.complete()


async def aioread_file(dl_dir: str = "./", fname: str = "", ext: str = "") -> str:
    """
    Read data from a file asynchronously.

    Args:
    ----
        dl_dir (str, optional): Directory to read from. Defaults to "./".
        fname (str, optional): File name. Defaults to "".
        ext (str, optional): File extension. Defaults to "".

    Returns:
    -------
        str: The content of the file.

    """
    p_dl_dir = pathlib.Path(dl_dir)
    full_path_dl_dir = f"{p_dl_dir.absolute()}"
    p_new = pathlib.Path(f"{full_path_dl_dir}/{fname}.{ext}")
    logger.debug(f"Reading from {p_new.absolute()}")
    async with aiofiles.open(p_new.absolute()) as f:
        content = await f.read()
    # await logger.complete()
    return content


def check_file_size(a_file: str) -> tuple[bool, str]:
    """
    Check if a file size exceeds the maximum allowed size.

    Args:
    ----
        a_file (str): Path to the file.

    Returns:
    -------
        tuple[bool, str]: Tuple containing a boolean indicating if the file size exceeds the limit and a message.

    """
    p = pathlib.Path(a_file)
    file_size = p.stat().st_size
    logger.debug(f"File: {p} | Size(bytes): {file_size} | Size(type): {type(file_size)}")
    check = file_size > MAX_BYTES_UPLOAD_DISCORD
    msg = f"Is file size greater than {MAX_BYTES_UPLOAD_DISCORD}: {check}"
    logger.debug(msg)
    return check, msg


# ------------------------------------------------------------
# NOTE: MOVE THIS TO A FILE UTILITIES LIBRARY
# ------------------------------------------------------------
# SOURCE: https://github.com/tgbugs/pyontutils/blob/05dc32b092b015233f4a6cefa6c157577d029a40/ilxutils/tools.py
def is_file(path: str) -> bool:
    """
    Check if a path points to a file.

    Args:
    ----
        path (str): Path to check.

    Returns:
    -------
        bool: True if the path points to a file, False otherwise.

    """
    """Check if path contains a file

    Args:
        path (_type_): _description_

    Returns:
        _type_: _description_
    """
    return pathlib.Path(path).is_file()


def is_directory(path: str) -> bool:
    """
    Check if a path points to a directory.

    Args:
    ----
        path (str): Path to check.

    Returns:
    -------
        bool: True if the path points to a directory, False otherwise.

    """
    """Check if path contains a dir

    Args:
        path (str): _description_

    Returns:
        _type_: _description_
    """
    return pathlib.Path(path).is_dir()


def is_a_symlink(path: str) -> bool:
    """
    Check if a path points to a symlink.

    Args:
    ----
        path (str): Path to check.

    Returns:
    -------
        bool: True if the path points to a symlink, False otherwise.

    """
    """Check if path contains a dir

    Args:
        path (str): _description_

    Returns:
        _type_: _description_
    """
    return pathlib.Path(path).is_symlink()


def expand_path_str(path: str) -> pathlib.Path:
    """
    Expand a path string to a full path.

    Args:
    ----
        path (str): Path string to expand.

    Returns:
    -------
        pathlib.Path: Expanded path.

    """
    """_summary_

    Args:
        path (str): _description_

    Returns:
        pathlib.PosixPath: _description_
    """
    return pathlib.Path(tilda(path))


def tilda(obj: str | list[str]) -> str | list[str]:
    """
    Expand tilde to home directory in a path.

    Args:
    ----
        obj (str | list[str]): Path string or list of path strings.

    Returns:
    -------
        str | list[str]: Expanded path string or list of expanded path strings.

    """
    """wrapper for linux ~/ shell notation

    Args:
        obj (_type_): _description_

    Returns:
        _type_: _description_
    """
    if isinstance(obj, list):
        return [str(pathlib.Path(o).expanduser()) if isinstance(o, str) else o for o in obj]
    elif isinstance(obj, str):
        return str(pathlib.Path(obj).expanduser())
    else:
        return obj


def fix_path(path: str) -> str | list[str]:
    """
    Automatically convert path to fully qualified file URI.

    Args:
    ----
        path (str): Path string to fix.

    Returns:
    -------
        str | list[str]: Fixed path string or list of fixed path strings.

    """

    def __fix_path(path):
        if not isinstance(path, str):
            return path
        elif path[0] == "~":
            tilda_fixed_path = tilda(path)
            if is_file(tilda_fixed_path):
                return tilda_fixed_path
            else:
                exit(path, ": does not exit.")
        elif is_file(pathlib.Path.home() / path) or is_directory(pathlib.Path.home() / path):
            return str(pathlib.Path().home() / path)
        else:
            return path

    if isinstance(path, str):
        return __fix_path(path)
    elif isinstance(path, list):
        return [__fix_path(p) for p in path]
    else:
        return path


def unlink_orig_file(a_filepath: str) -> str:
    """
    _summary_

    Args:
    ----
        a_filepath (str): _description_

    Returns:
    -------
        _type_: _description_

    """
    logger.debug(f"deleting ... {a_filepath}")
    rich.print(f"deleting ... {a_filepath}")
    os.unlink(f"{a_filepath}")
    return a_filepath


def get_files_to_upload(tmpdirname: str) -> list[str]:
    """
    Get directory and iterate over files to upload

    Args:
    ----
        tmpdirname (str): _description_

    Returns:
    -------
        _type_: _description_

    """
    tree_list = tree(pathlib.Path(f"{tmpdirname}"))
    rich.print(tree_list)

    file_to_upload_list = [f"{p}" for p in tree_list]
    logger.debug(f"get_files_to_upload -> file_to_upload_list = {file_to_upload_list}")
    rich.print(file_to_upload_list)

    file_to_upload = filter_media(file_to_upload_list)

    logger.debug(f"get_files_to_upload -> file_to_upload = {file_to_upload}")

    rich.print(file_to_upload)
    return file_to_upload


def run_tree(tmpdirname: str) -> list[str]:
    """
    run_tree

    Args:
    ----
        tmpdirname (str): _description_

    Returns:
    -------
        _type_: _description_

    """
    # Now that we are finished processing, we can upload the files to discord

    tree_list = tree(pathlib.Path(f"{tmpdirname}"))
    rich.print("tree_list ->")
    rich.print(tree_list)

    file_to_upload_list = [f"{p}" for p in tree_list]
    logger.debug(f"compress_video-> file_to_upload_list = {file_to_upload_list}")
    rich.print(file_to_upload_list)

    file_to_upload = filter_media(file_to_upload_list)

    return file_to_upload


# smoke tests

async def aprint_and_append(dir_listing: list[str], tree_str: str, silent: bool = False) -> None:
    """
    Async version of print_and_append function.

    Args:
    ----
        dir_listing (list[str]): List to append to.
        tree_str (str): String to print and append.
        silent (bool, optional): Whether to suppress printing. Defaults to False.

    """
    if not silent:
        print(tree_str)  # We keep print since it's not blocking
    dir_listing.append(tree_str)

async def atree(directory: str | pathlib.Path, silent: bool = False) -> list[pathlib.Path]:
    """
    Async version of tree function to generate a tree structure of a directory.

    Args:
    ----
        directory (pathlib.Path): Path to the directory.
        silent (bool, optional): Whether to suppress printing. Defaults to False.

    Returns:
    -------
        list[pathlib.Path]: List of file paths in the directory.

    """
    logger.debug(f"directory -> {directory}")
    if isinstance(directory, str):
        directory = fix_path(directory)
        logger.debug(f"directory -> {directory}")
        directory = pathlib.Path(directory)
        logger.debug(f"directory -> {directory}")
    try:
        assert directory.is_dir()
    except:
        raise OSError(f"{directory} is not a directory.")

    file_system: list[pathlib.Path] = []
    _tree = []
    await aprint_and_append(_tree, f"+ {directory}", silent=silent)

    # Get all paths using rglob synchronously since it's a generator
    paths = sorted(directory.rglob("*"))

    for path in paths:
        file_system.append(pathlib.Path(f"{path.resolve()}"))
        try:
            depth = len(path.resolve().relative_to(directory.resolve()).parts)
        except ValueError:
            continue
        spacer = "    " * depth
        await aprint_and_append(_tree, f"{spacer}+ {path.name}", silent=silent)

    return sorted(file_system, key=os.path.getmtime)

async def afilter_images(working_dir: list[str]) -> list[str]:
    """
    Async version of filter_images function.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of image file paths.

    """
    filtered = []
    for f in working_dir:
        path = pathlib.Path(f"{f}")
        if path.is_file() and path.suffix.lower() in IMAGE_EXTENSIONS:
            filtered.append(f)
    return filtered

async def afilter_videos(working_dir: list[str]) -> list[str]:
    """
    Async version of filter_videos function.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of video file paths.

    """
    filtered = []
    for f in working_dir:
        path = pathlib.Path(f"{f}")
        if path.is_file() and path.suffix.lower() in VIDEO_EXTENSIONS:
            filtered.append(f)
    return filtered

async def afilter_media(working_dir: list[str]) -> list[str]:
    """
    Async version of filter_media function.
    Filters image and video files from a directory.

    Args:
    ----
        working_dir (list[str]): List of file paths.

    Returns:
    -------
        list[str]: List of image and video file paths.

    """
    imgs = await afilter_images(working_dir)
    videos = await afilter_videos(working_dir)
    return imgs + videos

async def aget_files_to_upload(tmpdirname: str) -> list[str]:
    """
    Async version of get_files_to_upload.
    Get directory and iterate over files to upload.

    Args:
    ----
        tmpdirname (str): Directory path to process.

    Returns:
    -------
        list[str]: List of files to upload.

    """
    tree_list = await atree(pathlib.Path(f"{tmpdirname}"))
    rich.print(tree_list)

    file_to_upload_list = [f"{p}" for p in tree_list]
    logger.debug(f"aget_files_to_upload -> file_to_upload_list = {file_to_upload_list}")
    rich.print(file_to_upload_list)

    file_to_upload = await afilter_media(file_to_upload_list)

    logger.debug(f"aget_files_to_upload -> file_to_upload = {file_to_upload}")

    rich.print(file_to_upload)
    return file_to_upload

async def arun_tree(tmpdirname: str) -> list[str]:
    """
    Async version of run_tree.

    Args:
    ----
        tmpdirname (str): Directory path to process.

    Returns:
    -------
        list[str]: List of media files found.

    """
    # Now that we are finished processing, we can upload the files to discord
    tree_list = await atree(pathlib.Path(f"{tmpdirname}"))
    rich.print("tree_list ->")
    rich.print(tree_list)

    file_to_upload_list = [f"{p}" for p in tree_list]
    logger.debug(f"compress_video-> file_to_upload_list = {file_to_upload_list}")
    rich.print(file_to_upload_list)

    file_to_upload = await afilter_media(file_to_upload_list)

    return file_to_upload

</document_content>
</document>
<document index="39">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/file_operations.py</source>
<document_content>
# pylint: disable=no-member
# pylint: disable=possibly-used-before-assignment
# pyright: reportImportCycles=false
# pyright: reportAttributeAccessIssue=false
# mypy: disable-error-code="index"
# mypy: disable-error-code="no-redef"

"""Utility functions for file system operations."""

from __future__ import annotations

import asyncio
import base64
import io
import os
import pathlib
import sys
import uuid

from io import BytesIO
from typing import Any, Dict, List, Tuple, Union

import aiofiles
import aiofiles.os
import aiohttp
import structlog

from discord import Attachment, File


logger = structlog.get_logger(__name__)
from PIL import Image

from democracy_exe import shell


async def download_image(url: str) -> BytesIO:
    """Download an image from a given URL asynchronously.

    Args:
        url (str): The URL of the image to download.

    Returns:
        BytesIO: The downloaded image data as a BytesIO object.
    """
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            if response.status == 200:
                data = await response.read()
                return io.BytesIO(data)
            else:
                raise ValueError(f"Failed to download image. Status code: {response.status}")


async def file_to_data_uri(file: File) -> str:
    """Convert a discord.File object to a data URI.

    Args:
        file (File): The discord.File object to convert.

    Returns:
        str: The data URI string representation of the file.
    """
    with BytesIO(file.fp.read()) as f:
        file_bytes = f.read()
    base64_encoded = base64.b64encode(file_bytes).decode("ascii")
    return f"data:image;base64,{base64_encoded}"


async def data_uri_to_file(data_uri: str, filename: str) -> File:
    """Convert a data URI to a discord.File object.

    Args:
        data_uri (str): The data URI string to convert.
        filename (str): The filename to use for the resulting File object.

    Returns:
        File: The converted discord.File object.
    """
    metadata, base64_data = data_uri.split(",")
    content_type = metadata.split(";")[0].split(":")[1]
    file_bytes = base64.b64decode(base64_data)
    return File(BytesIO(file_bytes), filename=filename, spoiler=False)


def file_to_local_data_dict(fname: str, dir_root: str) -> dict[str, Any]:
    """Convert a file to a dictionary with metadata.

    Args:
        fname (str): The filename of the file.
        dir_root (str): The root directory path.

    Returns:
        dict[str, Any]: A dictionary containing the file metadata.
    """
    file_api = pathlib.Path(fname)
    return {
        "filename": f"{dir_root}/{file_api.stem}{file_api.suffix}",
        "size": file_api.stat().st_size,
        "ext": f"{file_api.suffix}",
        "api": file_api,
    }


def create_nested_directories(file_path: str) -> str:
    """
    Create nested directories from a file path.

    Args:
        file_path (str): The file path containing the nested directories.

    Returns:
        str: The path of the created nested directories.
    """
    path = pathlib.Path(file_path)
    parent_dir = path.parent
    parent_dir.mkdir(parents=True, exist_ok=True)
    return str(parent_dir)


async def handle_save_attachment_locally(attm_data_dict: dict[str, Any], dir_root: str) -> str:
    """Save a Discord attachment locally.

    Args:
        attm_data_dict (dict[str, Any]): A dictionary containing the attachment data.
        dir_root (str): The root directory path to save the attachment.

    Returns:
        str: The path of the saved attachment file.
    """
    fname = f"{dir_root}/orig_{attm_data_dict['id']}_{attm_data_dict['filename']}"
    print(f"Saving to ... {fname}")
    parent_dir = await aio_create_nested_directories(fname)
    logger.info(f"created parent_dir = {parent_dir}")
    await attm_data_dict["attachment_obj"].save(fname, use_cached=True)
    await asyncio.sleep(1)
    return fname


async def details_from_file(path_to_media_from_cli: str, cwd: str | None = None) -> tuple[str, str, str]:
    """Generate input and output file paths and retrieve the timestamp of the input file.

    Args:
        path_to_media_from_cli (str): The path to the media file from the command line.
        cwd (Union[str, None], optional): The current working directory. Defaults to None.

    Returns:
        tuple[str, str, str]: A tuple containing the input file path, output file path, and timestamp.
    """
    p = pathlib.Path(path_to_media_from_cli)
    full_path_input_file = f"{p.stem}{p.suffix}"
    full_path_output_file = f"{p.stem}_smaller.mp4"
    print(full_path_input_file)
    print(full_path_output_file)
    if sys.platform == "darwin":
        get_timestamp = await shell._aio_run_process_and_communicate(
            ["gstat", "-c", "%y", f"{p.stem}{p.suffix}"], cwd=cwd
        )
    elif sys.platform == "linux":
        get_timestamp = await shell._aio_run_process_and_communicate(
            ["stat", "-c", "%y", f"{p.stem}{p.suffix}"], cwd=cwd
        )

    return full_path_input_file, full_path_output_file, get_timestamp


async def aio_create_temp_directory() -> str:
    """Create a temporary directory and return its path.

    Returns:
        str: The path of the created temporary directory.
    """
    tmpdirname = f"temp/{uuid.uuid4()!s}"
    try:
        # await aiofiles.os.mkdir(os.path.dirname(tmpdirname))
        await aiofiles.os.makedirs(os.path.dirname(tmpdirname), exist_ok=True)
    except Exception as e:
        logger.error(f"Error creating temporary directory: {e}")
        raise e
    print("created temporary directory", tmpdirname)
    logger.info("created temporary directory", tmpdirname)
    # await logger.complete()
    return tmpdirname


def create_temp_directory() -> str:
    """Create a temporary directory and return its path.

    Returns:
        str: The path of the created temporary directory.
    """
    tmpdirname = f"temp/{uuid.uuid4()!s}"
    os.makedirs(os.path.dirname(tmpdirname), exist_ok=True)
    print("created temporary directory", tmpdirname)
    logger.info("created temporary directory", tmpdirname)
    return tmpdirname


def get_file_tree(directory: str) -> list[str]:
    """Get the directory tree of the given directory.

    Args:
        directory (str): The directory path to get the file tree from.

    Returns:
        list[str]: A list of file paths in the directory tree.
    """
    return [str(p) for p in pathlib.Path(directory).rglob("*")]


async def aio_create_nested_directories(file_path: str) -> str:
    """
    Create nested directories from a file path asynchronously.

    Args:
        file_path (str): The file path containing the nested directories.

    Returns:
        str: The path of the created nested directories.

    """
    path = pathlib.Path(file_path)
    parent_dir = path.parent
    logger.info(f"path = {path}")
    logger.info(f"parent_dir = {parent_dir}")
    await aiofiles.os.makedirs(parent_dir, exist_ok=True)
    # await logger.complete()
    return str(parent_dir)


async def acreate_temp_directory() -> str:
    """Create a temporary directory and return its path.

    This is an alias for aio_create_temp_directory() to maintain consistent naming conventions.

    Returns:
        str: The path of the created temporary directory.
    """
    return await aio_create_temp_directory()

</document_content>
</document>
<document index="40">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/files_import.py</source>
<document_content>
"""Utility functions to import files or folders."""

from __future__ import annotations

import os

from typing import Any, List

from langchain_core.documents import Document

from democracy_exe.ai import index
from democracy_exe.ai.load_file import read


def index_file_folder(
    file_path: str,
    collection: str,
    **kwargs: Any,
) -> int:
    """Load documents in collection index from a file or folder"""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File {file_path} does not exist.")

    chunks_num = 0
    docs = load_file_folder_documents(file_path=file_path, **kwargs)
    for doc in docs:
        chunks_num += index.add_document(
            document=doc,
            collection_name=collection,
        )

    return chunks_num


def load_file_folder_documents(file_path: str, **kwargs: Any) -> list[Document]:
    """Load documents from file or folder"""
    if os.path.isdir(file_path):
        docs = []
        for file in os.listdir(file_path):
            docs.append(
                Document(
                    page_content=read(file_path=f"{file_path}/{file}", **kwargs),
                    metadata={"type": "files", "path": file_path},
                )
            )
        return docs

    doc = Document(
        page_content=read(file_path=file_path, **kwargs),
        metadata={"type": "files", "path": file_path},
    )

    return [doc]

</document_content>
</document>
<document index="41">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/formatutil.py</source>
<document_content>
"""A set of functions to format text."""

# SOURCE: https://github.com/CrosswaveOmega/NikkiBot/blob/main/utility/formatutil.py
from __future__ import annotations

import os
import re

from typing import Any

import aiohttp

from langchain_text_splitters import CharacterTextSplitter
from PyPDF2 import PdfReader


bar_emoji = {
    "1e": "<a:emptyleft:1118209186917011566>",
    "1h": "<a:halfleft:1118209195683094639>",
    "1f": "<a:fullleft:1118209191845318806>",
    "e": "<a:emptymiddle:1118209505973522584>",
    "h": "<a:halfmiddle:1118209197486645269>",
    "f": "<a:fullmiddle:1118209193221029982>",
    "2e": "<a:emptyright:1118209190553460866>",
    "2h": "<a:halfright:1118209198967238716>",
    "2f": "<a:fullright:1118209194257031230>",
}


def chunk_list(sentences: list[Any], chunk_size: int = 10) -> list[list[str]]:
    """
    Summary:
    Chunk a list into blocks of a specified size. Chunk list into blocks of 10.

    Explanation:
    This function takes a list of 'sentences' and divides it into blocks of size 'chunk_size'. It returns a list of lists where each inner list contains a chunk of sentences based on the specified chunk size.

    Args:
    ----
    - sentences (List[Any]): The list of items to be chunked.
    - chunk_size (int): The size of each chunk. Default is 10.

    Returns:
    -------
    - List[List[str]]: A list of lists where each inner list represents a chunk of sentences.

    """
    return [sentences[i : i + chunk_size] for i in range(0, len(sentences), chunk_size)]


def progress_bar(current: int, total: int, width: int = 5):
    """Print a progress bar."""
    current = min(current, total)
    # if current > total:
    #     current = total
    fraction = float(current / total)
    filled_width = int(fraction * width)
    half_width = int(fraction * width * 2) % 2
    empty_width = width - filled_width - half_width
    bar = f"{'f' * filled_width}{'h' * half_width}{'e' * empty_width}"
    if len(bar) <= 1:
        return f"{bar_emoji['1e']}{bar_emoji['2e']}"
    middle = "".join(bar_emoji[i] for i in bar[1:-1])
    return f"{bar_emoji[f'1{bar[0]}']}{middle}{bar_emoji[f'2{bar[-1]}']}"


# https://github.com/darren-rose/DiscordDocChatBot/blob/63a2f25d2cb8aaace6c1a0af97d48f664588e94e/main.py#L28
def extract_url(s: str):
    # Regular expression to match URLs
    """
    Summary:
    Extract URLs from a given string.

    Explanation:
    This function uses a regular expression pattern to extract URLs from the input string 's'. It then returns a list of URLs found in the string.

    Args:
    ----
    - s (str): The input string from which URLs need to be extracted.

    Returns:
    -------
    - List[str]: A list of URLs extracted from the input string.

    """
    url_pattern = re.compile(r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+")
    return re.findall(url_pattern, s)


async def download_html(url: str, filepath: str):
    """
    Summary:
    Download HTML content from a URL and save it to a file.

    Explanation:
    This asynchronous function downloads the HTML content from the specified URL using a GET request. It then writes the HTML content to the file specified by 'filepath'. If the GET request is unsuccessful, it prints an error message.

    Args:
    ----
    - url (str): The URL from which to download the HTML content.
    - filepath (str): The file path where the HTML content will be saved.

    Returns:
    -------
    - None

    """
    # Send a GET request to the URL
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            # Check that the GET request was successful
            if response.status == 200:
                # Write the response content (the HTML) to a file
                with open(filepath, "w", encoding="utf-8") as f:
                    f.write(response.text)
            else:
                print(f"Failed to download {url}: status code {response.status}")


def get_pdf_text(path: str):
    # define the path
    """
    Summary:
    Extract text content from PDF files in a specified directory.

    Explanation:
    This function iterates through the files in the provided directory 'path' and extracts text content from PDF files. It reads each PDF file, extracts text from each page, and concatenates the text from all pages into a single string that is returned.

    Args:
    ----
    - path (str): The directory path containing the PDF files to extract text from.

    Returns:
    -------
    - str: The concatenated text content extracted from all PDF files in the directory.

    """
    for filename in os.listdir(path):
        # check if the file is a pdf
        if filename.endswith(".pdf"):
            text = ""
            with open(os.path.join(path, filename), "rb") as pdf_doc:
                pdf_reader = PdfReader(pdf_doc)
                for page in pdf_reader.pages:
                    text += page.extract_text()
                # os.remove(os.path.join(path, filename))
            return text


def get_text_chunks(text: str):
    """
    Summary:
    Split text into chunks based on specified parameters.

    Explanation:
    This function takes a text input and splits it into chunks using a CharacterTextSplitter object with defined parameters such as separator, chunk size, chunk overlap, and length function. It returns a list of text chunks based on the splitting criteria.

    Args:
    ----
    - text (str): The input text to be split into chunks.

    Returns:
    -------
    - List[str]: A list of text chunks split based on the specified parameters.

    """
    text_splitter = CharacterTextSplitter(separator="\n", chunk_size=1000, chunk_overlap=200, length_function=len)
    return text_splitter.split_text(text)

</document_content>
</document>
<document index="42">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/globalfunctions.py</source>
<document_content>
# pylint: disable=no-member
# pylint: disable=possibly-used-before-assignment
# pyright: reportImportCycles=false
# pyright: reportAttributeAccessIssue=false
# pyright: reportInvalidTypeForm=false
# mypy: disable-error-code="index"
# mypy: disable-error-code="no-redef"
# SOURCE: https://github.com/CrosswaveOmega/NikkiBot/blob/main/utility/globalfunctions.py
from __future__ import annotations

import os
import re
import site

from collections.abc import Callable
from datetime import UTC, datetime, timezone
from io import BytesIO
from re import Pattern
from typing import List, Tuple, Union

import discord

from discord.utils import escape_markdown
from PIL import Image


def find_urls(text: str) -> list[str]:
    """
    Find URLs within a given text.

    This function uses a regular expression pattern to search for URLs within the input text 'text'.
    It then returns a list of URLs found in the text.

    Args:
        text (str): The text in which to search for URLs.

    Returns:
        List[str]: A list of URLs extracted from the input text.
    """
    url_pattern = r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
    return re.findall(url_pattern, text)


def split_and_cluster_strings(
    input_string: str, max_cluster_size: int, split_substring: str | Pattern, length: Callable[[str], int] = len
) -> list[str]:
    """
    Split up the input_string by the split_substring and group the resulting substrings into
    clusters of about max_cluster_size length. Return the list of clusters.

    Args:
        input_string (str): The string to be split and clustered.
        max_cluster_size (int): The preferred maximum length of each cluster.
        split_substring (Union[str, Pattern]): The substring or regex pattern used to split the input_string.
        length (Callable[[str], int], optional): Function to determine string length with. Defaults to len.

    Returns:
        List[str]: A list of clusters.
    """
    clusters = []
    # There's no reason to split if input is already less than max_cluster_size
    if length(input_string) < max_cluster_size:
        return [input_string]

    split_by = split_substring

    is_regex = isinstance(split_substring, re.Pattern)
    if is_regex:
        result = split_substring.split(input_string)
        substrings = [r for r in result if r]
    else:
        if "%s" not in split_substring:
            split_by = f"%s{split_by}"
        split_character = split_by.replace("%s", "")

        # Split the input string based on the specified substring
        substrings = input_string.split(split_character)

    # No reason to run the loop if there's less than two
    # strings within the substrings list.  That means
    # it couldn't find anything to split up.
    if len(substrings) < 2:
        return [input_string]

    current_cluster = substrings[0]
    for substring in substrings[1:]:
        new_string = substring if is_regex else split_by.replace("%s", substring, 1)
        sublength = length(new_string)
        if length(current_cluster) + sublength <= max_cluster_size:
            # Add the substring to the current cluster
            current_cluster += new_string
        else:
            # Adding to the current cluster will exceed the maximum size,
            # So start a new cluster.
            if current_cluster:
                # Don't add to clusters if current_cluster is empty.
                clusters.append(current_cluster)
            current_cluster = new_string if substring else ""
    if current_cluster:
        clusters.append(current_cluster)  # Remove the trailing split_substring

    return clusters


def prioritized_string_split(
    input_string: str,
    substring_split_order: list[str | tuple[str, int]],
    default_max_len: int = 1024,
    trim: bool = False,
    length: Callable[[str], int] = len,
) -> list[str]:
    """
    Segment the input string based on the delimiters specified in `substring_split_order`.
    Then, concatenate these segments to form a sequence of grouped strings,
    ensuring that no cluster surpasses a specified maximum length.
    The maximum length for each cluster addition can be individually adjusted along with the list of delimiters.

    Args:
        input_string (str): The string to be split.
        substring_split_order (List[Union[str, Tuple[str, int]]]): A list of strings or tuples containing
            the delimiters to split by and their max lengths. If an argument here is "%s\\n", then the input
            string will be split by "\\n" and will place the relevant substrings in the position given by %s.
        default_max_len (int, optional): The maximum length a string in a cluster may be if not given
            within a specific tuple for that delimiter. Defaults to 1024.
        trim (bool, optional): If True, trim leading and trailing whitespaces in each cluster. Defaults to False.
        length (Callable[[str], int], optional): Function to determine string length with. Defaults to len.

    Returns:
        List[str]: A list of clusters containing the split substrings.
    """
    # Initalize new cluster
    current_clusters = [input_string]
    for _e, arg in enumerate(substring_split_order):
        if isinstance(arg, (str, re.Pattern)):
            s, max_len = arg, None
        elif len(arg) == 1:
            s, max_len = arg[0], None
        else:
            s, max_len = arg

        max_len = max_len or default_max_len  # Use default if not specified
        split_substring = s
        new_splits = []

        for cluster in current_clusters:
            result_clusters = split_and_cluster_strings(cluster, max_len, split_substring, length=length)
            new_splits.extend(result_clusters)
        # for c_num, cluster in enumerate(new_splits):
        #    print(f"Pass {e},  Cluster {c_num + 1}: {len(cluster)}, {len(cluster)}")
        current_clusters = new_splits

    # Optional trimming of leading and trailing whitespaces
    if trim:
        current_clusters = [cluster.strip() for cluster in current_clusters]

    return current_clusters


def split_string_with_code_blocks(input_str: str, max_length: int, oncode: bool = False) -> list[str]:
    """
    Split a string into segments based on specified criteria.

    This function takes an input string 'input_str' and splits it into segments based on various splitting
    criteria such as Markdown headings, horizontal lines, and code blocks. It returns a list of segmented
    strings based on the provided maximum length 'max_length' and the presence of code blocks.

    Args:
        input_str (str): The input string to be split into segments.
        max_length (int): The maximum length of each segment.
        oncode (bool, optional): Flag to indicate whether to split on code blocks. Defaults to False.

    Returns:
        List[str]: A list of segmented strings based on the splitting criteria and maximum length.
    """
    tosplitby = [
        # First, try to split along Markdown headings (starting with level 2)
        "\n#{1,6} ",
        # Note the alternative syntax for headings (below) is not handled here
        # Heading level 2
        # ---------------
        # Horizontal lines
        "\n\\*\\*\\*+\n",
        "\n---+\n",
        "\n___+\n",
        " #{1,6} ",
        # Note that this splitter doesn't handle horizontal lines defined
        # by *three or more* of ***, ---, or ___, but this is not handled
        "\n\n",
        "\n",
        " ",
        "",
    ]
    if len(input_str) <= max_length:
        return [input_str]
    symbol = re.escape("```")
    pattern = re.compile(f"({symbol}(?:(?!{symbol}).)+{symbol})", re.DOTALL)

    splitorder = [pattern, "\n### %s", "%s\n", " %s"]
    return prioritized_string_split(input_str, splitorder, default_max_len=max_length)


def replace_working_directory(text: str) -> str:
    """
    Replace the current working directory with a shorthand in the given text.

    Args:
        text (str): The text in which to replace the current working directory.

    Returns:
        str: The text with the current working directory replaced by a shorthand.
    """
    cwd = os.getcwd()  # Replace backslashes for regex

    parent_dir = os.path.dirname(cwd)
    replaced_string = text
    for rawsite in site.getsitepackages():
        sites = os.path.dirname(rawsite)
        replaced_string = re.sub(re.escape(sites), "site", text, flags=re.IGNORECASE)

    replaced_string = re.sub(re.escape(parent_dir), "..", replaced_string, flags=re.IGNORECASE)

    return escape_markdown(replaced_string)


# def filter_trace_stack(stack):
#     """This function is for filtering call stacks so that ONLY the trace related to code files is shown"""
#     cwd = os.getcwd()  # Replace backslashes for regex
#     newlines = []

#     parent_dir = os.path.dirname(cwd)
#     for line in stack:
#         if parent_dir.upper() in line.upper().strip() and not ".venv" in line.strip():
#             newlines.append(line)
#     replaced_string = "\n".join(newlines)

#     return escape_markdown(replaced_string)


"""Utility functions here to assist."""


def the_string_numerizer(
    num: int | float,
    thestring: str,
    comma: bool = False,
    force: bool = False,
    have_s: bool = True,
) -> str:
    """
    Format a numerical value and string into a readable format.

    This function takes a numerical value 'num', a string 'thestring', and optional parameters to format
    the value and string together. It returns a formatted string with the numerical value, string, and
    optional pluralization and comma based on the provided parameters.

    Args:
        num (Union[int, float]): The numerical value to be formatted.
        thestring (str): The string to be combined with the numerical value.
        comma (bool, optional): Flag to include a comma in the formatted string. Defaults to False.
        force (bool, optional): Flag to force formatting even if the numerical value is 0. Defaults to False.
        have_s (bool, optional): Flag to pluralize the string if the numerical value is greater than 1. Defaults to True.

    Returns:
        str: The formatted string combining the numerical value and string based on the provided parameters.
    """
    if num > 0 or force:
        retstr = f"{num:.2f} {thestring}"
        if num > 1 and have_s:
            retstr += "s"
        if comma:
            retstr += ", "
        return retstr
    return ""


def seconds_to_time_string(seconds_start: int | float) -> str:
    """
    Convert seconds to a string representation of days, hours, minutes, and seconds.

    Args:
        seconds_start (Union[int, float]): The number of seconds to convert.

    Returns:
        str: A string representation of days, hours, minutes, and seconds.
    """
    return_string = ""
    seconds = seconds_start % 60
    minutes_r = (seconds_start - seconds) // 60
    minutes = minutes_r % 60
    hours_r = (minutes_r - minutes) // 60
    hours = hours_r % 24
    days = (hours_r - hours) // 24

    return "{}{}{}{}".format(
        the_string_numerizer(days, "day", True),
        the_string_numerizer(hours, "hour", True),
        the_string_numerizer(minutes, "minute", True),
        the_string_numerizer(seconds, "second", force=True),
    )


def seconds_to_time_stamp(seconds_init: int | float) -> str:
    """
    Convert seconds to a time stamp string in the format 'd:h:m:s'.

    This function takes an initial number of seconds 'seconds_init' and converts it into a time stamp string
    representing days, hours, minutes, and seconds. It calculates the corresponding values for days, hours,
    minutes, and seconds and returns the formatted time stamp string.

    Args:
        seconds_init (Union[int, float]): The initial number of seconds to convert to a time stamp.

    Returns:
        str: The time stamp string in the format 'd:h:m:s' representing the converted seconds.
    """
    return_string = ""
    seconds_start = int(round(seconds_init))
    seconds = seconds_start % 60
    minutes_r = (seconds_start - seconds) // 60
    minutes = minutes_r % 60
    hours_r = (minutes_r - minutes) // 60
    hours = hours_r % 24
    if hours > 1:
        return_string += f"{hours:02d}:"
    return_string += f"{minutes:02d}:{seconds:02d}"
    return return_string


async def get_server_icon_color(guild: discord.Guild) -> str | int:
    """
    Get the guild icon color.

    Args:
        guild (discord.Guild): The guild to get the icon color from.

    Returns:
        Union[str, int]: The guild icon color as a hex string or integer.
    """
    "Get the guild icon, and color."
    if not guild.icon:
        return 0xFFFFFF
    icon_bytes = await guild.icon.read()
    icon_image = Image.open(BytesIO(icon_bytes))

    # Resize the image to 1x1 and get the most visible average color
    icon_image = icon_image.resize((1, 1))
    icon_color = icon_image.getpixel((0, 0))

    # Convert the color to hex format
    hex_color = f"{icon_color[0]:02x}{icon_color[1]:02x}{icon_color[2]:02x}"
    return int(hex_color, 16)


def extract_timestamp(timestamp: str) -> datetime:
    """
    Extract a timestamp string and convert it to a datetime object.

    This function takes a timestamp string 'timestamp' and adjusts it to include up to 6 digits of fractional
    seconds. It then converts the adjusted timestamp string to a datetime object with timezone information
    and returns the datetime object.

    Args:
        timestamp (str): The timestamp string to be converted to a datetime object.

    Returns:
        datetime: The datetime object representing the converted timestamp with timezone information.
    """
    # Define the format of the timestamp string (with 7-digit fractional seconds)
    format_string = "%Y-%m-%dT%H:%M:%S.%fZ"

    # Extract the fractional seconds (up to 6 digits) and Z separately
    timestamp_parts = timestamp.split(".")
    timestamp_adjusted = timestamp
    if len(timestamp_parts) >= 2:
        timestamp_adjusted = f"{timestamp_parts[0]}.{timestamp_parts[1][:6]}"
    else:
        format_string = "%Y-%m-%dT%H:%M:%SZ"
        # timestamp_adjusted=timestamp_adjusted
    if not timestamp_adjusted.endswith("Z"):
        timestamp_adjusted += "Z"
    return datetime.strptime(timestamp_adjusted, format_string).replace(tzinfo=UTC)


def human_format(num: int | float) -> str:
    """
    Format a large number into a human-readable string.

    Args:
        num (Union[int, float]): The number to format.

    Returns:
        str: The formatted number as a string with a suffix (e.g., 1.5K, 2.3M).
    """
    """Format a large number"""
    num = float(f"{num:.3g}")
    magnitude = 0
    while abs(num) >= 1000:
        magnitude += 1
        num /= 1000.0
    suffixes = ["", "K", "M", "B", "T", "Q", "Qi"]
    return "{}{}".format(f"{num:f}".rstrip("0").rstrip("."), suffixes[magnitude])

</document_content>
</document>
<document index="43">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/huggingfaceutils.py</source>
<document_content>
# from __future__ import annotations

# import torch
# import torch.nn as nn

# from huggingface_hub import PyTorchModelHubMixin


# # SOURCE: https://huggingface.co/docs/hub/en/models-uploading#upload-a-pytorch-model-using-huggingfacehub
# def upload_to_huggingface(
#     model: nn.Module, model_name: str, model_description: str, model_tags: list[str], model_dir: str
# ) -> None:
#     """
#     Summary:
#     Upload a model to the Hugging Face model hub.

#     Args:
#     model (nn.Module): The model to upload.
#     model_name (str): The name of the model.
#     model_description (str): A description of the model.
#     model_tags (list[str]): A list of tags for the model.
#     model_dir (str): The directory in which to save the model.
#     """
#     model.save_pretrained(model_dir)
#     hub_model = PyTorchModelHubMixin.from_pretrained(model_dir)
#     hub_model.push_to_hub(model_name, model_description, model_tags)
#     print(f"Model uploaded to Hugging Face model hub as {model_name}.")

#     # reload
#     model = MyModel.from_pretrained("username/my-awesome-model")

</document_content>
</document>
<document index="44">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/imgops.py</source>
<document_content>
# pylint: disable=no-member
# pylint: disable=possibly-used-before-assignment
# pyright: reportImportCycles=false
# pyright: reportAttributeAccessIssue=false
# pyright: reportInvalidTypeForm=false
# mypy: disable-error-code="index"
# mypy: disable-error-code="no-redef"
# pylint: disable=possibly-used-before-assignment
# pylint: disable=consider-using-from-import
# https://github.com/universityofprofessorex/ESRGAN-Bot

# Creative Commons may be contacted at creativecommons.org.
# NOTE: For more examples tqdm + aiofile, search https://github.com/search?l=Python&q=aiofile+tqdm&type=Code
# pylint: disable=no-member

from __future__ import annotations

import asyncio
import base64
import gc
import io
import math
import pathlib
import re
import time
import typing

from enum import IntEnum
from io import BytesIO
from typing import Any, NewType, Optional

import cv2
import numpy as np
import pytz
import rich
import structlog
import torch
import torchvision.transforms as transforms
import torchvision.transforms.functional as FT


logger = structlog.get_logger(__name__)
from PIL import Image
from scipy.spatial import KDTree
from torchvision.transforms.functional import InterpolationMode
from torchvision.utils import make_grid
from tqdm.auto import tqdm
from webcolors import CSS3_HEX_TO_NAMES, hex_to_rgb

from democracy_exe.utils import file_functions


IMG_SIZE_CUTOFF = 1080

TYPE_IMAGE_ARRAY = typing.Union[np.ndarray, typing.Any]

TYPE_SCALE = typing.Union[str, int]


class Dimensions(IntEnum):
    HEIGHT = 224
    WIDTH = 224


ImageNdarrayBGR = NewType("ImageBGR", np.ndarray)
ImageNdarrayHWC = NewType("ImageHWC", np.ndarray)
TensorCHW = NewType("TensorCHW", torch.Tensor)

OPENCV_GREEN = (0, 255, 0)
OPENCV_RED = (255, 0, 0)


utc = pytz.utc


def looks_like_base64(sb):
    """Check if the string looks like base64"""
    return re.match("^[A-Za-z0-9+/]+[=]{0,2}$", sb) is not None


def is_image_data(b64data):
    """
    Check if the base64 data is an image by looking at the start of the data
    """
    image_signatures = {
        b"\xff\xd8\xff": "jpg",
        b"\x89\x50\x4e\x47\x0d\x0a\x1a\x0a": "png",
        b"\x47\x49\x46\x38": "gif",
        b"\x52\x49\x46\x46": "webp",
    }
    try:
        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes
        for sig, format in image_signatures.items():
            if header.startswith(sig):
                return True
        return False
    except Exception:
        return False


# SOURCE: https://github.com/langchain-ai/langchain/blob/master/cookbook/multi_modal_QA.ipynb
def encode_image(image_path: str):
    """Getting the base64 string"""

    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")


def split_image_text_types(docs):
    """Split numpy array images and texts"""
    images = []
    text = []
    for doc in docs:
        doc = doc.page_content  # Extract Document contents
        if is_base64(doc):
            # Resize image to avoid OAI server error
            images.append(resize_base64_image(doc, size=(250, 250)))  # base64 encoded str
        else:
            text.append(doc)
    return {"images": images, "texts": text}


def is_base64(s):
    """Check if a string is Base64 encoded"""
    try:
        return base64.b64encode(base64.b64decode(s)) == s.encode()
    except Exception:
        return False


def resize_base64_image(base64_string, size=(128, 128)):
    """
    Resize an image encoded as a Base64 string.

    :param base64_string: A Base64 encoded string of the image to be resized.
    :param size: A tuple representing the new size (width, height) for the image.
    :return: A Base64 encoded string of the resized image.
    """
    # Decode the Base64 string
    img_data = base64.b64decode(base64_string)
    img = Image.open(io.BytesIO(img_data))

    # Resize the image
    resized_img = img.resize(size, Image.Resampling.LANCZOS)

    # Save the resized image to a bytes buffer
    buffered = io.BytesIO()
    resized_img.save(buffered, format=img.format)

    return base64.b64encode(buffered.getvalue()).decode


def convert_to_base64(pil_image):
    """
    Convert PIL images to Base64 encoded strings

    :param pil_image: PIL image
    :return: Re-sized Base64 string
    """
    buffered = BytesIO()
    pil_image.save(buffered, format="JPEG")  # You can change the format if needed
    img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
    # img_str = resize_base64_image(img_str, size=(831,623))
    return img_str


def normalize_rectangle_coords(
    # images_filepath: str,
    # cols: int = 5,
    # model: Optional[torch.nn.Module] = None,
    # device: torch.device = DEVICE,
    # args: Optional[dict] = None,
    # resize: bool = False,
    predict_results=None,
):
    image, bboxes = predict_results
    temp = image.copy()
    img_as_array = np.asarray(temp)
    img_as_array = cv2.cvtColor(img_as_array, cv2.COLOR_RGB2BGR)

    # get fullsize bboxes
    xmin_fullsize, ymin_fullsize, xmax_fullsize, ymax_fullsize = bboxes[0]

    # if we have a negative point to make a rectange with, set it to 0
    startY = max(int(ymin_fullsize), 0)
    endY = max(int(ymax_fullsize), 0)
    startX = max(int(xmin_fullsize), 0)
    endX = max(int(xmax_fullsize), 0)

    rich.print(startY, endY, startX, endX)

    return [startY, endY, startX, endX]


def display_normalized_rectangle(image, out_bbox):
    out_xmin, out_ymin, out_xmax, out_ymax = out_bbox[0]

    out_pt1 = (int(out_xmin), int(out_ymin))
    out_pt2 = (int(out_xmax), int(out_ymax))

    return cv2.rectangle(
        image.squeeze().permute(1, 2, 0).cpu().numpy(),
        out_pt1,
        out_pt2,
        OPENCV_RED,
        2,
    )


def get_pil_image_channels(image_path: str) -> int:
    """
    Open an image and get the number of channels it has.

    This function loads an image using the Pillow library and converts it to a tensor.
    It then returns the number of channels in the image.

    Args:
    ----
        image_path (str): The path to the image file.

    Returns:
    -------
        int: The number of channels in the image.

    """
    # load pillow image
    pil_img = Image.open(
        image_path
    )  # 'pil_img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1179x2556 at 0x32AA189D0>,
    # import bpdb

    # bpdb.set_trace()

    # PILToTensor: Convert a PIL Image to a tensor of the same type - this does not scale values.
    # This transform does not support torchscript.
    # Converts a PIL Image (H x W x C) to a Tensor of shape (C x H x W).
    transform = transforms.Compose([transforms.PILToTensor()])

    ########################################################################
    # >>> type(transform)
    # <class 'torchvision.transforms.transforms.Compose'>
    #  dtype=torch.uint8
    #  shape = torch.Size([3, 2556, 1179])
    # Converts a PIL Image (H x W x C) to a Tensor of shape (C x H x W).
    ########################################################################
    img_tensor: torch.Tensor = transform(pil_img)

    # # Converts a PIL Image (H x W x C) to a Tensor of shape (C x H x W).
    # pil_img_tensor = transforms.PILToTensor()(pil_img)

    return img_tensor.shape[0]


def convert_pil_image_to_rgb_channels(image_path: str) -> Image:
    """
    Convert a PIL image to have the appropriate number of color channels.

    This function checks the number of channels in a PIL image and converts it to RGB if it has a different number of channels.

    Args:
    ----
        image_path (str): The path to the image file.

    Returns:
    -------
        Image: The converted PIL image with RGB channels.

    """
    return Image.open(image_path).convert("RGB") if get_pil_image_channels(image_path) != 4 else Image.open(image_path)


def read_image_to_bgr(image_path: str) -> tuple[np.ndarray, int, int, int]:
    """
    Read an image from the specified file path and convert it to BGR format.

    This function reads an image from the given file path using OpenCV, converts it from BGR to RGB format,
    and returns the image array along with its number of channels, height, and width.

    Args:
    ----
        image_path (str): The path to the image file.

    Returns:
    -------
        Tuple[np.ndarray, int, int, int]: A tuple containing the image array in BGR format,
                                          the number of channels, the height, and the width of the image.

    Raises:
    ------
        FileNotFoundError: If the image file does not exist at the specified path.

    Example:
    -------
        >>> image, channels, height, width = read_image_to_bgr("path/to/image.jpg")
        >>> print(image.shape)
        (height, width, channels)

    """
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    img_channel = image.shape[2]
    img_height = image.shape[0]
    img_width = image.shape[1]
    return image, img_channel, img_height, img_width


def convert_image_from_hwc_to_chw(img: ImageNdarrayBGR) -> torch.Tensor:
    """
    Convert an image from HWC (Height, Width, Channels) format to CHW (Channels, Height, Width) format.

    This function takes an image in HWC format and converts it to CHW format, which is commonly used in
    deep learning frameworks like PyTorch.

    Args:
    ----
        img (ImageNdarrayBGR): The input image in HWC format.

    Returns:
    -------
        torch.Tensor: The output image in CHW format.

    """
    img: torch.Tensor = torch.from_numpy(img).permute(2, 0, 1) / 255.0  # (h,w,c) -> (c,h,w)
    return img


# convert image back and forth if needed: https://stackoverflow.com/questions/68207510/how-to-use-torchvision-io-read-image-with-image-as-variable-not-stored-file
def convert_pil_image_to_torch_tensor(pil_image: Image.Image) -> torch.Tensor:
    """
    Convert a PIL image to a PyTorch tensor.

    This function takes a PIL image and converts it to a PyTorch tensor.
    The resulting tensor will have its channels in the order expected by PyTorch (C x H x W).

    Args:
    ----
        pil_image (Image.Image): The input image in PIL format.

    Returns:
    -------
        torch.Tensor: The converted image as a PyTorch tensor.

    """
    # NOTE: https://github.com/ultralytics/ultralytics/issues/9185
    return FT.to_tensor(pil_image).half()


# convert image back and forth if needed: https://stackoverflow.com/questions/68207510/how-to-use-torchvision-io-read-image-with-image-as-variable-not-stored-file
def convert_tensor_to_pil_image(tensor_image: torch.Tensor) -> Image.Image:
    """
    Convert a PyTorch tensor to a PIL image.

    This function takes a PyTorch tensor and converts it to a PIL image.
    The input tensor is expected to have its channels in the order expected by PyTorch (C x H x W).

    Args:
    ----
        tensor_image (torch.Tensor): The input image as a PyTorch tensor.

    Returns:
    -------
        Image.Image: The converted image in PIL format.

    """
    return FT.to_pil_image(tensor_image)


def get_pixel_rgb(image_pil: Image) -> str:
    """
    Get the color of the first pixel in an image.

    This function retrieves the RGB values of the first pixel (at position (1, 1)) in the provided PIL image.
    It then determines if the color is white or dark mode based on the RGB values.

    Args:
    ----
        image_pil (Image): The input image in PIL format.

    Returns:
    -------
        str: A string indicating the color of the first pixel, either "white" or "darkmode".

    """
    r, g, b = image_pil.getpixel((1, 1))  # pyright: ignore[reportAttributeAccessIssue]

    color = "white" if (r, g, b) == (255, 255, 255) else "darkmode"
    print(f"GOT COLOR {color} -- {r},{g},{b}")
    return color


def resize_and_pillarbox(image_pil: Image.Image, width: int, height: int, background: bool = True) -> Image.Image:
    """
    Resize a PIL image while maintaining its aspect ratio and adding pillarbox.

    This function resizes a PIL image to fit within the specified width and height while maintaining
    the original aspect ratio. It adds pillarbox (padding) to the image to fill the remaining space
    with a specified background color.

    Args:
    ----
        image_pil (Image.Image): The input image in PIL format.
        width (int): The target width for the resized image.
        height (int): The target height for the resized image.
        background (str, optional): The background color for the pillarbox. Defaults to "white".

    Returns:
    -------
        Image.Image: The resized image with pillarbox added.

    Example:
    -------
        >>> image = Image.open("path/to/image.jpg")
        >>> resized_image = resize_and_pillarbox(image, 1080, 1350, background="white")
        >>> resized_image.show()

    """
    autodetect_background = get_pixel_rgb(image_pil)

    ratio_w = width / image_pil.width  # pyright: ignore[reportAttributeAccessIssue]
    ratio_h = height / image_pil.height  # pyright: ignore[reportAttributeAccessIssue]
    if ratio_w < ratio_h:
        # It must be fixed by width
        resize_width = width
        resize_height = round(ratio_w * image_pil.height)  # pyright: ignore[reportAttributeAccessIssue]
    else:
        # Fixed by height
        resize_width = round(ratio_h * image_pil.width)  # pyright: ignore[reportAttributeAccessIssue]
        resize_height = height
    image_resize = image_pil.resize((resize_width, resize_height), Image.Resampling.LANCZOS)  # pyright: ignore[reportAttributeAccessIssue]
    if background and autodetect_background == "white":
        background = Image.new("RGBA", (width, height), (255, 255, 255, 255))
    elif background and autodetect_background == "darkmode":
        background = Image.new("RGBA", (width, height), (22, 32, 42, 1))
    offset = (round((width - resize_width) / 2), round((height - resize_height) / 2))
    background.paste(image_resize, offset)  # pyright: ignore[reportAttributeAccessIssue]
    return background.convert("RGB")  # pyright: ignore[reportAttributeAccessIssue]


def convert_rgb_to_names(rgb_tuple: tuple[int, int, int]) -> str:
    # a dictionary of all the hex and their respective names in css3
    css3_db = CSS3_HEX_TO_NAMES
    names = []
    rgb_values = []
    for color_hex, color_name in css3_db.items():
        names.append(color_name)
        rgb_values.append(hex_to_rgb(color_hex))

    kdt_db = KDTree(rgb_values)
    distance, index = kdt_db.query(rgb_tuple)
    rich.print(f"closest match: {names[index]}")
    return f"{names[index]}"


def get_all_corners_color(urls: list[str]) -> dict[str, str]:
    """
    Get the colors of the four corners of images.

    This function opens each image from the provided URLs, converts them to RGB,
    and retrieves the colors of the top-left, top-right, bottom-left, and bottom-right corners.

    Args:
    ----
        urls (List[str]): A list of URLs pointing to the image files.

    Returns:
    -------
        Dict[str, str]: A dictionary containing the colors of the four corners of the images.

    """
    pbar = tqdm(urls)
    pixel_data = {
        "top_left": "",
        "top_right": "",
        "bottom_left": "",
        "bottom_right": "",
    }
    for url in pbar:
        img = Image.open(url).convert("RGB")
        width, height = img.size
        pixel_layout = img.load()
        pixel_data["top_left"] = top_left = pixel_layout[0, 0]
        pixel_data["top_right"] = top_right = pixel_layout[width - 1, 0]
        pixel_data["bottom_left"] = bottom_left = pixel_layout[0, height - 1]
        pixel_data["bottom_right"] = bottom_right = pixel_layout[width - 1, height - 1]
        rich.print(pixel_data)
    return pixel_data


def rgb2hex(r: int, g: int, b: int) -> str:
    """
    Convert RGB values to a hexadecimal color string.

    This function takes the red, green, and blue components of a color
    and converts them to a hexadecimal string representation.

    Args:
    ----
        r (int): The red component of the color, in the range [0, 255].
        g (int): The green component of the color, in the range [0, 255].
        b (int): The blue component of the color, in the range [0, 255].

    Returns:
    -------
        str: The hexadecimal string representation of the color, prefixed with '#'.

    Example:
    -------
        >>> rgb2hex(255, 0, 0)
        '#ff0000'

    """
    logger.info(f"RGB2HEX: {r} {g} {b}")
    return f"#{r:02x}{g:02x}{b:02x}"
    # logger.info(f"TYPE RGB2HEX: {type(r)} {type(g)} {type(b)}")
    # return "#{:02x}{:02x}{:02x}".format(r, g, b)


def handle_get_dominant_color(urls: list[str], return_type: str = "name") -> str:
    """
    Get the dominant color from the corners of images.

    This function retrieves the colors of the four corners of images from the provided URLs.
    It then determines the dominant color based on the corner colors and returns it either as a color name or hex value.

    Args:
    ----
        urls (List[str]): A list of URLs pointing to the image files.
        return_type (str, optional): The format to return the dominant color.
            Can be "name" for color name or "hex" for hex value. Defaults to "name".

    Returns:
    -------
        str: The dominant color in the specified format (name or hex).

    """
    start_time = time.time()
    corner_pixels = get_all_corners_color(urls)

    if (
        corner_pixels["top_left"]
        == corner_pixels["top_right"]
        == corner_pixels["bottom_left"]
        == corner_pixels["bottom_right"]
    ):
        r, g, b = corner_pixels["top_left"]
    else:
        r, g, b = corner_pixels["top_right"]
    background_color = rgb2hex(r, g, b)
    rich.print(background_color)
    color_name = convert_rgb_to_names((r, g, b))
    rich.print(color_name)

    duration = time.time() - start_time
    print(f"Calculated 1 image in {duration} seconds")
    if return_type == "name":
        return color_name
    elif return_type == "hex":
        return background_color


def bgr_to_rgb(image: torch.Tensor) -> torch.Tensor:
    """
    Convert a BGR image to RGB.

    This function takes an image tensor in BGR format and converts it to RGB format by flipping the color channels.

    Args:
    ----
        image (torch.Tensor): The input image tensor in BGR format.

    Returns:
    -------
        torch.Tensor: The output image tensor in RGB format.

    """
    out: torch.Tensor = image.flip(-3)
    return out


def rgb_to_bgr(image: torch.Tensor) -> torch.Tensor:
    # same operation as bgr_to_rgb(), flip image channels
    return bgr_to_rgb(image)


def bgra_to_rgba(image: torch.Tensor) -> torch.Tensor:
    """
    Convert a BGRA image to RGBA.

    This function takes an image tensor in BGRA format and converts it to RGBA format
    by rearranging the color channels.

    Args:
    ----
        image (torch.Tensor): The input image tensor in BGRA format.

    Returns:
    -------
        torch.Tensor: The output image tensor in RGBA format.

    """
    out: torch.Tensor = image[[2, 1, 0, 3], :, :]
    return out


def rgba_to_bgra(image: torch.Tensor) -> torch.Tensor:
    """
    Convert a RGBA image to BGRA format.

    This function takes an image tensor in RGBA format and converts it to BGRA format
    by rearranging the color channels.

    Args:
    ----
        image (torch.Tensor): The input image tensor in RGBA format.

    Returns:
    -------
        torch.Tensor: The output image tensor in BGRA format.

    """
    # same operation as bgra_to_rgba(), flip image channels
    return bgra_to_rgba(image)


def denorm(x: torch.Tensor | np.ndarray, min_max: tuple[float, float] = (-1.0, 1.0)) -> torch.Tensor | np.ndarray:
    """
    Denormalize a tensor or numpy array from a specified range to [0, 1].

    This function converts values from a given range (default is [-1, 1]) to the range [0, 1].
    It is useful for reversing normalization applied during preprocessing.

    Args:
    ----
        x (torch.Tensor | np.ndarray): The input tensor or numpy array to be denormalized.
        min_max (tuple[float, float], optional): The range of the input values. Defaults to (-1.0, 1.0).

    Returns:
    -------
        torch.Tensor | np.ndarray: The denormalized tensor or numpy array.

    Raises:
    ------
        TypeError: If the input is not a torch.Tensor or np.ndarray.

    """
    out = (x - min_max[0]) / (min_max[1] - min_max[0])
    if isinstance(x, torch.Tensor):
        return out.clamp(0, 1)
    elif isinstance(x, np.ndarray):
        return np.clip(out, 0, 1)
    else:
        raise TypeError(
            "Got unexpected object type, expected torch.Tensor or \
        np.ndarray"
        )


def norm(x: torch.Tensor | np.ndarray) -> torch.Tensor | np.ndarray:
    """
    Normalize a tensor or numpy array from [0, 1] range to [-1, 1] range.

    This function converts values from the range [0, 1] to the range [-1, 1].
    It is useful for normalizing data before feeding it into a neural network.

    Args:
    ----
        x (torch.Tensor | np.ndarray): The input tensor or numpy array to be normalized.

    Returns:
    -------
        torch.Tensor | np.ndarray: The normalized tensor or numpy array.

    Raises:
    ------
        TypeError: If the input is not a torch.Tensor or np.ndarray.

    """
    out = (x - 0.5) * 2.0
    if isinstance(x, torch.Tensor):
        return out.clamp(-1, 1)
    elif isinstance(x, np.ndarray):
        return np.clip(out, -1, 1)
    else:
        raise TypeError(
            "Got unexpected object type, expected torch.Tensor or \
        np.ndarray"
        )


async def np2tensor(
    img: np.ndarray,
    bgr2rgb: bool = True,
    data_range: float = 1.0,
    normalize: bool = False,
    change_range: bool = True,
    add_batch: bool = True,
) -> torch.Tensor:
    """
    Convert a numpy image array into a PyTorch tensor.

    This function converts a numpy image array into a PyTorch tensor. It supports
    various options such as converting BGR to RGB, normalizing the image, changing
    the data range, and adding a batch dimension.

    Args:
    ----
        img (np.ndarray): The input image as a numpy array.
        bgr2rgb (bool, optional): Whether to convert BGR to RGB. Defaults to True.
        data_range (float, optional): The data range for the image. Defaults to 1.0.
        normalize (bool, optional): Whether to normalize the image. Defaults to False.
        change_range (bool, optional): Whether to change the data range. Defaults to True.
        add_batch (bool, optional): Whether to add a batch dimension. Defaults to True.

    Returns:
    -------
        torch.Tensor: The converted image as a PyTorch tensor.

    Raises:
    ------
        TypeError: If the input is not a numpy array.

    """
    if not isinstance(img, np.ndarray):  # images expected to be uint8 -> 255
        raise TypeError("Got unexpected object type, expected np.ndarray")
    # check how many channels the image has, then condition, like in my BasicSR. ie. RGB, RGBA, Gray
    # if bgr2rgb:
    # img = img[:, :, [2, 1, 0]] #BGR to RGB -> in numpy, if using OpenCV, else not needed. Only if image has colors.
    if change_range:
        if np.issubdtype(img.dtype, np.integer):
            info = np.iinfo
        elif np.issubdtype(img.dtype, np.floating):
            info = np.finfo
        img = img * data_range / info(img.dtype).max  # uint8 = /255
    img = torch.from_numpy(
        np.ascontiguousarray(np.transpose(img, (2, 0, 1)))
    ).float()  # "HWC to CHW" and "numpy to tensor"
    if bgr2rgb:
        if img.shape[0] == 3:  # RGB
            # BGR to RGB -> in tensor, if using OpenCV, else not needed. Only if image has colors.
            img = bgr_to_rgb(img)
        elif img.shape[0] == 4:  # RGBA
            # BGR to RGB -> in tensor, if using OpenCV, else not needed. Only if image has colors.)
            img = bgra_to_rgba(img)
    if add_batch:
        # Add fake batch dimension = 1 . squeeze() will remove the dimensions of size 1
        img.unsqueeze_(0)
    if normalize:
        img = norm(img)
    return img


# 2np


async def tensor2np(
    img: torch.Tensor,
    rgb2bgr: bool = True,
    remove_batch: bool = True,
    data_range: int = 255,
    denormalize: bool = False,
    change_range: bool = True,
    imtype: type = np.uint8,
) -> np.ndarray:
    """
    Convert a Tensor array into a numpy image array.

    This function converts a PyTorch tensor into a numpy image array. It supports
    various options such as converting RGB to BGR, removing the batch dimension,
    changing the data range, and denormalizing the image.

    Args:
    ----
        img (torch.Tensor): The input image tensor array. It can be 4D (B, (3/1), H, W),
            3D (C, H, W), or 2D (H, W), with any range and RGB channel order.
        rgb2bgr (bool, optional): Whether to convert RGB to BGR. Defaults to True.
        remove_batch (bool, optional): Whether to remove the batch dimension if the tensor
            is of shape BCHW. Defaults to True.
        data_range (int, optional): The data range for the image. Defaults to 255.
        denormalize (bool, optional): Whether to denormalize the image from [-1, 1] range
            back to [0, 1]. Defaults to False.
        change_range (bool, optional): Whether to change the data range. Defaults to True.
        imtype (type, optional): The desired type of the converted numpy array. Defaults to np.uint8.

    Returns:
    -------
        np.ndarray: The converted image as a numpy array. It will be 3D (H, W, C) or 2D (H, W),
            with values in the range [0, 255] and of type np.uint8 (default).

    Raises:
    ------
        TypeError: If the input is not a torch.Tensor.

    Example:
    -------
        >>> tensor_image = torch.randn(1, 3, 256, 256)
        >>> numpy_image = await tensor2np(tensor_image)
        >>> print(numpy_image.shape)
        (256, 256, 3)

    """
    if not isinstance(img, torch.Tensor):
        raise TypeError("Got unexpected object type, expected torch.Tensor")
    n_dim = img.dim()

    # TODO: Check: could denormalize here in tensor form instead, but end result is the same

    img = img.float().cpu()

    if n_dim in [4, 3]:
        # if n_dim == 4, has to convert to 3 dimensions, either removing batch or by creating a grid
        if n_dim == 4 and remove_batch:
            if img.shape[0] > 1:
                # leave only the first image in the batch
                img = img[0, ...]
            else:
                # remove a fake batch dimension
                img = img.squeeze()
                # squeeze removes batch and channel of grayscale images (dimensions = 1)
                if len(img.shape) < 3:
                    # add back the lost channel dimension
                    img = img.unsqueeze(dim=0)
        # convert images in batch (BCHW) to a grid of all images (C B*H B*W)
        else:
            n_img = len(img)
            img = make_grid(img, nrow=int(math.sqrt(n_img)), normalize=False)

        if img.shape[0] == 3 and rgb2bgr:  # RGB
            # RGB to BGR -> in tensor, if using OpenCV, else not needed. Only if image has colors.
            img_np = rgb_to_bgr(img).numpy()
        elif img.shape[0] == 4 and rgb2bgr:  # RGBA
            # RGBA to BGRA -> in tensor, if using OpenCV, else not needed. Only if image has colors.
            img_np = rgba_to_bgra(img).numpy()
        else:
            img_np = img.numpy()
        img_np = np.transpose(img_np, (1, 2, 0))  # "CHW to HWC" -> # HWC, BGR
    elif n_dim == 2:
        img_np = img.numpy()
    else:
        raise TypeError(f"Only support 4D, 3D and 2D tensor. But received with dimension: {n_dim:d}")

    # if rgb2bgr:
    # img_np = img_np[[2, 1, 0], :, :] #RGB to BGR -> in numpy, if using OpenCV, else not needed. Only if image has colors.
    # TODO: Check: could denormalize in the begining in tensor form instead
    if denormalize:
        img_np = denorm(img_np)  # denormalize if needed
    if change_range:
        # clip to the data_range
        img_np = np.clip(data_range * img_np, 0, data_range).round()
        # Important. Unlike matlab, numpy.unit8() WILL NOT round by default.
    # has to be in range (0,255) before changing to np.uint8, else np.float32
    return img_np.astype(imtype)


def auto_split_upscale(
    lr_img: np.ndarray,
    upscale_function: typing.Callable[[np.ndarray], np.ndarray],
    scale: int = 4,
    overlap: int = 32,
    max_depth: int | None = None,
    current_depth: int = 1,
) -> tuple[np.ndarray, int]:
    """
    Recursively upscale an image by splitting it into smaller sections.

    This function attempts to upscale an image using the provided `upscale_function`.
    If the upscaling process runs out of memory, the image is split into four quadrants,
    and the upscaling is attempted on each quadrant recursively.

    Args:
    ----
        lr_img (np.ndarray): The low-resolution input image.
        upscale_function (typing.Callable[[np.ndarray], np.ndarray]): The function to upscale the image.
        scale (int, optional): The scaling factor. Defaults to 4.
        overlap (int, optional): The overlap between image sections to avoid seams. Defaults to 32.
        max_depth (Optional[int], optional): The maximum recursion depth. If None, recursion continues until out of memory. Defaults to None.
        current_depth (int, optional): The current recursion depth. Defaults to 1.

    Returns:
    -------
        typing.Tuple[np.ndarray, int]: The upscaled image and the depth of recursion used.

    """
    if current_depth > 1 and (lr_img.shape[0] == lr_img.shape[1] == overlap):
        raise RecursionError("Reached bottom of recursion depth.")

    # Attempt to upscale if unknown depth or if reached known max depth
    if max_depth is None or max_depth == current_depth:
        try:
            result = upscale_function(lr_img)
            return result, current_depth
        except RuntimeError as e:
            if "allocate" not in str(e):
                raise RuntimeError(e) from e

            # Collect garbage (clear VRAM)
            torch.cuda.empty_cache()
            gc.collect()
    h, w, c = lr_img.shape

    # Split image into 4ths
    top_left = lr_img[: h // 2 + overlap, : w // 2 + overlap, :]
    top_right = lr_img[: h // 2 + overlap, w // 2 - overlap :, :]
    bottom_left = lr_img[h // 2 - overlap :, : w // 2 + overlap, :]
    bottom_right = lr_img[h // 2 - overlap :, w // 2 - overlap :, :]

    # Recursively upscale the quadrants
    # After we go through the top left quadrant, we know the maximum depth and no longer need to test for out-of-memory
    top_left_rlt, depth = auto_split_upscale(
        top_left,
        upscale_function,
        scale=scale,
        overlap=overlap,
        current_depth=current_depth + 1,
    )
    top_right_rlt, _ = auto_split_upscale(
        top_right,
        upscale_function,
        scale=scale,
        overlap=overlap,
        max_depth=depth,
        current_depth=current_depth + 1,
    )
    bottom_left_rlt, _ = auto_split_upscale(
        bottom_left,
        upscale_function,
        scale=scale,
        overlap=overlap,
        max_depth=depth,
        current_depth=current_depth + 1,
    )
    bottom_right_rlt, _ = auto_split_upscale(
        bottom_right,
        upscale_function,
        scale=scale,
        overlap=overlap,
        max_depth=depth,
        current_depth=current_depth + 1,
    )

    # Define output shape
    out_h = h * scale
    out_w = w * scale

    # Create blank output image
    output_img = np.zeros((out_h, out_w, c), np.uint8)

    # Fill output image with tiles, cropping out the overlaps
    output_img[: out_h // 2, : out_w // 2, :] = top_left_rlt[: out_h // 2, : out_w // 2, :]
    output_img[: out_h // 2, -out_w // 2 :, :] = top_right_rlt[: out_h // 2, -out_w // 2 :, :]
    output_img[-out_h // 2 :, : out_w // 2, :] = bottom_left_rlt[-out_h // 2 :, : out_w // 2, :]
    output_img[-out_h // 2 :, -out_w // 2 :, :] = bottom_right_rlt[-out_h // 2 :, -out_w // 2 :, :]

    return output_img, depth


async def aio_main():
    # Load the test image
    image_path = "tests/fixtures/screenshot_image_larger00013.PNG"
    img = Image.open(image_path)
    test_image = np.array(img)

    # Test with default parameters
    tensor_image = await np2tensor(test_image)
    # import bpdb

    # bpdb.set_trace()
    assert isinstance(tensor_image, torch.Tensor)
    assert tensor_image.shape == (
        1,
        test_image.shape[2],
        test_image.shape[0],
        test_image.shape[1],
    )
    # await logger.complete()


def main():
    asyncio.run(aio_main())

</document_content>
</document>
<document index="45">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/imitools.py</source>
<document_content>
# pylint: disable=no-member
# pylint: disable=possibly-used-before-assignment
# pyright: reportImportCycles=false
# pyright: reportAttributeAccessIssue=false
# pyright: reportInvalidTypeForm=false
# mypy: disable-error-code="index"
# mypy: disable-error-code="no-redef"
# pylint: disable=consider-using-with, consider-using-min-builtin
# SOURCE: https://github.com/GDi4K/imitools/blob/main/imitools.py
from __future__ import annotations

import io
import math
import os
import tempfile

from collections.abc import Callable
from concurrent.futures import ThreadPoolExecutor, as_completed
from os import PathLike
from pathlib import Path
from typing import Any, Union

import requests
import torch

from matplotlib import pyplot as plt
from PIL import Image, UnidentifiedImageError
from torchvision.transforms import transforms


_last_search_wrapper = None
N_WORKERS = min(10, os.cpu_count())


class ImageDefaults:
    """
    A class to hold default settings for image processing.

    Attributes
    ----------
        device (str): The device to be used for image processing, default is "cpu".

    """

    def __init__(self) -> None:
        """Initialize the ImageDefaults class with default settings."""
        self.device = "cpu"


defaults = ImageDefaults()


def download_image(img_url: str) -> Image.Image | None:
    """
    Download an image from a given URL.

    This function downloads an image from the specified URL and returns it as a PIL Image object.
    If the download fails or the image cannot be opened, it returns None.

    Args:
    ----
        img_url (str): The URL of the image to be downloaded.

    Returns:
    -------
        Image.Image | None: The downloaded image as a PIL Image object, or None if the download fails.

    """
    image = None
    try:
        buffer = tempfile.SpooledTemporaryFile(max_size=1e9)  # type: ignore
        r = requests.get(img_url, stream=True)
        if r.status_code == 200:
            for chunk in r.iter_content(chunk_size=1024):
                buffer.write(chunk)
            buffer.seek(0)
            image = Image.open(io.BytesIO(buffer.read()))
        buffer.close()
        return image
    except Exception:
        return image


# based on https://gist.github.com/sigilioso/2957026
def image_crop(img: Image.Image, size: tuple[int, int], crop_type: str = "middle") -> Image.Image:
    """
    Crop an image to the specified size.

    This function resizes and crops an image to the desired size. The cropping can be done
    from the top, middle, or bottom of the image based on the specified crop type.

    Args:
    ----
        img (Image.Image): The input image to be cropped.
        size (tuple[int, int]): The desired size (width, height) of the cropped image.
        crop_type (str, optional): The type of cropping to perform. Can be 'top', 'middle', or 'bottom'. Defaults to 'middle'.

    Returns:
    -------
        Image.Image: The cropped image.

    Raises:
    ------
        ValueError: If an invalid value is provided for crop_type.

    """
    # Get current and desired ratio for the images
    img_ratio = img.size[0] / float(img.size[1])
    ratio = size[0] / float(size[1])
    # The image is scaled/cropped vertically or horizontally depending on the ratio
    if ratio > img_ratio:
        img = img.resize(
            (size[0], round(size[0] * img.size[1] / img.size[0])),
            Image.Resampling.BILINEAR,
        )
        # Crop in the top, middle or bottom
        if crop_type == "top":
            box = (0, 0, img.size[0], size[1])
        elif crop_type == "middle":
            box = (
                0,
                round((img.size[1] - size[1]) / 2),
                img.size[0],
                round((img.size[1] + size[1]) / 2),
            )
        elif crop_type == "bottom":
            box = (0, img.size[1] - size[1], img.size[0], img.size[1])
        else:
            """
            Display images in multiple rows.

            If there are multiple rows of images, they are displayed in a grid with optional captions.
            """
            raise ValueError("ERROR: invalid value for crop_type")
        img = img.crop(box)
    elif ratio < img_ratio:
        img = img.resize(
            (round(size[1] * img.size[0] / img.size[1]), size[1]),
            Image.Resampling.BILINEAR,
        )
        # Crop in the top, middle or bottom
        if crop_type == "top":
            box = (0, 0, size[0], img.size[1])
        elif crop_type == "middle":
            box = (
                round((img.size[0] - size[0]) / 2),
                0,
                round((img.size[0] + size[0]) / 2),
                img.size[1],
            )
        elif crop_type == "bottom":
            box = (img.size[0] - size[0], 0, img.size[0], img.size[1])
        else:
            raise ValueError("ERROR: invalid value for crop_type")
        img = img.crop(box)
    else:
        img = img.resize((size[0], size[1]), Image.Resampling.BILINEAR)

    return img


def thread_loop(fn: Callable[[Any], Any], input_array: list[Any], n_workers: int = N_WORKERS) -> list[Any]:
    return_data = []

    with ThreadPoolExecutor(n_workers) as executor:
        futures = [executor.submit(fn, input_item) for input_item in input_array]

        for future in as_completed(futures):
            result = future.result()
            return_data.append(result)

    return return_data


class VideoWrapper:
    """
    A class to wrap video file information.

    Attributes
    ----------
        video_path (str): The path to the video file.
        video_size (tuple[int, int]): The size of the video (width, height).

    """

    def __init__(self, video_path: str, video_size: tuple[int, int]) -> None:
        """
        Initialize the VideoWrapper class.

        Args:
        ----
            video_path (str): The path to the video file.
            video_size (tuple[int, int]): The size of the video (width, height).

        """
        self.video_path = video_path
        self.video_size = video_size

    def path(self) -> str:
        """
        Get the path to the video file.

        Returns
        -------
            str: The path to the video file.

        """
        return self.video_path

    # def show(self) -> HTML:
    #     mp4 = open(self.video_path, "rb").read()
    #     data_url = "data:video/mp4;base64," + b64encode(mp4).decode()

    #     width, height = self.video_size

    #     return HTML(
    #         f"""
    #     <video width={width} height={height} controls>
    #           <source src="%s" type="video/mp4">
    #     </video>
    #     """
    #         % data_url
    #     )


class ImageWrapper:
    def __init__(
        self,
        data: list[Image.Image] | torch.Tensor,
        image_type: str,
        labels: list[int] | None = None,
    ) -> None:
        """
        Initialize the ImageWrapper class.

        Args:
        ----
            data (list[Image.Image] | torch.Tensor): The image data.
            image_type (str): The type of the image data ('pil' or 'pt').
            labels (list[int] | None, optional): The labels for the images. Defaults to None.

        """
        self.data: list[Image.Image] | torch.Tensor = data
        self.image_type: str = image_type
        self.labels: list[int] = list(range(len(data))) if labels is None else labels

    def resize(self, size: tuple[int, int] = (256, 256), **kwargs: Any) -> ImageWrapper:
        """
        Resize the images to the specified size.

        Args:
        ----
            size (tuple[int, int], optional): The desired size (width, height) of the resized images. Defaults to (256, 256).
            **kwargs (Any): Additional keyword arguments to be passed to the resize method of the PIL Image class.

        Returns:
        -------
            ImageWrapper: A new ImageWrapper instance containing the resized images.

        """
        ref = self
        if self.image_type != "pil":
            ref = self.cpil()

        if not isinstance(size, tuple):
            size = (size, size)

        i_size = (int(size[0]), int(size[1]))

        new_images = [im.resize(i_size, **kwargs) for im in ref.data]
        return ImageWrapper(new_images, "pil")

    def crop(self, size: tuple[int, int] = (256, 256), crop_type: str = "middle") -> ImageWrapper:
        """
        Crop the images to the specified size.

        Args:
        ----
            size (tuple[int, int], optional): The desired size (width, height) of the cropped images. Defaults to (256, 256).
            crop_type (str, optional): The type of cropping to perform. Can be 'top', 'middle', or 'bottom'. Defaults to 'middle'.

        Returns:
        -------
            ImageWrapper: A new ImageWrapper instance containing the cropped images.

        """
        ref = self
        if ref.image_type != "pil":
            ref = ref.cpil()

        if not isinstance(size, tuple):
            size = (size, size)

        i_size = (int(size[0]), int(size[1]))

        new_images = [image_crop(im, size, crop_type) for im in ref.data]
        return ImageWrapper(new_images, "pil")

    def normalize(self) -> ImageWrapper:
        """
        Normalize the image data.

        Returns
        -------
            ImageWrapper: A new ImageWrapper instance containing the normalized image data.

        """
        ref: ImageWrapper = self
        if self.image_type != "pt":
            ref = self.cpt()

        normalized = (ref.data - ref.data.min()) / (ref.data.max() - ref.data.min())
        return ImageWrapper(normalized, "pt")

    def pick(self, *args: int | list[int]) -> ImageWrapper:  # type: ignore
        """
        Select specific images from the ImageWrapper instance.

        Args:
        ----
            *args (int | list[int]): The indexes of the images to be picked. Can be individual integers or a list of integers.

        Returns:
        -------
            ImageWrapper: A new ImageWrapper instance containing the selected images.

        Raises:
        ------
            Exception: If no indexes are provided.

        """
        if not args:
            raise Exception("provide some indexes to pick")

        indexes = list(args)
        if isinstance(args[0], list):
            indexes = args[0]

        if self.image_type == "pil":
            return ImageWrapper([self.data[i] for i in indexes], "pil")

        if self.image_type == "pt":
            return ImageWrapper(self.data[indexes], "pt")

    def sinrange(self: ImageWrapper) -> ImageWrapper:
        """
        Scale image data to the range [-1, 1].

        Returns
        -------
            ImageWrapper: A new ImageWrapper instance containing the scaled image data.

        """
        ref = self
        if self.image_type != "pt":
            ref = self.cpt()

        return ImageWrapper(ref.data * 2 - 1, "pt")

    def pil(self) -> Image.Image | list[Image.Image]:  # type: ignore
        """
        Convert the image data to PIL format.

        Returns
        -------
            Image.Image | list[Image.Image]: The image data in PIL format. If there is only one image, it returns a single PIL Image object. Otherwise, it returns a list of PIL Image objects.

        """
        if self.image_type == "pil":
            return self.data[0] if len(self.data) == 1 else self.data

        if self.image_type == "pt":
            make_pil = transforms.ToPILImage()
            pt_images = self.data.cpu()  # type: ignore
            pil_images = [make_pil(i) for i in pt_images]
            return pil_images[0] if len(pil_images) == 1 else pil_images

    def pt(self) -> torch.Tensor:
        """
        Convert the image data to PyTorch tensor format.

        Returns
        -------
            torch.Tensor: The image data in PyTorch tensor format.

        """
        if self.image_type == "pil":
            pt_images = [transforms.ToTensor()(im) for im in self.data]
            return torch.stack(pt_images).to(defaults.device)

        if self.image_type == "pt":
            return self.data

    def to(self, device: str = "cpu") -> ImageWrapper:
        """
        Move the image data to the specified device.

        Args:
        ----
            device (str, optional): The device to move the image data to. Defaults to "cpu".

        Returns:
        -------
            ImageWrapper: A new ImageWrapper instance with the image data moved to the specified device.

        Raises:
        ------
            Exception: If the image data is not in PyTorch tensor format.

        """
        if self.image_type != "pt":
            raise Exception("to() only applied for pytorch tensors")

        return ImageWrapper(self.data.to(device), "pt")  # type: ignore

    def cpil(self) -> ImageWrapper:
        """
        Convert the image data to PIL format.

        Returns
        -------
            ImageWrapper: A new ImageWrapper instance containing the image data in PIL format.

        """
        images: list[Image.Image] | Image.Image = self.pil()
        if isinstance(images, Image.Image):
            images = [images]

        return ImageWrapper(images, "pil")

    def cpt(self) -> ImageWrapper:
        """
        Convert the image data to PyTorch tensor format.

        Returns
        -------
            ImageWrapper: A new ImageWrapper instance containing the image data in PyTorch tensor format.

        """
        return ImageWrapper(self.pt(), "pt")

    def show(
        self,
        cmap: Any = None,
        figsize: tuple[int, int] | None = None,
        cols: int = 6,
        max_count: int = 36,
        scale: int = -1,
        captions: bool = True,
    ) -> None:
        """
        Display a grid of images.

        Args:
        ----
            cmap (Any, optional): The colormap to use for displaying the images. Defaults to None.
            figsize (tuple[int, int] | None, optional): The size of the figure. Defaults to None.
            cols (int, optional): The number of columns in the grid. Defaults to 6.
            max_count (int, optional): The maximum number of images to display. Defaults to 36.
            scale (int, optional): The scale of the images. Defaults to -1.
            captions (bool, optional): Whether to display captions for the images. Defaults to True.

        """
        if len(self.data) == 1:
            """
            Display a single image.

            If there is only one image in the ImageWrapper instance, it is displayed with the specified scale.
            """
            scale = 4 if scale == -1 else scale
            plt.figure(figsize=(scale, scale))
            plt.axis("off")
            if self.image_type == "pil":
                plt.imshow(self.data[0], cmap=cmap)
            else:
                plt.imshow(self.data[0].permute(1, 2, 0).cpu(), cmap=cmap)  # type: ignore

            return

        scale = 2.5 if scale == -1 else scale
        images = self.data.cpu() if self.image_type == "pt" else self.data  # type: ignore
        labels = self.labels
        image_count = len(self.data)

        if image_count > max_count:
            print(
                f"Only showing {max_count} images of the total {image_count}. Use the `max_count` parameter to change it."
            )
            images = self.data[:max_count]
            image_count = max_count

        cols = min(image_count, cols)

        rows = math.ceil(image_count / cols)

        if figsize is None:
            figsize = figsize = (cols * scale, rows * scale)

        _, ax = plt.subplots(rows, cols, figsize=figsize)
        if rows == 1:
            for i in range(image_count):
                image = images[i] if self.image_type == "pil" else images[i].permute(1, 2, 0)  # type: ignore
                ax[i].imshow(image, cmap=cmap)
                ax[i].axis("off")
                if captions:
                    ax[i].set_title(f"{labels[i]}")
        else:
            for row in range(rows):
                for col in range(cols):
                    i = row * cols + col
                    if i < image_count:
                        image = images[i] if self.image_type == "pil" else images[i].permute(1, 2, 0)  # type: ignore
                        ax[row][col].imshow(image, cmap=cmap)
                        ax[row][col].axis("off")
                        if captions:
                            ax[row][col].set_title(f"{labels[i]}")
                    else:
                        ax[row][col].axis("off")

    def to_dir(self, output_dir: str, prefix: str = "image", max_workers: int = N_WORKERS) -> None:
        """
        Save images to a specified directory.

        Args:
        ----
            output_dir (str): The path to the directory where the images will be saved.
            prefix (str, optional): The prefix for the saved image filenames. Defaults to "image".
            max_workers (int, optional): The maximum number of worker threads to use for saving images. Defaults to N_WORKERS.

        """
        ref = self
        if self.image_type != "pil":
            ref = self.cpil()

        dir_path = Path(output_dir)
        dir_path.mkdir(exist_ok=True, parents=True)

        images = ref.data

        def save_image(i: int) -> None:
            """
            Save an image to the specified directory.

            This function saves an image from the list of images to the specified
            directory with a given prefix and index. If an error occurs during
            the saving process, it prints an error message.

            Args:
            ----
                i (int): The index of the image in the list to be saved.

            Raises:
            ------
                Exception: If an error occurs during the image saving process.

            """
            try:
                path = Path(output_dir) / f"{prefix}_{i:04}.png"
                images[i].save(path)
            except Exception as e:
                print("image saving error:", e)

        thread_loop(save_image, range(len(images)))  # type: ignore

    def to_video(self, out_path: PathLike | str | None = None, frame_rate: int = 12) -> VideoWrapper:
        """
        Convert a sequence of images to a video.

        Args:
        ----
            out_path (PathLike | str | None, optional): The path to save the video file. If None, a temporary path is used. Defaults to None.
            frame_rate (int, optional): The frame rate of the video. Defaults to 12.

        Returns:
        -------
            VideoWrapper: An instance of the VideoWrapper class containing the video file path and size.

        """
        ref = self
        if self.image_type == "pt":
            ref = self.cpil()

        id = int(torch.rand(1)[0].item() * 9999999)
        image_dir = Path(f"/tmp/{id}/images")
        image_dir.mkdir(exist_ok=True, parents=True)

        if out_path is None:
            out_path = f"/tmp/{id}/video.mp4"

        video_path = Path(out_path)  # type: ignore
        video_size = ref.data[0].size
        images_selector = image_dir / "image_%04d.png"

        ref.to_dir(image_dir, prefix="image")  # type: ignore

        command = f"ffmpeg -v 0 -y -f image2 -framerate {frame_rate} -i {images_selector} -c:v h264_nvenc -preset slow -qp 18 -pix_fmt yuv420p {video_path}"
        os.system(command)

        return VideoWrapper(video_path, video_size)  # type: ignore


def wrap(
    input_data: (
        ImageWrapper |
        torch.Tensor |
        Image.Image |
        list[torch.Tensor | Image.Image | ImageWrapper]
    ),
    labels: list[int] | None = None,
) -> ImageWrapper:
    """
    Wrap various types of image data into an ImageWrapper instance.

    This function takes different types of image data, such as ImageWrapper instances,
    PyTorch tensors, PIL Images, or lists of these types, and wraps them into a single
    ImageWrapper instance.

    Args:
    ----
        input_data (Union[ImageWrapper, torch.Tensor, Image.Image, list[Union[torch.Tensor, Image.Image, ImageWrapper]]]):
            The image data to be wrapped.
        labels (list[int] | None, optional): The labels for the images. Defaults to None.

    Returns:
    -------
        ImageWrapper: An ImageWrapper instance containing the wrapped image data.

    Raises:
    ------
        Exception: If the input data type is not supported.

    """
    if isinstance(input_data, ImageWrapper):
        # If the input data is already an ImageWrapper, return it as is.
        return input_data

    if isinstance(input_data, torch.Tensor):
        # If the input data is a PyTorch tensor, adjust its dimensions and wrap it.
        if len(input_data.shape) == 2:
            input_data = input_data.unsqueeze(0).unsqueeze(0)

        if len(input_data.shape) == 3:
            input_data = input_data.unsqueeze(0)

        return ImageWrapper(input_data.detach().float(), "pt", labels)

    if isinstance(input_data, Image.Image):
        # If the input data is a single PIL Image, wrap it in a list and then wrap it.
        return ImageWrapper([input_data], "pil", labels)

    if isinstance(input_data, list):
        # If the input data is a list, determine the type of its elements and wrap accordingly.
        if isinstance(input_data[0], torch.Tensor):
            images = torch.stack(input_data).squeeze(1).detach().float()
            return ImageWrapper(images, "pt", labels)

        if isinstance(input_data[0], Image.Image):
            return ImageWrapper(input_data, "pil", labels)

        if isinstance(input_data[0], ImageWrapper):
            image_list = list(map(lambda w: w.pt(), input_data))  # type: ignore
            images = torch.stack(image_list).squeeze(1).detach().float()
            return ImageWrapper(images, "pt", labels)

    raise Exception("not implemented!")


def from_dir(dir_path: str) -> ImageWrapper:
    """
    Load images from a directory and return them as an ImageWrapper instance.

    This function iterates through the files in the specified directory,
    reads the images, and converts them to RGB format. The images are then
    wrapped in an ImageWrapper instance and returned.

    Args:
    ----
        dir_path (str): The path to the directory containing the images.

    Returns:
    -------
        ImageWrapper: An ImageWrapper instance containing the loaded images.

    Raises:
    ------
        UnidentifiedImageError: If a file in the directory is not a valid image.

    """
    file_list = [f for f in Path(dir_path).iterdir() if not f.is_dir()]
    image_list = []

    def read_image(f: Path) -> None:
        """
        Read an image from a file and convert it to RGB format.

        This function attempts to open an image file and convert it to RGB format.
        If the file is not a valid image, it is ignored.

        Args:
        ----
            f (Path): The path to the image file.

        """
        try:
            image_list.append(Image.open(f).convert("RGB"))
        except UnidentifiedImageError:
            None

    thread_loop(read_image, file_list)

    return ImageWrapper(image_list, "pil")


def from_path(input_data: str | Path) -> ImageWrapper:
    """
    Load an image from a file path and return it as an ImageWrapper instance.

    This function reads an image from the specified file path, converts it to RGB format,
    and wraps it in an ImageWrapper instance.

    Args:
    ----
        input_data (Union[str, Path]): The path to the image file.

    Returns:
    -------
        ImageWrapper: An ImageWrapper instance containing the loaded image.

    Raises:
    ------
        UnidentifiedImageError: If the file at the specified path is not a valid image.

    """
    pil_image = Image.open(input_data).convert("RGB")
    return ImageWrapper([pil_image], "pil")


def download(image_urls: str | list[str]) -> ImageWrapper:
    """
    Download images from the given URLs.

    This function takes a single URL or a list of URLs, downloads the images,
    and returns them wrapped in an ImageWrapper instance.

    Args:
    ----
        image_urls (Union[str, List[str]]): A single image URL or a list of image URLs.

    Returns:
    -------
        ImageWrapper: An ImageWrapper instance containing the downloaded images.

    Raises:
    ------
        Exception: If the image URLs are invalid or the images cannot be downloaded.

    """
    if isinstance(image_urls, str):
        """
        Convert a single URL to a list of URLs.

        If the input is a single URL, it is converted to a list containing that URL.
        """
        image_urls = [image_urls]

    result_list = thread_loop(download_image, image_urls)
    """
    Download images using multiple threads.

    The function uses a thread pool to download images concurrently.
    """
    images = []
    """
    Filter out None values from the downloaded images.

    The function iterates through the downloaded images and filters out any None values.
    """
    images.extend(image for image in result_list if image is not None)
    return wrap(images)


def search_history() -> ImageWrapper | None:
    """
    Retrieve the last searched images.

    This function returns the ImageWrapper instance containing the images
    from the last search performed using the `search_images` function.
    If no search has been performed yet, it returns None.

    Returns
    -------
        ImageWrapper | None: The ImageWrapper instance containing the last searched images,
                                or None if no search has been performed.

    """
    return _last_search_wrapper

</document_content>
</document>
<document index="46">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/introspect.py</source>
<document_content>
"""
democracy_exe/utils/introspect.py

Defines built in democracy_exe functions to aid in introspection
"""
# SOURCE: https://github.com/hugapi/hug/blob/e4a3fa40f98487a67351311d0da659a6c9ce88a6/hug/introspect.py#L33

# from __future__ import absolute_import
from __future__ import annotations

from types import MethodType


def is_method(function):
    """Returns True if the passed in function is identified as a method (NOT a function)"""
    return isinstance(function, MethodType)


def is_coroutine(function):
    """Returns True if the passed in function is a coroutine"""
    return function.__code__.co_flags & 0x0080 or getattr(function, "_is_coroutine", False)


def name(function):
    """Returns the name of a function"""
    return function.__name__


def arguments(function, extra_arguments=0):
    """Returns the name of all arguments a function takes"""
    if not hasattr(function, "__code__"):
        return ()

    return function.__code__.co_varnames[: function.__code__.co_argcount + extra_arguments]


def takes_kwargs(function):
    """Returns True if the supplied function takes keyword arguments"""
    return bool(function.__code__.co_flags & 0x08)


def takes_args(function):
    """Returns True if the supplied functions takes extra non-keyword arguments"""
    return bool(function.__code__.co_flags & 0x04)


def takes_arguments(function, *named_arguments):
    """Returns the arguments that a function takes from a list of requested arguments"""
    return set(named_arguments).intersection(arguments(function))


def takes_all_arguments(function, *named_arguments):
    """Returns True if all supplied arguments are found in the function"""
    return takes_arguments(function, *named_arguments) == set(named_arguments)


def generate_accepted_kwargs(function, *named_arguments):
    """
    Dynamically creates a function that when called with dictionary of arguments will produce a kwarg that's
    compatible with the supplied function
    """
    if hasattr(function, "__code__") and takes_kwargs(function):
        function_takes_kwargs = True
        function_takes_arguments = []
    else:
        function_takes_kwargs = False
        function_takes_arguments = takes_arguments(function, *named_arguments)

    def accepted_kwargs(kwargs):
        if function_takes_kwargs:
            return kwargs
        elif function_takes_arguments:
            return {key: value for key, value in kwargs.items() if key in function_takes_arguments}
        return {}

    return accepted_kwargs

</document_content>
</document>
<document index="47">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/io.py</source>
<document_content>
# pylint: disable=possibly-used-before-assignment
"""io.py"""

from __future__ import annotations

import csv
import os
import re

from glob import glob
from pathlib import Path
from typing import Optional, Union

import numpy as np

from dask import array as da
from dask import delayed

from democracy_exe.utils.misc import abspath_or_url


# SOURCE: https://github.com/napari/napari/blob/5f96d5d814aad697c367bdadbb1a57750e2114ad/napari/utils/io.py
def imread(filename: str) -> np.ndarray:
    """
    Custom implementation of imread to avoid skimage dependency.

    Parameters
    ----------
    filename : string
        The path from which to read the image.

    Returns
    -------
    data : np.ndarray
        The image data.

    """
    filename = abspath_or_url(filename)

    import imageio

    return imageio.imread(filename)


def _alphanumeric_key(s):
    """
    Convert string to list of strings and ints that gives intuitive sorting.

    Parameters
    ----------
    s : string

    Returns
    -------
    k : a list of strings and ints

    Examples
    --------
    >>> _alphanumeric_key("z23a")
    ['z', 23, 'a']
    >>> filenames = ["f9.10.png", "e10.png", "f9.9.png", "f10.10.png", "f10.9.png"]
    >>> sorted(filenames)
    ['e10.png', 'f10.10.png', 'f10.9.png', 'f9.10.png', 'f9.9.png']
    >>> sorted(filenames, key=_alphanumeric_key)
    ['e10.png', 'f9.9.png', 'f9.10.png', 'f10.9.png', 'f10.10.png']

    """
    return [int(c) if c.isdigit() else c for c in re.split("([0-9]+)", s)]


# SOURCE: https://github.com/napari/napari/blob/main/napari/plugins/io.py
def magic_imread(filenames, *, use_dask=None, stack=True):
    """
    Dispatch the appropriate reader given some files.

    The files are assumed to all have the same shape.

    Parameters
    ----------
    filenames : list
        List of filenames or directories to be opened.
        A list of `pathlib.Path` objects and a single filename or `Path` object
        are also accepted.
    use_dask : bool
        Whether to use dask to create a lazy array, rather than NumPy.
        Default of None will resolve to True if filenames contains more than
        one image, False otherwise.
    stack : bool
        Whether to stack the images in multiple files into a single array. If
        False, a list of arrays will be returned.

    Returns
    -------
    image : array-like
        Array or list of images

    """
    # cast Path to string
    if isinstance(filenames, Path):
        filenames = filenames.as_posix()

    if len(filenames) == 0:
        return None
    if isinstance(filenames, str):
        filenames = [filenames]  # ensure list

    # replace folders with their contents
    filenames_expanded = []
    for filename in filenames:
        # zarr files are folders, but should be read as 1 file
        if os.path.isdir(filename):
            dir_contents = sorted(glob(os.path.join(filename, "*.*")), key=_alphanumeric_key)
            # remove subdirectories
            dir_contents_files = filter(lambda f: not os.path.isdir(f), dir_contents)
            filenames_expanded.extend(dir_contents_files)
        else:
            filenames_expanded.append(filename)

    if use_dask is None:
        use_dask = len(filenames_expanded) > 1

    if not filenames_expanded:
        raise ValueError(f"No files found in {filenames} after removing subdirectories")

    # then, read in images
    images = []
    shape = None
    for filename in filenames_expanded:
        if shape is None:
            image = imread(filename)
            shape = image.shape
            dtype = image.dtype
        if use_dask:
            image = da.from_delayed(delayed(imread)(filename), shape=shape, dtype=dtype)
        elif len(images) > 0:  # not read by shape clause
            image = imread(filename)
        images.append(image)
    if len(images) == 1:
        image = images[0]
    elif stack:
        if use_dask:
            image = da.stack(images)
        else:
            try:
                image = np.stack(images)
            except ValueError as e:
                if "input arrays must have the same shape" not in str(e):
                    raise e
                msg = (
                    "To stack multiple files into a single array with "
                    "numpy, all input arrays must have the same shape."
                    " Set `use_dask` to True to stack arrays with "
                    "different shapes."
                )
                raise ValueError(msg) from e
    else:
        image = images  # return a list
    return image


# def guess_zarr_path(path):
#     """Guess whether string path is part of a zarr hierarchy.

#     Parameters
#     ----------
#     path : str
#         Path to a file or directory.

#     Returns
#     -------
#     bool
#         Whether path is for zarr.
#     >>> guess_zarr_path('dataset.zarr')
#     True
#     >>> guess_zarr_path('dataset.zarr/path/to/array')
#     True
#     >>> guess_zarr_path('dataset.zarr/component.png')
#     True
#     """
#     return any(part.endswith(".zarr") for part in Path(path).parts)


# def read_zarr_dataset(path):
#     """Read a zarr dataset, including an array or a group of arrays.

#     Parameters
#     ----------
#     path : str
#         Path to directory ending in '.zarr'. Path can contain either an array
#         or a group of arrays in the case of multiscale data.
#     Returns
#     -------
#     image : array-like
#         Array or list of arrays
#     shape : tuple
#         Shape of array or first array in list
#     """
#     if os.path.exists(os.path.join(path, ".zarray")):
#         # load zarr array
#         image = da.from_zarr(path)
#         shape = image.shape
#     elif os.path.exists(os.path.join(path, ".zgroup")):
#         # else load zarr all arrays inside file, useful for multiscale data
#         image = []
#         for subpath in sorted(os.listdir(path)):
#             if not subpath.startswith("."):
#                 image.append(read_zarr_dataset(os.path.join(path, subpath))[0])
#         shape = image[0].shape
#     else:
#         raise ValueError(f"Not a zarr dataset or group: {path}")
#     return image, shape


def write_csv(
    filename: str,
    data: list | np.ndarray,
    column_names: list[str] | None = None,
):
    """
    Write a csv file.

    Parameters
    ----------
    filename : str
        Filename for saving csv.
    data : list or ndarray
        Table values, contained in a list of lists or an ndarray.
    column_names : list, optional
        List of column names for table data.

    """
    with open(filename, mode="w", newline="") as csvfile:
        writer = csv.writer(
            csvfile,
            delimiter=",",
            quotechar='"',
            quoting=csv.QUOTE_MINIMAL,
        )
        if column_names is not None:
            writer.writerow(column_names)
        for row in data:
            writer.writerow(row)


def guess_layer_type_from_column_names(
    column_names: list[str],
) -> str | None:
    """
    Guess layer type based on column names from a csv file.

    Parameters
    ----------
    column_names : list of str
        List of the column names from the csv.

    Returns
    -------
    str or None
        Layer type if recognized, otherwise None.

    """
    if {"index", "shape-type", "vertex-index", "axis-0", "axis-1"}.issubset(column_names):
        return "shapes"
    elif {"axis-0", "axis-1"}.issubset(column_names):
        return "points"
    else:
        return None


def read_csv(filename: str, require_type: str = None) -> tuple[np.array, list[str], str | None]:
    """
    Return CSV data only if column names match format for ``require_type``.

    Reads only the first line of the CSV at first, then optionally raises an
    exception if the column names are not consistent with a known format, as
    determined by the ``require_type`` argument and
    :func:`guess_layer_type_from_column_names`.

    Parameters
    ----------
    filename : str
        Path of file to open
    require_type : str, optional
        The desired layer type. If provided, should be one of the keys in
        ``csv_reader_functions`` or the string "any".  If ``None``, data, will
        not impose any format requirements on the csv, and data will always be
        returned.  If ``any``, csv must be recognized as one of the valid layer
        data formats, otherwise a ``ValueError`` will be raised.  If a specific
        layer type string, then a ``ValueError`` will be raised if the column
        names are not of the predicted format.

    Returns
    -------
    (data, column_names, layer_type) : Tuple[np.array, List[str], str]
        The table data and column names from the CSV file, along with the
        detected layer type (string).

    Raises
    ------
    ValueError
        If the column names do not match the format requested by
        ``require_type``.

    """
    with open(filename, newline="") as csvfile:
        reader = csv.reader(csvfile, delimiter=",")
        column_names = next(reader)

        layer_type = guess_layer_type_from_column_names(column_names)
        if require_type:
            if not layer_type:
                raise ValueError(f'File "{filename}" not recognized as valid Layer data')
            elif layer_type != require_type and require_type.lower() != "any":
                raise ValueError(f'File "{filename}" not recognized as {require_type} data')

        data = np.array(list(reader))
    return data, column_names, layer_type


# def csv_to_layer_data(path: str, require_type: str = None) -> Optional[FullLayerData]:
#     """Return layer data from a CSV file if detected as a valid type.

#     Parameters
#     ----------
#     path : str
#         Path of file to open
#     require_type : str, optional
#         The desired layer type. If provided, should be one of the keys in
#         ``csv_reader_functions`` or the string "any".  If ``None``,
#         unrecognized CSV files will simply return ``None``.  If ``any``,
#         unrecognized CSV files will raise a ``ValueError``.  If a specific
#         layer type string, then a ``ValueError`` will be raised if the column
#         names are not of the predicted format.

#     Returns
#     -------
#     layer_data : tuple, or None
#         3-tuple ``(array, dict, str)`` (points data, metadata, layer_type) if
#         CSV is recognized as a valid type.

#     Raises
#     ------
#     ValueError
#         If ``require_type`` is not ``None``, but the CSV is not detected as a
#         valid data format.
#     """
#     try:
#         # pass at least require "any" here so that we don't bother reading the
#         # full dataset if it's not going to yield valid layer_data.
#         _require = require_type or "any"
#         table, column_names, _type = read_csv(path, require_type=_require)
#     except ValueError:
#         if not require_type:
#             return None
#         raise
#     if _type in csv_reader_functions:
#         return csv_reader_functions[_type](table, column_names)
#     return None  # only reachable if it is a valid layer type without a reader


# def _points_csv_to_layerdata(
#     table: np.ndarray, column_names: List[str]
# ) -> FullLayerData:
#     """Convert table data and column names from a csv file to Points LayerData.

#     Parameters
#     ----------
#     table : np.ndarray
#         CSV data.
#     column_names : list of str
#         The column names of the csv file

#     Returns
#     -------
#     layer_data : tuple
#         3-tuple ``(array, dict, str)`` (points data, metadata, 'points')
#     """

#     data_axes = [cn.startswith("axis-") for cn in column_names]
#     data = np.array(table[:, data_axes]).astype("float")

#     # Add properties to metadata if provided
#     prop_axes = np.logical_not(data_axes)
#     if column_names[0] == "index":
#         prop_axes[0] = False
#     meta = {}
#     if np.any(prop_axes):
#         meta["properties"] = {}
#         for ind in np.nonzero(prop_axes)[0]:
#             values = table[:, ind]
#             try:
#                 values = np.array(values).astype("int")
#             except ValueError:
#                 try:
#                     values = np.array(values).astype("float")
#                 except ValueError:
#                     pass
#             meta["properties"][column_names[ind]] = values

#     return data, meta, "points"


# def _shapes_csv_to_layerdata(
#     table: np.ndarray, column_names: List[str]
# ) -> FullLayerData:
#     """Convert table data and column names from a csv file to Shapes LayerData.

#     Parameters
#     ----------
#     table : np.ndarray
#         CSV data.
#     column_names : list of str
#         The column names of the csv file

#     Returns
#     -------
#     layer_data : tuple
#         3-tuple ``(array, dict, str)`` (points data, metadata, 'shapes')
#     """

#     data_axes = [cn.startswith("axis-") for cn in column_names]
#     raw_data = np.array(table[:, data_axes]).astype("float")

#     inds = np.array(table[:, 0]).astype("int")
#     n_shapes = max(inds) + 1
#     # Determine when shape id changes
#     transitions = list((np.diff(inds)).nonzero()[0] + 1)
#     shape_boundaries = [0] + transitions + [len(table)]
#     if n_shapes != len(shape_boundaries) - 1:
#         raise ValueError("Expected number of shapes not found")

#     data = []
#     shape_type = []
#     for ind_a, ind_b in zip(shape_boundaries[:-1], shape_boundaries[1:]):
#         data.append(raw_data[ind_a:ind_b])
#         shape_type.append(table[ind_a, 1])

#     return data, {"shape_type": shape_type}, "shapes"


# csv_reader_functions = {
#     "points": _points_csv_to_layerdata,
#     "shapes": _shapes_csv_to_layerdata,
# }

</document_content>
</document>
<document index="48">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/json_api.py</source>
<document_content>
"""Utility functions to format sqlalchemy query data as JSON API."""

from __future__ import annotations

from sqlalchemy.orm import Query


def query_data_pagination(query: Query, page: int = 1, page_size: int = 50):
    """
    Format query data with pagination
    """
    page = max(1, page)  # min page number is 1
    page_size = min(1000, page_size)  # max page size is 1000
    offset = (page - 1) * page_size

    count = query.count()
    results = [u._asdict() for u in query.offset(offset).limit(page_size).all()]
    pcount = int(count / page_size)
    pcount += 0 if (count % page_size) == 0 else 1

    return {
        "data": results,
        "meta": {
            "pagination": {
                "count": count,
                "page": page,
                "page_count": pcount,
                "page_items": len(results),
                "page_size": page_size,
            },
        },
    }

</document_content>
</document>
<document index="49">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/misc.py</source>
<document_content>
"""Miscellaneous utility functions."""

# SOURCE: https://github.com/napari/napari/blob/6ce45aed3c893c03a47dc1d743e43b342b7022cb/napari/utils/misc.py#L400
# pylint: disable=no-member
from __future__ import annotations

import builtins
import contextlib
import importlib.metadata
import inspect
import itertools
import re
import sys

from collections.abc import Callable, Iterable, Sequence
from os import PathLike, fspath, path
from os import path as os_path
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union

import numpy as np


if TYPE_CHECKING:
    import packaging.version


ROOT_DIR = path.dirname(path.dirname(__file__))


def parse_version(v: str) -> packaging.version._BaseVersion:
    """
    Parse a version string and return a packaging.version.Version obj.

    Args:
        v (str): The version string to parse.

    Returns:
        packaging.version._BaseVersion: The parsed version object.
    """
    import packaging.version

    try:
        return packaging.version.Version(v)
    except packaging.version.InvalidVersion:
        return packaging.version.LegacyVersion(v)  # type: ignore[attr-defined]


def running_as_bundled_app() -> bool:
    """
    Infer whether we are running as a briefcase bundle.

    Returns:
        bool: True if running as a bundled app, False otherwise.
    """
    # https://github.com/beeware/briefcase/issues/412
    # https://github.com/beeware/briefcase/pull/425
    # note that a module may not have a __package__ attribute
    # From 0.4.12 we add a sentinel file next to the bundled sys.executable
    if (Path(sys.executable).parent / ".napari_is_bundled").exists():
        return True

    try:
        app_module = sys.modules["__main__"].__package__
    except AttributeError:
        return False

    if not app_module:
        return False

    try:
        metadata = importlib.metadata.metadata(app_module)
    except importlib.metadata.PackageNotFoundError:
        return False

    return "Briefcase-Version" in metadata


def bundle_bin_dir() -> str | None:
    """
    Return path to briefcase app_packages/bin if it exists.

    Returns:
        Optional[str]: The path to the bin directory if it exists, None otherwise.
    """
    path_to_bin: builtins.str = os_path.join(os_path.dirname(sys.exec_prefix), "app_packages", "bin")
    if path.isdir(path_to_bin):
        return f"{path_to_bin}"
    return None


def str_to_rgb(arg: str) -> list[int]:
    """
    Convert an rgb string 'rgb(x,y,z)' to a list of ints [x,y,z].

    Args:
        arg (str): The RGB string to convert.

    Returns:
        list[int]: The RGB values as a list of integers.
    """
    return list(map(int, re.match(r"rgb\((\d+),\s*(\d+),\s*(\d+)\)", arg).groups()))


def ensure_iterable(arg: Any, color: bool = False) -> Iterable:
    """
    Ensure an argument is an iterable. Useful when an input argument
    can either be a single value or a list. If a color is passed then it
    will be treated specially to determine if it is iterable.

    Args:
        arg (Any): The argument to check for iterability.
        color (bool, optional): Whether the argument represents a color. Defaults to False.

    Returns:
        Iterable: The input as an iterable.
    """
    return arg if is_iterable(arg, color=color) else itertools.repeat(arg)


def is_iterable(arg: Any, color: bool = False) -> bool:
    """
    Determine if a single argument is an iterable. If a color is being
    provided and the argument is a 1-D array of length 3 or 4 then the input
    is taken to not be iterable.

    Args:
        arg (Any): The argument to check for iterability.
        color (bool, optional): Whether the argument represents a color. Defaults to False.

    Returns:
        bool: True if the argument is iterable, False otherwise.
    """
    if arg is None or type(arg) is str or np.isscalar(arg):
        return False
    elif color and isinstance(arg, (list, np.ndarray)):
        return np.array(arg).ndim != 1 or len(arg) not in [3, 4]
    else:
        return True


def is_sequence(arg: Any) -> bool:
    """
    Check if ``arg`` is a sequence like a list or tuple.

    Args:
        arg (Any): The argument to check.

    Returns:
        bool: True if the argument is a sequence, False otherwise.
    """
    return isinstance(arg, Sequence) and not isinstance(arg, str)


def ensure_sequence_of_iterables(obj: Any, length: int | None = None) -> Iterable:
    """
    Ensure that ``obj`` behaves like a (nested) sequence of iterables.

    If length is provided and the object is already a sequence of iterables,
    a ValueError will be raised if ``len(obj) != length``.

    Args:
        obj (Any): The object to check.
        length (int, optional): Expected length of the sequence. Defaults to None.

    Returns:
        Iterable: A nested sequence of iterables, or an itertools.repeat instance.

    Raises:
        ValueError: If length is provided and the object's length does not match.
    """
    if obj is not None and is_sequence(obj) and is_iterable(obj[0]):
        if length is not None and len(obj) != length:
            raise ValueError(f"length of {obj} must equal {length}")
        return obj
    return itertools.repeat(obj)


def formatdoc(obj: Any) -> Any:
    """
    Substitute globals and locals into an object's docstring.

    Args:
        obj (Any): The object to format the docstring for.

    Returns:
        Any: The input object with the formatted docstring.
    """
    frame = inspect.currentframe().f_back
    try:
        obj.__doc__ = obj.__doc__.format(**{**frame.f_globals, **frame.f_locals})
        return obj
    finally:
        del frame


camel_to_snake_pattern = re.compile(r"(.)([A-Z][a-z]+)")
camel_to_spaces_pattern = re.compile(r"((?<=[a-z])[A-Z]|(?<!\A)[A-R,T-Z](?=[a-z]))")


def camel_to_snake(name: str) -> str:
    """
    Convert a camel case string to snake case.

    Args:
        name (str): The camel case string to convert.

    Returns:
        str: The snake case string.
    """
    return camel_to_snake_pattern.sub(r"\1_\2", name).lower()


def camel_to_spaces(val: str) -> str:
    """
    Convert a camel case string to a string with spaces before uppercase letters.

    Args:
        val (str): The camel case string to convert.

    Returns:
        str: The string with spaces added.
    """
    return camel_to_spaces_pattern.sub(r" \1", val)


T = TypeVar("T", str, Sequence[str])


def abspath_or_url(relpath: T) -> T:
    """
    Utility function that normalizes paths or a sequence thereof.

    Expands user directory and converts relpaths to abspaths... but ignores
    URLS that begin with "http", "ftp", or "file".

    Args:
        relpath (str or Sequence[str]): A path, or list or tuple of paths.

    Returns:
        str or Sequence[str]: An absolute path, or list or tuple of absolute paths (same type as input).

    Raises:
        TypeError: If the input is not a string, PathLike, or sequence thereof.
    """
    from urllib.parse import urlparse

    if isinstance(relpath, (tuple, list)):
        return type(relpath)(abspath_or_url(p) for p in relpath)

    if isinstance(relpath, (str, PathLike)):
        relpath = fspath(relpath)
        urlp = urlparse(relpath)
        if urlp.scheme and urlp.netloc:
            return relpath
        return path.abspath(path.expanduser(relpath))

    raise TypeError("Argument must be a string, PathLike, or sequence thereof")


class CallDefault(inspect.Parameter):
    def __str__(self) -> str:
        """
        Wrap defaults in a string representation.

        Returns:
            str: The string representation of the parameter with wrapped defaults.
        """
        kind = self.kind
        formatted = self.name

        # Fill in defaults
        if self.default is not inspect._empty or kind == inspect.Parameter.KEYWORD_ONLY:
            formatted = f"{formatted}={formatted}"

        if kind == inspect.Parameter.VAR_POSITIONAL:
            formatted = f"*{formatted}"
        elif kind == inspect.Parameter.VAR_KEYWORD:
            formatted = f"**{formatted}"

        return formatted


class CallSignature(inspect.Signature):
    _parameter_cls = CallDefault

    def __str__(self) -> str:
        """
        Return a string representation of the signature without separators.

        Returns:
            str: The string representation of the signature.
        """
        result = [str(param) for param in self.parameters.values()]
        rendered = f'({", ".join(result)})'

        if self.return_annotation is not inspect._empty:
            anno = inspect.formatannotation(self.return_annotation)
            rendered += f" -> {anno}"

        return rendered


callsignature = CallSignature.from_callable


def all_subclasses(cls: type) -> set[type]:
    """
    Recursively find all subclasses of class ``cls``.

    Args:
        cls (type): A python class (or anything that implements a __subclasses__ method).

    Returns:
        set[type]: The set of all classes that are subclassed from ``cls``.
    """
    return set(cls.__subclasses__()).union([s for c in cls.__subclasses__() for s in all_subclasses(c)])


def ensure_n_tuple(val: Iterable, n: int, fill: Any = 0) -> tuple:
    """
    Ensure input is a length n tuple.

    Args:
        val (Iterable): Iterable to be forced into length n-tuple.
        n (int): Length of tuple.
        fill (Any, optional): Fill value for missing elements. Defaults to 0.

    Returns:
        tuple: Coerced tuple.

    Raises:
        AssertionError: If n is not greater than 0.
    """
    assert n > 0, "n must be greater than 0"
    tuple_value = tuple(val)
    return (fill,) * (n - len(tuple_value)) + tuple_value[-n:]


def ensure_layer_data_tuple(val: Any) -> tuple:
    """
    Ensure input is a valid layer data tuple.

    Args:
        val (Any): The value to check.

    Returns:
        tuple: The input as a valid layer data tuple.

    Raises:
        TypeError: If the input is not a valid layer data tuple.
    """
    if not (isinstance(val, tuple) and (0 < len(val) <= 3)):
        raise TypeError(f"Not a valid layer data tuple: {val!r}")
    return val


def ensure_list_of_layer_data_tuple(val: Any) -> list[tuple]:
    """
    Ensure input is a valid list of layer data tuples.

    Args:
        val (Any): The value to check.

    Returns:
        list[tuple]: The input as a valid list of layer data tuples.

    Raises:
        TypeError: If the input is not a valid list of layer data tuples.
    """
    if isinstance(val, list) and len(val):
        with contextlib.suppress(TypeError):
            return [ensure_layer_data_tuple(v) for v in val]
    raise TypeError("Not a valid list of layer data tuples!")


def yesno(question: str, force: bool = False) -> bool:
    """
    Simple Yes/No prompt.

    Args:
        question (str): The question to ask.
        force (bool, optional): Whether to force a yes response. Defaults to False.

    Returns:
        bool: True if the user responds 'y', False if 'n'.
    """
    prompt = f"{question} ? (y/n): "
    ans = input(prompt).strip().lower()
    if ans not in ["y", "n"] and not force:
        print(f"{ans} is invalid, please try again...")
        return yesno(question)
    return ans == "y" or force


def divide_chunks(l: list[str], n: int = 10) -> Iterable[list[str]]:
    """
    Yield successive n-sized chunks from list l.

    Args:
        l (list[str]): The list to divide into chunks.
        n (int, optional): The size of each chunk. Defaults to 10.

    Yields:
        Iterable[list[str]]: Successive n-sized chunks from the input list.
    """
    for i in range(0, len(l), n):
        yield l[i : i + n]


CURRENTFUNCNAME: Callable[[int], str] = lambda n=0: sys._getframe(n + 1).f_code.co_name
"""
Get the name of the current function, or the name of the caller of the current function.

Args:
    n (int, optional): The number of frames to go back. Defaults to 0.
        0 returns the name of the current function.
        1 returns the name of the caller of the current function, etc.

Returns:
    str: The name of the requested function.
"""

</document_content>
</document>
<document index="50">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/naming.py</source>
<document_content>
"""Automatically generate names."""

from __future__ import annotations

import collections
import inspect
import re

from democracy_exe.utils.misc import formatdoc


sep = " "
start = 1

# Match integer between square brackets at end of string if after space
# or at beginning of string or just match end of string
numbered_patt = re.compile(r"((?<=\A\[)|(?<=\s\[))(?:\d+|)(?=\]$)|$")


def _inc_name_count_sub(match):
    count = match.group(0)

    try:
        count = int(count)
    except ValueError:  # not an int
        count = f"{sep}[{start}]"
    else:
        count = f"{count + 1}"

    return count


@formatdoc
def inc_name_count(name):
    """
    Increase a name's count matching `{numbered_patt}` by ``1``.

    If the name is not already numbered, append '{sep}[{start}]'.

    Parameters
    ----------
    name : str
        Original name.

    Returns
    -------
    incremented_name : str
        Numbered name incremented by ``1``.

    """
    return numbered_patt.sub(_inc_name_count_sub, name, count=1)


def magic_name(value, *, path_prefix):
    """
    Fetch the name of the variable with the given value passed to the calling function.

    Parameters
    ----------
    value : any
        The value of the desired variable.
    path_prefix : absolute path-like, kwonly
        The path prefixes to ignore.

    Returns
    -------
    name : str or None
        Name of the variable, if found.

    """
    frame = inspect.currentframe()
    try:
        # See issue #1635 regarding potential AttributeError
        # since frame could be None.
        # https://github.com/democracy_exe/democracy_exe/pull/1635
        if inspect.isframe(frame):
            frame = frame.f_back

        # Iterate frames while filename starts with path_prefix (part of democracy_exe)
        # or is autogenerated such as for the add_* for layers (#1694 / #1709)
        while (
            inspect.isframe(frame)
            and inspect.iscode(frame.f_code)
            and (frame.f_code.co_filename.startswith(path_prefix) or frame.f_code.co_filename == "<string>")
        ):
            frame = frame.f_back

        if inspect.isframe(frame) and inspect.iscode(frame.f_code):
            varmap = collections.ChainMap(frame.f_locals, frame.f_globals)
            names = *frame.f_code.co_varnames, *frame.f_code.co_names

            for name in names:
                if name.isidentifier() and name in varmap and varmap[name] is value:
                    return name
        return None
    finally:
        # We need to delete the frame explicitly according to the inspect
        # documentation for deterministic removal of the frame.
        # Otherwise, proper deletion is dependent on a cycle detector and
        # automatic garbage collection.
        # See handle_stackframe_without_leak example at the following URLs:
        # https://docs.python.org/3/library/inspect.html#the-interpreter-stack
        # https://bugs.python.org/issue543148
        del frame

</document_content>
</document>
<document index="51">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/paths.py</source>
<document_content>
# inspired by boucanpy
from __future__ import annotations

from os.path import abspath, dirname, join


_utils_dir = abspath(dirname(__file__))


def _ajoin(target: str, path: str) -> str:
    return abspath(join(target, path))

</document_content>
</document>
<document index="52">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/repo_typing.py</source>
<document_content>
# SOURCE: https://github.com/drawjk705/us-congress/blob/dedae9171953f770d9b6efe4f1e4db83f60dd4b4/run_pyright.py#L9
from __future__ import annotations

import os
import re


PYRIGHT_CMD = "pyright -p pyproject.toml ."
MISSING_TYPESTUB_PATTERN = r'.*error: Stub file not found for "(.*)".*'


def run_pyright() -> None:
    """
    Find all missing typestubs, generate them,
    then run pyright
    """
    modulesMissingStubs: set[str] = set()

    for line in os.popen(PYRIGHT_CMD).readlines():
        match = re.match(MISSING_TYPESTUB_PATTERN, line)
        print(f"math: {match}")
        if match:
            group = match[1]
            group = re.sub(r"\..*", "", group)
            modulesMissingStubs.add(group)

    for module in modulesMissingStubs:
        cmd = f"{PYRIGHT_CMD} --createstub {module}"
        print(cmd)
        # os.system(cmd)

    print(PYRIGHT_CMD)
    os.system(PYRIGHT_CMD)


if __name__ == "__main__":
    run_pyright()

</document_content>
</document>
<document index="53">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/run_service.py</source>
<document_content>
# pylint: disable=no-name-in-module
"""Utility functions to run services"""

from __future__ import annotations

from os import path

import structlog
import yaml


logger = structlog.get_logger(__name__)

from democracy_exe.ai.async_jobs import create_service


def run_service_from_yaml(file_path: str, num_attempts: int = 1):
    """Run service from YAML file"""
    if not path.isfile(file_path):
        raise FileNotFoundError(file_path)

    with open(file_path) as stream:
        parsed_yaml = yaml.safe_load(stream)

    attempt = 1
    while attempt <= num_attempts:
        run_service(
            service=parsed_yaml.get("service"),
            payload=parsed_yaml.get("payload"),
            output_path=parsed_yaml.get("output_path"),
            num_attempt=attempt,
        )
        attempt += 1


def save_test_result(file_path: str, num_attempt: int, output: str):
    """Save JOB result output to file, adding num attempt to file name"""
    file_name, file_extension = path.splitext(file_path)

    with open(f"{file_name}-{num_attempt}{file_extension}", "w") as stream:
        stream.write(output)


def run_service(service: str, payload: dict, output_path: str = None, num_attempt: int = 1):
    """Create and run service"""
    service = create_service(service)
    attempt_message = f'Attempt: {num_attempt} - File: {payload.get("file_path")}'
    logger.info("%s - start", attempt_message)
    result = service.run(payload)
    logger.info("%s - end", attempt_message)

    if output_path is None:
        logger.info("%s - `output_path` not specified, file not saved", attempt_message)
    else:
        save_test_result(file_path=output_path, num_attempt=num_attempt, output=result.get("output"))

</document_content>
</document>
<document index="54">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/shell.py</source>
<document_content>
"""Shell utility functions for executing commands and managing processes."""
from __future__ import annotations

import asyncio
import functools

from collections.abc import Callable
from typing import Any, TypeVar

import structlog


logger = structlog.get_logger(__name__)

from democracy_exe.shell import _aio_run_process_and_communicate, run_coroutine_subprocess


T = TypeVar("T")


def to_async(func: Callable[..., T]) -> Callable[..., asyncio.Future[T]]:
    """Convert a synchronous function to an asynchronous one.

    Args:
        func: The synchronous function to convert

    Returns:
        An asynchronous version of the function
    """
    @functools.wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> asyncio.Future[T]:
        return asyncio.get_event_loop().run_in_executor(None, lambda: func(*args, **kwargs))
    return wrapper


# async def _aio_run_process_and_communicate(
#     cmd: list[str], cwd: str | None = None
# ) -> str:
#     """Run a process asynchronously and get its output.

#     Args:
#         cmd: Command to run as list of strings
#         cwd: Working directory for the command

#     Returns:
#         The command output as a string

#     Raises:
#         subprocess.CalledProcessError: If command execution fails
#     """
#     try:
#         process = await asyncio.create_subprocess_exec(
#             *cmd,
#             stdout=asyncio.subprocess.PIPE,
#             stderr=asyncio.subprocess.PIPE,
#             cwd=cwd
#         )
#         stdout, stderr = await process.communicate()

#         if process.returncode != 0:
#             logger.error(f"Command failed: {' '.join(cmd)}")
#             logger.error(f"stderr: {stderr.decode()}")
#             raise RuntimeError(f"Command failed with return code {process.returncode}")

#         return stdout.decode().strip()
#     except Exception as e:
#         logger.error(f"Error running command {cmd}: {e}")
#         raise


# async def run_coroutine_subprocess(cmd: str, uri: str) -> None:
#     """Run a subprocess coroutine.

#     Args:
#         cmd: Command to run
#         uri: URI for the command

#     Raises:
#         RuntimeError: If command execution fails
#     """
#     try:
#         process = await asyncio.create_subprocess_shell(
#             cmd,
#             stdout=asyncio.subprocess.PIPE,
#             stderr=asyncio.subprocess.PIPE
#         )
#         stdout, stderr = await process.communicate()

#         if process.returncode != 0:
#             logger.error(f"Command failed: {cmd}")
#             logger.error(f"stderr: {stderr.decode()}")
#             raise RuntimeError(f"Command failed with return code {process.returncode}")

#         logger.info(f"Command output: {stdout.decode()}")
#     except Exception as e:
#         logger.error(f"Error running command {cmd}: {e}")
#         raise


async_ = to_async  # Alias for backward compatibility

</document_content>
</document>
<document index="55">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/tables.py</source>
<document_content>
# importing required library
from __future__ import annotations

</document_content>
</document>
<document index="56">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/temporary_file.py</source>
<document_content>
"""temporary_file.py"""

from __future__ import annotations

import os

from contextlib import contextmanager
from tempfile import NamedTemporaryFile


@contextmanager
def temporary_file(suffix=""):
    """
    Yield a writable temporary filename that is deleted on context exit.

    Parameters
    ----------
    suffix : string, optional
        The suffix for the file.

    Examples
    --------
    >>> import numpy as np
    >>> from democracy_exe.utils import io
    >>> with temporary_file(".tif") as tempfile:
    ...     im = np.arange(25, dtype=np.uint8).reshape((5, 5))
    ...     io.imsave(tempfile, im)
    ...     assert np.all(io.imread(tempfile) == im)

    """
    tempfile_stream = NamedTemporaryFile(suffix=suffix, delete=False)
    tempfile = tempfile_stream.name
    tempfile_stream.close()
    yield tempfile
    os.remove(tempfile)

</document_content>
</document>
<document index="57">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/timer.py</source>
<document_content>
"""Utility class for simple Timer and TimerCollection classes."""

from __future__ import annotations

import time

from typing import Optional


class Timer:
    """
    Utility timer class.

    This class can be used to time operations. It can be started, stopped, and reset. The duration of the timer can be
    retrieved at any time.

    Example::

        import time
        from mltemplate.utils import Timer

        timer = Timer()
        timer.start()
        time.sleep(1)
        timer.stop()
        print(f"The timer ran for {timer.duration()} seconds.")  # The timer ran for 1.0000000000000002 seconds.
        timer.reset()
        timer.start()
        time.sleep(2)
        timer.stop()
        print(f"The timer ran for {timer.duration()} seconds.")  # The timer ran for 2.0000000000000004 seconds.
    """

    def __init__(self):
        self._start_time: float | None = None
        self._stop_time: float | None = None
        self._duration: float = 0.0

    def start(self) -> None:
        """Start the timer."""
        if self._start_time is None:
            self._start_time = time.time()
            self._stop_time = None

    def stop(self) -> None:
        """Stop the timer."""
        if self._start_time is not None:
            self._stop_time = time.time()
            self._duration += self._stop_time - self._start_time
            self._start_time = None

    def duration(self) -> float:
        """Get the duration of the timer."""
        if self._start_time is not None and self._stop_time is None:
            return self._duration + (time.time() - self._start_time)
        else:
            return self._duration

    def reset(self) -> None:
        """Reset the timer."""
        self._start_time = None
        self._stop_time = None
        self._duration = 0.0

    def __str__(self) -> str:
        return f"{self.duration():.3f}s"


class TimerCollection:
    """
    Utility class for timing multiple operations.

    This class keeps a collection of named timers. Each timer can be started, stopped, and reset. The duration of each
    timer can be retrieved at any time. If a timer is stopped and restarted, the duration will be added to the previous
    duration. The timers can be reset individually, or all at once.

    Example::

        import time
        from mltemplate.utils import TimerCollection

        tc = TimerCollection()
        tc.start("Timer 1")
        tc.start("Timer 2")
        time.sleep(1)
        tc.stop("Timer 1")
        time.sleep(1)
        tc.stop("Timer 2")
        tc.start("Timer 3")
        time.sleep(1)
        tc.reset("Timer 1")
        print(tc)
        # Timer 1: 0.000s
        # Timer 2: 2.000s
        # Timer 3: 1.000s
        tc.reset_all()
        print(tc)
        # Timer 1: 0.000s
        # Timer 2: 0.000s
        # Timer 3: 0.000s

    """

    def __init__(self):
        self._timers: dict[str, Timer] = {}

    def start(self, name: str) -> None:
        """Start the timer with the given name."""
        if name not in self._timers:
            self._timers[name] = Timer()
        self._timers[name].start()

    def stop(self, name: str) -> None:
        """Stop the timer with the given name."""
        if name not in self._timers:
            raise KeyError(f"Timer {name} does not exist. Unable to stop.")
        self._timers[name].stop()

    def duration(self, name: str) -> float:
        """Get the duration of the timer with the given name."""
        if name not in self._timers:
            raise KeyError(f"Timer {name} does not exist. Unable to get duration.")
        return self._timers[name].duration()

    def reset(self, name: str) -> None:
        """Reset the timer with the given name."""
        if name not in self._timers:
            raise KeyError(f"Timer {name} does not exist. Unable to reset.")
        self._timers[name].reset()

    def reset_all(self) -> None:
        """Reset all timers."""
        for timer in self._timers.values():
            timer.reset()

    def names(self) -> dict[str, Timer]:
        """Get the names of all timers."""
        return self._timers.keys()

    def __str__(self) -> str:
        """Print each timer to the nearest microsecond."""
        return "\n".join([f"{name}: {timer.duration():.6f}s" for name, timer in self._timers.items()])

</document_content>
</document>
<document index="58">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/unpickler.py</source>
<document_content>
"""unpickler.py"""
# https://github.com/universityofprofessorex/ESRGAN-Bot
# Attribution - NonCommercial - ShareAlike 4.0 International

# == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == =

# Creative Commons Corporation("Creative Commons") is not a law firm and
# does not provide legal services or legal advice. Distribution of
# Creative Commons public licenses does not create a lawyer - client or
# other relationship. Creative Commons makes its licenses and related
# information available on an "as-is" basis. Creative Commons gives no
# warranties regarding its licenses, any material licensed under their
# terms and conditions, or any related information. Creative Commons
# disclaims all liability for damages resulting from their use to the
# fullest extent possible.

# Using Creative Commons Public Licenses

# Creative Commons public licenses provide a standard set of terms and
# conditions that creators and other rights holders may use to share
# original works of authorship and other material subject to copyright
# and certain other rights specified in the public license below. The
# following considerations are for informational purposes only, are not
# exhaustive, and do not form part of our licenses.

#      Considerations for licensors: Our public licenses are
#      intended for use by those authorized to give the public
#      permission to use material in ways otherwise restricted by
#      copyright and certain other rights. Our licenses are
#      irrevocable. Licensors should read and understand the terms
#      and conditions of the license they choose before applying it.
#      Licensors should also secure all rights necessary before
#      applying our licenses so that the public can reuse the
#      material as expected. Licensors should clearly mark any
#      material not subject to the license. This includes other CC-
#      licensed material, or material used under an exception or
#      limitation to copyright. More considerations for licensors:
#     wiki.creativecommons.org/Considerations_for_licensors

#      Considerations for the public: By using one of our public
#      licenses, a licensor grants the public permission to use the
#      licensed material under specified terms and conditions. If
#      the licensor's permission is not necessary for any reason--for
#      example, because of any applicable exception or limitation to
#      copyright--then that use is not regulated by the license. Our
#      licenses grant only permissions under copyright and certain
#      other rights that a licensor has authority to grant. Use of
#      the licensed material may still be restricted for other
#      reasons, including because others have copyright or other
#      rights in the material. A licensor may make special requests,
#      such as asking that all changes be marked or described.
#      Although not required by our licenses, you are encouraged to
#      respect those requests where reasonable. More considerations
#      for the public:
#     wiki.creativecommons.org/Considerations_for_licensees

# =======================================================================

# Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International
# Public License

# By exercising the Licensed Rights (defined below), You accept and agree
# to be bound by the terms and conditions of this Creative Commons
# Attribution-NonCommercial-ShareAlike 4.0 International Public License
# ("Public License"). To the extent this Public License may be
# interpreted as a contract, You are granted the Licensed Rights in
# consideration of Your acceptance of these terms and conditions, and the
# Licensor grants You such rights in consideration of benefits the
# Licensor receives from making the Licensed Material available under
# these terms and conditions.


# Section 1 -- Definitions.

#   a. Adapted Material means material subject to Copyright and Similar
#      Rights that is derived from or based upon the Licensed Material
#      and in which the Licensed Material is translated, altered,
#      arranged, transformed, or otherwise modified in a manner requiring
#      permission under the Copyright and Similar Rights held by the
#      Licensor. For purposes of this Public License, where the Licensed
#      Material is a musical work, performance, or sound recording,
#      Adapted Material is always produced where the Licensed Material is
#      synched in timed relation with a moving image.

#   b. Adapter's License means the license You apply to Your Copyright
#      and Similar Rights in Your contributions to Adapted Material in
#      accordance with the terms and conditions of this Public License.

#   c. BY-NC-SA Compatible License means a license listed at
#      creativecommons.org/compatiblelicenses, approved by Creative
#      Commons as essentially the equivalent of this Public License.

#   d. Copyright and Similar Rights means copyright and/or similar rights
#      closely related to copyright including, without limitation,
#      performance, broadcast, sound recording, and Sui Generis Database
#      Rights, without regard to how the rights are labeled or
#      categorized. For purposes of this Public License, the rights
#      specified in Section 2(b)(1)-(2) are not Copyright and Similar
#      Rights.

#   e. Effective Technological Measures means those measures that, in the
#      absence of proper authority, may not be circumvented under laws
#      fulfilling obligations under Article 11 of the WIPO Copyright
#      Treaty adopted on December 20, 1996, and/or similar international
#      agreements.

#   f. Exceptions and Limitations means fair use, fair dealing, and/or
#      any other exception or limitation to Copyright and Similar Rights
#      that applies to Your use of the Licensed Material.

#   g. License Elements means the license attributes listed in the name
#      of a Creative Commons Public License. The License Elements of this
#      Public License are Attribution, NonCommercial, and ShareAlike.

#   h. Licensed Material means the artistic or literary work, database,
#      or other material to which the Licensor applied this Public
#      License.

#   i. Licensed Rights means the rights granted to You subject to the
#      terms and conditions of this Public License, which are limited to
#      all Copyright and Similar Rights that apply to Your use of the
#      Licensed Material and that the Licensor has authority to license.

#   j. Licensor means the individual(s) or entity(ies) granting rights
#      under this Public License.

#   k. NonCommercial means not primarily intended for or directed towards
#      commercial advantage or monetary compensation. For purposes of
#      this Public License, the exchange of the Licensed Material for
#      other material subject to Copyright and Similar Rights by digital
#      file-sharing or similar means is NonCommercial provided there is
#      no payment of monetary compensation in connection with the
#      exchange.

#   l. Share means to provide material to the public by any means or
#      process that requires permission under the Licensed Rights, such
#      as reproduction, public display, public performance, distribution,
#      dissemination, communication, or importation, and to make material
#      available to the public including in ways that members of the
#      public may access the material from a place and at a time
#      individually chosen by them.

#   m. Sui Generis Database Rights means rights other than copyright
#      resulting from Directive 96/9/EC of the European Parliament and of
#      the Council of 11 March 1996 on the legal protection of databases,
#      as amended and/or succeeded, as well as other essentially
#      equivalent rights anywhere in the world.

#   n. You means the individual or entity exercising the Licensed Rights
#      under this Public License. Your has a corresponding meaning.


# Section 2 -- Scope.

#   a. License grant.

#        1. Subject to the terms and conditions of this Public License,
#           the Licensor hereby grants You a worldwide, royalty-free,
#           non-sublicensable, non-exclusive, irrevocable license to
#           exercise the Licensed Rights in the Licensed Material to:

#             a. reproduce and Share the Licensed Material, in whole or
#                in part, for NonCommercial purposes only; and

#             b. produce, reproduce, and Share Adapted Material for
#                NonCommercial purposes only.

#        2. Exceptions and Limitations. For the avoidance of doubt, where
#           Exceptions and Limitations apply to Your use, this Public
#           License does not apply, and You do not need to comply with
#           its terms and conditions.

#        3. Term. The term of this Public License is specified in Section
#           6(a).

#        4. Media and formats; technical modifications allowed. The
#           Licensor authorizes You to exercise the Licensed Rights in
#           all media and formats whether now known or hereafter created,
#           and to make technical modifications necessary to do so. The
#           Licensor waives and/or agrees not to assert any right or
#           authority to forbid You from making technical modifications
#           necessary to exercise the Licensed Rights, including
#           technical modifications necessary to circumvent Effective
#           Technological Measures. For purposes of this Public License,
#           simply making modifications authorized by this Section 2(a)
#           (4) never produces Adapted Material.

#        5. Downstream recipients.

#             a. Offer from the Licensor -- Licensed Material. Every
#                recipient of the Licensed Material automatically
#                receives an offer from the Licensor to exercise the
#                Licensed Rights under the terms and conditions of this
#                Public License.

#             b. Additional offer from the Licensor -- Adapted Material.
#                Every recipient of Adapted Material from You
#                automatically receives an offer from the Licensor to
#                exercise the Licensed Rights in the Adapted Material
#                under the conditions of the Adapter's License You apply.

#             c. No downstream restrictions. You may not offer or impose
#                any additional or different terms or conditions on, or
#                apply any Effective Technological Measures to, the
#                Licensed Material if doing so restricts exercise of the
#                Licensed Rights by any recipient of the Licensed
#                Material.

#        6. No endorsement. Nothing in this Public License constitutes or
#           may be construed as permission to assert or imply that You
#           are, or that Your use of the Licensed Material is, connected
#           with, or sponsored, endorsed, or granted official status by,
#           the Licensor or others designated to receive attribution as
#           provided in Section 3(a)(1)(A)(i).

#   b. Other rights.

#        1. Moral rights, such as the right of integrity, are not
#           licensed under this Public License, nor are publicity,
#           privacy, and/or other similar personality rights; however, to
#           the extent possible, the Licensor waives and/or agrees not to
#           assert any such rights held by the Licensor to the limited
#           extent necessary to allow You to exercise the Licensed
#           Rights, but not otherwise.

#        2. Patent and trademark rights are not licensed under this
#           Public License.

#        3. To the extent possible, the Licensor waives any right to
#           collect royalties from You for the exercise of the Licensed
#           Rights, whether directly or through a collecting society
#           under any voluntary or waivable statutory or compulsory
#           licensing scheme. In all other cases the Licensor expressly
#           reserves any right to collect such royalties, including when
#           the Licensed Material is used other than for NonCommercial
#           purposes.


# Section 3 -- License Conditions.

# Your exercise of the Licensed Rights is expressly made subject to the
# following conditions.

#   a. Attribution.

#        1. If You Share the Licensed Material (including in modified
#           form), You must:

#             a. retain the following if it is supplied by the Licensor
#                with the Licensed Material:

#                  i. identification of the creator(s) of the Licensed
#                     Material and any others designated to receive
#                     attribution, in any reasonable manner requested by
#                     the Licensor (including by pseudonym if
#                     designated);

#                 ii. a copyright notice;

#                iii. a notice that refers to this Public License;

#                 iv. a notice that refers to the disclaimer of
#                     warranties;

#                  v. a URI or hyperlink to the Licensed Material to the
#                     extent reasonably practicable;

#             b. indicate if You modified the Licensed Material and
#                retain an indication of any previous modifications; and

#             c. indicate the Licensed Material is licensed under this
#                Public License, and include the text of, or the URI or
#                hyperlink to, this Public License.

#        2. You may satisfy the conditions in Section 3(a)(1) in any
#           reasonable manner based on the medium, means, and context in
#           which You Share the Licensed Material. For example, it may be
#           reasonable to satisfy the conditions by providing a URI or
#           hyperlink to a resource that includes the required
#           information.
#        3. If requested by the Licensor, You must remove any of the
#           information required by Section 3(a)(1)(A) to the extent
#           reasonably practicable.

#   b. ShareAlike.

#      In addition to the conditions in Section 3(a), if You Share
#      Adapted Material You produce, the following conditions also apply.

#        1. The Adapter's License You apply must be a Creative Commons
#           license with the same License Elements, this version or
#           later, or a BY-NC-SA Compatible License.

#        2. You must include the text of, or the URI or hyperlink to, the
#           Adapter's License You apply. You may satisfy this condition
#           in any reasonable manner based on the medium, means, and
#           context in which You Share Adapted Material.

#        3. You may not offer or impose any additional or different terms
#           or conditions on, or apply any Effective Technological
#           Measures to, Adapted Material that restrict exercise of the
#           rights granted under the Adapter's License You apply.


# Section 4 -- Sui Generis Database Rights.

# Where the Licensed Rights include Sui Generis Database Rights that
# apply to Your use of the Licensed Material:

#   a. for the avoidance of doubt, Section 2(a)(1) grants You the right
#      to extract, reuse, reproduce, and Share all or a substantial
#      portion of the contents of the database for NonCommercial purposes
#      only;

#   b. if You include all or a substantial portion of the database
#      contents in a database in which You have Sui Generis Database
#      Rights, then the database in which You have Sui Generis Database
#      Rights (but not its individual contents) is Adapted Material,
#      including for purposes of Section 3(b); and

#   c. You must comply with the conditions in Section 3(a) if You Share
#      all or a substantial portion of the contents of the database.

# For the avoidance of doubt, this Section 4 supplements and does not
# replace Your obligations under this Public License where the Licensed
# Rights include other Copyright and Similar Rights.


# Section 5 -- Disclaimer of Warranties and Limitation of Liability.

#   a. UNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE
#      EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS
#      AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF
#      ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS,
#      IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION,
#      WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR
#      PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS,
#      ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT
#      KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT
#      ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.

#   b. TO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE
#      TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION,
#      NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT,
#      INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES,
#      COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR
#      USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN
#      ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR
#      DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR
#      IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.

#   c. The disclaimer of warranties and limitation of liability provided
#      above shall be interpreted in a manner that, to the extent
#      possible, most closely approximates an absolute disclaimer and
#      waiver of all liability.


# Section 6 -- Term and Termination.

#   a. This Public License applies for the term of the Copyright and
#      Similar Rights licensed here. However, if You fail to comply with
#      this Public License, then Your rights under this Public License
#      terminate automatically.

#   b. Where Your right to use the Licensed Material has terminated under
#      Section 6(a), it reinstates:

#        1. automatically as of the date the violation is cured, provided
#           it is cured within 30 days of Your discovery of the
#           violation; or

#        2. upon express reinstatement by the Licensor.

#      For the avoidance of doubt, this Section 6(b) does not affect any
#      right the Licensor may have to seek remedies for Your violations
#      of this Public License.

#   c. For the avoidance of doubt, the Licensor may also offer the
#      Licensed Material under separate terms or conditions or stop
#      distributing the Licensed Material at any time; however, doing so
#      will not terminate this Public License.

#   d. Sections 1, 5, 6, 7, and 8 survive termination of this Public
#      License.


# Section 7 -- Other Terms and Conditions.

#   a. The Licensor shall not be bound by any additional or different
#      terms or conditions communicated by You unless expressly agreed.

#   b. Any arrangements, understandings, or agreements regarding the
#      Licensed Material not stated herein are separate from and
#      independent of the terms and conditions of this Public License.


# Section 8 -- Interpretation.

#   a. For the avoidance of doubt, this Public License does not, and
#      shall not be interpreted to, reduce, limit, restrict, or impose
#      conditions on any use of the Licensed Material that could lawfully
#      be made without permission under this Public License.

#   b. To the extent possible, if any provision of this Public License is
#      deemed unenforceable, it shall be automatically reformed to the
#      minimum extent necessary to make it enforceable. If the provision
#      cannot be reformed, it shall be severed from this Public License
#      without affecting the enforceability of the remaining terms and
#      conditions.

#   c. No term or condition of this Public License will be waived and no
#      failure to comply consented to unless expressly agreed to by the
#      Licensor.

#   d. Nothing in this Public License constitutes or may be interpreted
#      as a limitation upon, or waiver of, any privileges and immunities
#      that apply to the Licensor or You, including from the legal
#      processes of any jurisdiction or authority.

# =======================================================================

# Creative Commons is not a party to its public
# licenses. Notwithstanding, Creative Commons may elect to apply one of
# its public licenses to material it publishes and in those instances
# will be considered the "Licensor." The text of the Creative Commons
# public licenses is dedicated to the public domain under the CC0 Public
# Domain Dedication. Except for the limited purpose of indicating that
# material is shared under a Creative Commons public license or as
# otherwise permitted by the Creative Commons policies published at
# creativecommons.org/policies, Creative Commons does not authorize the
# use of the trademark "Creative Commons" or any other trademark or logo
# of Creative Commons without its prior written consent including,
# without limitation, in connection with any unauthorized modifications
# to any of its public licenses or any other arrangements,
# understandings, or agreements concerning use of licensed material. For
# the avoidance of doubt, this paragraph does not form part of the
# public licenses.

# Creative Commons may be contacted at creativecommons.org.

# Safe unpickler to prevent arbitrary code execution
from __future__ import annotations

import pickle

from types import SimpleNamespace
from typing import Any


safe_list = {
    ("collections", "OrderedDict"),
    ("torch._utils", "_rebuild_tensor_v2"),
    ("torch", "FloatStorage"),
}


class RestrictedUnpickler(pickle.Unpickler):
    """
    _summary_

    Args:
    ----
        pickle (_type_): _description_

    """

    def find_class(self, module: Any, name: Any) -> Any:
        """
        _summary_

        Args:
        ----
            module (_type_): _description_
            name (_type_): _description_

        Raises:
        ------
            pickle.UnpicklingError: _description_

        Returns:
        -------
            _type_: _description_

        """
        # Only allow required classes to load state dict
        if (module, name) not in safe_list:
            raise pickle.UnpicklingError(f"Global '{module}.{name}' is forbidden")
        return super().find_class(module, name)


RestrictedUnpickle = SimpleNamespace(
    Unpickler=RestrictedUnpickler,
    __name__="pickle",
    load=lambda *args, **kwargs: RestrictedUnpickler(*args, **kwargs).load(),
)

</document_content>
</document>
<document index="59">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/uuid.py</source>
<document_content>
"""UUID utility functions"""

from __future__ import annotations

import uuid


def is_valid_uuid(val) -> bool:
    """Check UUID validity"""
    try:
        uuid.UUID(str(val))
        return True
    except (ValueError, TypeError):
        return False

</document_content>
</document>
<document index="60">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/vidops.py</source>
<document_content>
# pylint: disable=possibly-used-before-assignment
# pylint: disable=consider-using-from-import
# https://github.com/universityofprofessorex/ESRGAN-Bot

# Creative Commons may be contacted at creativecommons.org.
# NOTE: For more examples tqdm + aiofile, search https://github.com/search?l=Python&q=aiofile+tqdm&type=Code
# pylint: disable=no-member

from __future__ import annotations

import asyncio
import concurrent.futures
import functools
import pathlib
import sys
import traceback

from pathlib import Path

import structlog


logger = structlog.get_logger(__name__)

from democracy_exe import shell
from democracy_exe.shell import _aio_run_process_and_communicate
from democracy_exe.utils.file_functions import VIDEO_EXTENSIONS, unlink_orig_file


# https://github.com/universityofprofessorex/ESRGAN-Bot


async def get_duration(input_file: Path) -> float:
    """
    Get duration of a file using FFmpeg.

    Args:
    ----
        input_file (Path): The path to the input audio file.

    """
    logger.debug(f"Processing audio file: {input_file}")

    # Calculate input file duration
    duration_cmd = [
        "ffprobe",
        "-v",
        "error",
        "-show_entries",
        "format=duration",
        "-of",
        "default=noprint_wrappers=1:nokey=1",
        str(input_file),
    ]
    logger.debug(f"duration_cmd = {duration_cmd}")
    duration = float(await _aio_run_process_and_communicate(duration_cmd))
    logger.debug(f"duration = {duration}")
    # await logger.complete()
    return duration


def calculate_bitrate(duration: float, multiplier: int) -> int:
    """
    Calculate bitrate based on duration and multiplier.

    Args:
    ----
        duration (float): The duration of the media file in seconds.
        multiplier (int): A multiplier to adjust the bitrate calculation.

    Returns:
    -------
        int: The calculated bitrate in kbps.

    """
    bitrate = int(multiplier * 8 * 1000 / duration)
    logger.debug(f"bitrate = {bitrate}")
    return bitrate


async def process_video(input_file: Path) -> None:
    """
    Process and compress a video file using FFmpeg.

    This function calculates the appropriate bitrate based on the video duration,
    then compresses the video using FFmpeg with the calculated bitrate.

    Args:
    ----
        input_file (Path): The path to the input video file.

    Returns:
    -------
        None

    """
    logger.debug(f"Processing video file: {input_file}")

    # Calculate bitrate based on input file duration
    duration_cmd = [
        "ffprobe",
        "-v",
        "error",
        "-show_entries",
        "format=duration",
        "-of",
        "default=noprint_wrappers=1:nokey=1",
        str(input_file),
    ]
    duration = await get_duration(input_file)
    bitrate = calculate_bitrate(duration, 23)

    logger.debug(f"Video length: {duration}s")
    logger.debug(f"Bitrate target: {bitrate}k")

    # Exit if target bitrate is under 150kbps
    if bitrate < 150:
        logger.debug("Target bitrate is under 150kbps.")
        logger.debug("Unable to compress.")
        # await logger.complete()
        return

    video_bitrate = int(bitrate * 90 / 100)
    audio_bitrate = int(bitrate * 10 / 100)

    logger.debug(f"Video Bitrate: {video_bitrate}k")
    logger.debug(f"Audio Bitrate: {audio_bitrate}k")

    # Exit if target video bitrate is under 125kbps
    if video_bitrate < 125:
        logger.debug("Target video bitrate is under 125kbps.")
        logger.debug("Unable to compress.")
        # await logger.complete()
        return

    # Exit if target audio bitrate is under 32kbps
    if audio_bitrate < 32:
        logger.debug("Target audio bitrate is under 32.")
        logger.debug("Unable to compress.")
        # await logger.complete()
        return

    logger.debug("Compressing video file using FFmpeg...")
    output_file = input_file.parent / f"25MB_{input_file.stem}.mp4"
    compress_cmd = [
        "ffmpeg",
        "-y",
        "-hide_banner",
        "-loglevel",
        "warning",
        "-stats",
        "-threads",
        "0",
        "-hwaccel",
        "auto",
        "-i",
        str(input_file),
        "-preset",
        "slow",
        "-c:v",
        "libx264",
        "-b:v",
        f"{video_bitrate}k",
        "-c:a",
        "aac",
        "-b:a",
        f"{audio_bitrate}k",
        "-bufsize",
        f"{bitrate}k",
        "-minrate",
        "100k",
        "-maxrate",
        f"{bitrate}k",
        str(output_file),
    ]
    logger.debug(f"compress_cmd = {compress_cmd}")
    await _aio_run_process_and_communicate(compress_cmd)
    # await logger.complete()


async def process_audio(input_file: Path) -> None:
    """
    Process and compress an audio file using FFmpeg.

    This function calculates the appropriate bitrate based on the audio duration,
    then compresses the audio using FFmpeg with the calculated bitrate.

    Args:
    ----
        input_file (Path): The path to the input audio file.

    Returns:
    -------
        None

    """
    duration = await get_duration(input_file)
    bitrate = calculate_bitrate(duration, 25)

    logger.debug(f"Audio duration: {duration}s")
    logger.debug(f"Bitrate target: {bitrate}k")

    # Exit if target bitrate is under 32kbps
    if bitrate < 32:
        logger.debug("Target bitrate is under 32kbps.")
        logger.debug("Unable to compress.")
        # await logger.complete()
        return

    logger.debug("Compressing audio file using FFmpeg...")
    output_file = input_file.parent / f"25MB_{input_file.stem}.mp3"
    compress_cmd = [
        "ffmpeg",
        "-y",
        "-hide_banner",
        "-loglevel",
        "warning",
        "-stats",
        "-i",
        str(input_file),
        "-preset",
        "slow",
        "-c:a",
        "libmp3lame",
        "-b:a",
        f"{bitrate}k",
        "-bufsize",
        f"{bitrate}k",
        "-minrate",
        "100k",
        "-maxrate",
        f"{bitrate}k",
        str(output_file),
    ]
    logger.debug(f"compress_cmd = {compress_cmd}")
    await _aio_run_process_and_communicate(compress_cmd)
    # await logger.complete()


async def aio_compress_video(tmpdirname: str, file_to_compress: str) -> bool:
    """
    _summary_

    Args:
    ----
        tmpdirname (str): _description_
        file_to_compress (str): _description_
        bot (Any): _description_
        ctx (Any): _description_

    Returns:
    -------
        List[str]: _description_

    """
    if (pathlib.Path(f"{file_to_compress}").is_file()) and pathlib.Path(
        f"{file_to_compress}"
    ).suffix in VIDEO_EXTENSIONS:
        logger.debug(f"compressing file -> {file_to_compress}")
        ######################################################
        # compress the file if it is too large
        ######################################################
        compress_command = [
            "./scripts/compress-discord.sh",
            f"{file_to_compress}",
        ]

        loop = asyncio.get_running_loop()

        try:
            _ = await shell._aio_run_process_and_communicate(compress_command, cwd=f"{tmpdirname}")

            logger.debug(
                f"compress_video: new file size for {file_to_compress} = {pathlib.Path(file_to_compress).stat().st_size}"
            )

            ######################################################
            # nuke the uncompressed version
            ######################################################

            logger.info(f"nuking uncompressed: {file_to_compress}")

            # nuke the originals
            unlink_func = functools.partial(unlink_orig_file, f"{file_to_compress}")

            # 2. Run in a custom thread pool:
            with concurrent.futures.ThreadPoolExecutor() as pool:
                unlink_result = await loop.run_in_executor(pool, unlink_func)

            # await logger.complete()
            return True
        except Exception as ex:
            print(ex)
            exc_type, exc_value, exc_traceback = sys.exc_info()
            logger.error(f"Error Class: {ex.__class__!s}")
            output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
            logger.warning(output)
            logger.error(f"exc_type: {exc_type}")
            logger.error(f"exc_value: {exc_value}")
            traceback.print_tb(exc_traceback)
            # await logger.complete()

    else:
        logger.debug(f"no videos to process in {tmpdirname}")
        # await logger.complete()
        return False


def compress_video(tmpdirname: str, file_to_compress: str) -> bool:
    """
    _summary_

    Args:
    ----
        tmpdirname (str): _description_
        file_to_compress (str): _description_
        bot (Any): _description_
        ctx (Any): _description_

    Returns:
    -------
        List[str]: _description_

    """
    if (pathlib.Path(f"{file_to_compress}").is_file()) and pathlib.Path(
        f"{file_to_compress}"
    ).suffix in VIDEO_EXTENSIONS:
        logger.debug(f"compressing file -> {file_to_compress}")
        ######################################################
        # compress the file if it is too large
        ######################################################
        compress_command = [
            "./scripts/compress-discord.sh",
            f"{file_to_compress}",
        ]

        try:
            _ = shell.pquery(compress_command, cwd=f"{tmpdirname}")

            logger.debug(
                f"compress_video: new file size for {file_to_compress} = {pathlib.Path(file_to_compress).stat().st_size}"
            )

            logger.info(f"nuking uncompressed: {file_to_compress}")

            # nuke the originals
            unlink_orig_file(f"{file_to_compress}")
            # logger.complete()()
            return True
        except Exception as ex:
            print(ex)
            exc_type, exc_value, exc_traceback = sys.exc_info()
            logger.error(f"Error Class: {ex.__class__!s}")
            output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
            logger.warning(output)
            logger.error(f"exc_type: {exc_type}")
            logger.error(f"exc_value: {exc_value}")
            traceback.print_tb(exc_traceback)
            # logger.complete()()

    else:
        logger.debug(f"no videos to process in {tmpdirname}")
        # logger.complete()()
        return False

</document_content>
</document>
<document index="61">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/writer.py</source>
<document_content>
"""Write to disk asynchronously."""

# https://github.com/hackersandslackers/asyncio-tutorial/blob/0f4c99776b61ca3eafd850c43202bc7c52349552/asyncio_tutorial/part_II_aiohttp_aiofiles/writer.py
from __future__ import annotations

import asyncio

import aiofiles
import structlog


logger = structlog.get_logger(__name__)


async def write_file(fname: str, body: bytes, filetype: str, directory: str):
    """
    Write contents of fetched URL to new file in local directory.
    :param str fname: URL which was fetched.
    :param bytes body: Source HTML of a single fetched URL.
    :param str filetype: File extension to save fetched data as.
    :param str directory: Local directory to save exports to.
    """
    try:
        filename = f"{directory}/{fname}.{filetype}"
        logger.info(f"writing file -> {filename} ....")
        async with aiofiles.open(filename, mode="wb+") as f:
            await f.write(body)
            await f.close()
    except Exception as e:
        logger.error(f"Unexpected error while writing from `{fname}`: {e}")
    finally:
        # await logger.complete()
        await asyncio.sleep(1)
    return filename

</document_content>
</document>
<document index="62">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/retry/__init__.py</source>
<document_content>
#!/usr/bin/env python3
"""retry utility."""

# This module provides function with commmon parameters for the tenacity.retry
# decorator
from __future__ import annotations

import logging

from typing import Any, Dict

import tenacity

from democracy_exe.aio_settings import aiosettings


TenacityParameters = dict[str, Any]
import structlog


logger = structlog.get_logger(__name__)


def _base_parameters() -> TenacityParameters:
    return {
        "before_sleep": tenacity.before_sleep_log(logger, logging.WARN),
        "stop": tenacity.stop_after_attempt(aiosettings.retry_stop_after_attempt),
    }


def linear_backoff_parameters(**kwargs: Any) -> TenacityParameters:
    """
    Returns parameters for tenacity.retry that configure a linear backoff.

    This can be used as follows:
        @tenacity.retry(**linear_backoff())
        def function_to_retry()

    Keyword arguments for tenacity.retry can be passed to this function to
    override arguments as desired.
    """
    return {
        **_base_parameters(),
        **{"wait": tenacity.wait_fixed(aiosettings.retry_wait_fixed)},
        **kwargs,
    }


def exponential_backoff_parameters(**kwargs: Any) -> TenacityParameters:
    """
    Returns parameters for tenacity.retry that configure an exponential backoff.

    This can be used as follows:
        @tenacity.retry(**exponential_backoff())
        def function_to_retry()

    Keyword arguments for tenacity.retry can be passed to this function to
    override arguments as desired.
    """
    return {
        **_base_parameters(),
        **{
            "wait": tenacity.wait_exponential(
                min=aiosettings.retry_wait_exponential_min,
                max=aiosettings.retry_wait_exponential_max,
                multiplier=aiosettings.retry_wait_exponential_multiplier,
            )
        },
        **kwargs,
    }


def is_result_none(value: Any) -> bool:
    """
    Function that can be assigned to the retry_if_result paramter to
    tenacity.retry to conditionally retry if None is returned.
    """
    # https://github.com/jd/tenacity#whether-to-retry
    return value is None


def return_outcome_result(retry_state: tenacity.RetryCallState) -> Any:
    """
    Callback that can be assigned to the retry_error_callback parameter to
    tenacity.retry to return the last return value instead of raising RetryError
    when the retry stop condition is reached.
    """
    # https://github.com/jd/tenacity#custom-callbacks
    return None if retry_state.outcome is None else retry_state.outcome.result()

</document_content>
</document>
<document index="63">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/twitter_utils/__init__.py</source>
<document_content>

</document_content>
</document>
<document index="64">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/twitter_utils/client.py</source>
<document_content>
"""Twitter client utilities using gallery-dl."""
from __future__ import annotations

import re
import sys
import traceback

from collections.abc import Iterator
from datetime import datetime
from typing import Any, Dict, Final, List, Optional, TypedDict, cast, final

import structlog

from gallery_dl.extractor import twitter  # type: ignore


logger = structlog.get_logger(__name__)

from .types import TweetMetadata


class GalleryDLTweetItem(TypedDict, total=True):
    """Type definition for gallery-dl tweet item.

    Attributes:
        tweet_id: Unique identifier of the tweet
        tweet_url: Full URL of the tweet
        content: Tweet text content
        date: Tweet creation datetime
        author: Author information dictionary
        media: Optional list of media attachments
    """

    tweet_id: int
    tweet_url: str
    content: str
    date: datetime
    author: dict[str, str]
    media: list[dict[str, str]] | None


class GalleryDLError(Exception):
    """Base exception for gallery-dl related errors."""


class NoExtractorError(GalleryDLError):
    """Raised when no suitable extractor is found."""


class NoTweetDataError(GalleryDLError):
    """Raised when no tweet data is found."""


@final
class TwitterClient:
    """Client for Twitter interactions using gallery-dl.

    This class provides methods to extract and validate tweet metadata using
    gallery-dl as the backend. It handles authentication and configuration
    for gallery-dl's Twitter extractor.

    Attributes:
        auth_token: Twitter authentication token
        config: Gallery-dl configuration dictionary

    Example:
        >>> client = TwitterClient("your_auth_token")
        >>> metadata = client.get_tweet_metadata("https://twitter.com/user/status/123")
        >>> print(metadata["content"])
    """

    # URL patterns for tweet ID extraction
    URL_PATTERNS: Final[list[str]] = [
        r"twitter\.com/\w+/status/(\d+)",
        r"x\.com/\w+/status/(\d+)"
    ]

    # Gallery-dl configuration keys
    CONFIG_KEYS: Final[dict[str, Any]] = {
        "text-tweets": True,
        "include": "metadata",
        "videos": True
    }

    def __init__(self, auth_token: str) -> None:
        """Initialize Twitter client.

        Args:
            auth_token: Twitter authentication token for API access

        Example:
            >>> client = TwitterClient("your_auth_token")
        """
        self.auth_token = auth_token
        self.config = {
            "extractor": {
                "twitter": {
                    "cookies": {
                        "auth_token": auth_token
                    },
                    **self.CONFIG_KEYS
                }
            }
        }

    @staticmethod
    def extract_tweet_id(url: str) -> str | None:
        """Extract tweet ID from URL.

        Supports both twitter.com and x.com URLs in various formats.

        Args:
            url: Tweet URL to parse (twitter.com or x.com)

        Returns:
            Tweet ID if found, None otherwise

        Example:
            >>> TwitterClient.extract_tweet_id("https://twitter.com/user/status/123")
            '123'
            >>> TwitterClient.extract_tweet_id("invalid_url")
            None
        """
        for pattern in TwitterClient.URL_PATTERNS:
            if match := re.search(pattern, url):
                return match.group(1)
        return None

    def get_tweet_metadata(self, url: str) -> TweetMetadata:
        """Get metadata for a tweet.

        Fetches and parses metadata for a single tweet using gallery-dl.

        Args:
            url: Tweet URL to fetch metadata for

        Returns:
            Parsed tweet metadata

        Raises:
            NoExtractorError: If no suitable extractor is found
            NoTweetDataError: If no tweet data is found
            ValueError: If tweet cannot be parsed
            RuntimeError: If gallery-dl encounters an error

        Example:
            >>> metadata = client.get_tweet_metadata("https://twitter.com/user/status/123")
            >>> print(metadata["content"])
        """
        try:
            # Create extractor instance
            extractor = twitter.TwitterExtractor(url)

            # Configure extractor with our settings
            for key, value in self.config.get("extractor", {}).get("twitter", {}).items():
                setattr(extractor, key, value)

            # Get first (and should be only) item for single tweet
            items = list(extractor)
            if not items:
                raise NoTweetDataError(f"No tweet data found for URL: {url}")

            return self._parse_tweet_item(cast(GalleryDLTweetItem, items[0]))

        except Exception as e:
            logger.exception("Error fetching tweet metadata")
            if isinstance(e, (NoExtractorError, NoTweetDataError)):
                raise
            raise ValueError(f"Failed to fetch tweet: {e!s}") from e

    def get_thread_tweets(self, url: str) -> list[TweetMetadata]:
        """Get metadata for all tweets in a thread.

        Fetches and parses metadata for all tweets in a thread, starting from
        any tweet in the thread.

        Args:
            url: URL of any tweet in the thread

        Returns:
            List of tweet metadata for thread, sorted by creation date

        Raises:
            NoExtractorError: If no suitable extractor is found
            NoTweetDataError: If no tweets are found in thread
            ValueError: If thread cannot be parsed
            RuntimeError: If gallery-dl encounters an error

        Example:
            >>> thread = client.get_thread_tweets("https://twitter.com/user/status/123")
            >>> for tweet in thread:
            ...     print(tweet["content"])
        """
        try:
            extractor = twitter.TwitterExtractor(url)
            for key, value in self.config.get("extractor", {}).get("twitter", {}).items():
                setattr(extractor, key, value)

            thread_tweets = []
            for item in extractor:
                thread_tweets.append(
                    self._parse_tweet_item(cast(GalleryDLTweetItem, item))
                )

            if not thread_tweets:
                raise NoTweetDataError(f"No tweets found in thread at URL: {url}")

            # Sort by date
            return sorted(thread_tweets, key=lambda t: t["created_at"])

        except Exception as e:
            logger.exception("Error fetching thread")
            if isinstance(e, (NoExtractorError, NoTweetDataError)):
                raise
            raise ValueError(f"Failed to fetch thread: {e!s}") from e

    def _parse_tweet_item(self, item: GalleryDLTweetItem) -> TweetMetadata:
        """Parse gallery-dl tweet item into metadata.

        Converts a gallery-dl tweet item into our standardized metadata format.

        Args:
            item: Gallery-dl tweet item to parse

        Returns:
            Parsed tweet metadata in standardized format

        Raises:
            ValueError: If required fields are missing

        Example:
            >>> item = {"tweet_id": 123, "content": "Hello", ...}
            >>> metadata = client._parse_tweet_item(item)
            >>> print(metadata["content"])
        """
        try:
            media_urls: list[str] = []
            if item.get("media"):
                for media in item["media"]:
                    if url := media.get("url"):
                        media_urls.append(url)

            return {
                "id": str(item["tweet_id"]),
                "url": item["tweet_url"],
                "author": item["author"]["name"],
                "content": item["content"],
                "media_urls": media_urls,
                "created_at": item["date"].isoformat()
            }
        except KeyError as e:
            raise ValueError(f"Missing required field in tweet data: {e}") from e

    def validate_tweet(self, url: str) -> bool:
        """Check if a tweet URL exists and is accessible.

        Attempts to fetch tweet metadata to verify if the URL is valid and
        accessible with the current authentication.

        Args:
            url: Tweet URL to validate

        Returns:
            True if tweet exists and is accessible

        Example:
            >>> if client.validate_tweet("https://twitter.com/user/status/123"):
            ...     print("Tweet exists!")
        """
        try:
            self.get_tweet_metadata(url)
            return True
        except (NoExtractorError, NoTweetDataError, ValueError):
            return False

</document_content>
</document>
<document index="65">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/twitter_utils/compression.py</source>
<document_content>
"""Media compression utilities."""
from __future__ import annotations

import asyncio
import pathlib

from typing import Final, Optional

import structlog


logger = structlog.get_logger(__name__)

from democracy_exe.shell import ProcessException, ShellConsole, run_coroutine_subprocess


# Constants for Discord upload limits
MAX_DISCORD_SIZE: Final[int] = 8_388_608  # 8MB in bytes
MAX_DISCORD_NITRO_SIZE: Final[int] = 52_428_800  # 50MB in bytes

# Valid file extensions
VIDEO_EXTENSIONS: Final[set[str]] = {".mp4", ".avi", ".mkv", ".mov", ".flv", ".wmv", ".webm", ".mpeg", ".3gp"}
AUDIO_EXTENSIONS: Final[set[str]] = {".mp3", ".wav", ".m4a", ".flac", ".aac", ".ogg", ".wma"}

class CompressionError(Exception):
    """Base exception for compression-related errors."""


async def compress_media(
    file_path: str | pathlib.Path,
    max_size: int = MAX_DISCORD_SIZE,
    script_path: str | None = None
) -> str:
    """Compress media file to meet Discord size requirements.

    Args:
        file_path: Path to media file
        max_size: Maximum allowed file size in bytes
        script_path: Optional path to compression script

    Returns:
        Path to compressed file

    Raises:
        CompressionError: If compression fails
    """
    file_path = pathlib.Path(file_path)
    if not file_path.exists():
        raise CompressionError(f"File not found: {file_path}")

    # Check if compression is needed
    if file_path.stat().st_size <= max_size:
        return str(file_path)

    # Find compression script
    if script_path is None:
        script_path = pathlib.Path(__file__).parent.parent.parent / "scripts" / "compress-discord.sh"

    if not pathlib.Path(script_path).exists():
        raise CompressionError(f"Compression script not found: {script_path}")

    try:
        # Make script executable
        script_path = pathlib.Path(script_path)
        script_path.chmod(0o755) # type: ignore

        # Run compression script using shell utility
        cmd = f"{script_path} {file_path}"
        working_dir = str(file_path.parent)

        ShellConsole.message(f"Compressing file: {file_path}")
        await run_coroutine_subprocess(cmd, "file://" + str(file_path), working_dir)

        # Find compressed output file
        output_dir = file_path.parent
        compressed_name = f"25MB_{file_path.stem}"

        # Check for both mp4 and mp3 extensions
        compressed_path = output_dir / f"{compressed_name}.mp4"
        if not compressed_path.exists():
            compressed_path = output_dir / f"{compressed_name}.mp3"

        if not compressed_path.exists():
            raise CompressionError("Compressed output file not found")

        logger.info(f"Successfully compressed {file_path} to {compressed_path}")
        return str(compressed_path)

    except ProcessException as e:
        logger.exception("Compression failed")
        raise CompressionError(f"Failed to compress {file_path}: {e!s}") from e

def is_compressible(file_path: str | pathlib.Path) -> bool:
    """Check if file can be compressed.

    Args:
        file_path: Path to check

    Returns:
        True if file extension is supported
    """
    suffix = pathlib.Path(file_path).suffix.lower()
    return suffix in VIDEO_EXTENSIONS or suffix in AUDIO_EXTENSIONS

</document_content>
</document>
<document index="66">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/twitter_utils/download.py</source>
<document_content>
"""Utilities for downloading Twitter content."""

from __future__ import annotations

import asyncio
import glob
import json
import logging
import os
import pathlib
import sys
import tempfile
import traceback

from typing import Any, Dict, List, Optional

import aiofiles
import aiohttp
import rich
import structlog


logger = structlog.get_logger(__name__)
from tqdm.auto import tqdm

from democracy_exe import shell
from democracy_exe.constants import DL_SAFE_TWITTER_COMMAND, DL_TWITTER_CARD_COMMAND, DL_TWITTER_THREAD_COMMAND
from democracy_exe.utils.file_functions import filter_media, tree

from .models import TweetInfo
from .types import DownloadResult, TweetDownloadMode, TweetMetadata


def _get_media_urls(work_dir: str | pathlib.Path) -> list[str]:
    """Get media URLs from gallery-dl json format.

    Args:
        data: Gallery-dl json format

    Returns:
        List of media URLs
    """
    # Get downloaded files
    tree_list = tree(pathlib.Path(work_dir))
    # import bpdb
    # bpdb.set_trace()
    files = [str(p) for p in tree_list]
    media_files = filter_media(files)

    logger.info(f"tree_list: {tree_list}")
    logger.info(f"files: {files}")
    logger.info(f"media_files: {media_files}")

    return media_files


async def download_media(url: str, download_dir: str) -> list[str]:
    """Download media from Twitter URL.

    Args:
        url: Twitter URL
        download_dir: Directory to save files

    Returns:
        List of downloaded file paths
    """
    # Download implementation
    pass

async def download_thumbnail(url: str, path: str) -> str | None:
    """Download thumbnail image.

    Args:
        url: Image URL
        path: Save path

    Returns:
        Path to downloaded thumbnail or None
    """
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as resp:
                if resp.status == 200:
                    content = await resp.read()
                    file_path = pathlib.Path(path)
                    file_path.write_bytes(content)
                    return str(file_path)
    except Exception as e:
        logger.error(f"Error downloading thumbnail: {e}")
        return None


async def download_tweet(
    url: str,
    mode: TweetDownloadMode = "single",
    working_dir: str | None = None,
) -> DownloadResult:
    """Download content from a tweet URL.

    Args:
        url: The tweet URL to download from
        mode: The download mode - single tweet, thread, or card
        working_dir: Optional working directory for downloads

    Returns:
        DownloadResult containing success status, metadata and local files
    """
    # Use temporary dir if none provided
    with tempfile.TemporaryDirectory(delete=False) as tmpdirname:
        work_dir = working_dir or tmpdirname
        logger.info(f"work_dir: {work_dir}")

        try:
            # Select command based on mode
            cmd = {
                "single": DL_SAFE_TWITTER_COMMAND,
                "thread": DL_TWITTER_THREAD_COMMAND,
                "card": DL_TWITTER_CARD_COMMAND,
            }[mode]

            logger.info(f"cmd: {cmd}")
            logger.info(f"url: {url}")
            logger.info(f"work_dir: {work_dir}")

            # Execute download command
            await shell._aio_run_process_and_communicate(
                cmd.format(dl_uri=url).split(),
                cwd=work_dir
            )

            # Get downloaded files
            tree_list = tree(pathlib.Path(work_dir))
            # import bpdb
            # bpdb.set_trace()
            files = [str(p) for p in tree_list]
            media_files = filter_media(files)

            logger.info(f"tree_list: {tree_list}")
            logger.info(f"files: {files}")
            logger.info(f"media_files: {media_files}")

            # import bpdb
            # bpdb.set_trace()

            # Parse metadata from info.json
            # metadata = _parse_tweet_metadata(work_dir, url=url)
            metadata = await _a_parse_tweet_metadata(work_dir, url=url)

            logger.info(f"metadata: {metadata}")

            return DownloadResult(
                success=True,
                metadata=metadata,
                local_files=media_files,
                error=None
            )

        except Exception as e:
            logger.exception(f"Error downloading tweet: {e}")
            print(f"{e}")
            exc_type, exc_value, exc_traceback = sys.exc_info()
            print(f"Error Class: {e.__class__}")
            output = f"[UNEXPECTED] {type(e).__name__}: {e}"
            print(output)
            print(f"exc_type: {exc_type}")
            print(f"exc_value: {exc_value}")
            traceback.print_tb(exc_traceback)
            return DownloadResult(
                success=False,
                metadata={
                    "id": "",
                    "url": url,
                    "author": "",
                    "content": "",
                    "media_urls": [],
                    "created_at": "",
                },
                local_files=[],
                error=str(e)
            )
async def adownload_tweet(
    url: str,
    mode: TweetDownloadMode = "single",
    working_dir: str | None = None,
) -> DownloadResult:
    """Download content from a tweet URL.

    Args:
        url: The tweet URL to download from
        mode: The download mode - single tweet, thread, or card
        working_dir: Optional working directory for downloads

    Returns:
        DownloadResult containing success status, metadata and local files
    """
    # Use temporary dir if none provided
    with tempfile.TemporaryDirectory(delete=False) as tmpdirname:
        work_dir = working_dir or tmpdirname
        logger.info(f"work_dir: {work_dir}")

        try:
            # Select command based on mode
            cmd = {
                "single": DL_SAFE_TWITTER_COMMAND,
                "thread": DL_TWITTER_THREAD_COMMAND,
                "card": DL_TWITTER_CARD_COMMAND,
            }[mode]

            logger.info(f"cmd: {cmd}")
            logger.info(f"url: {url}")
            logger.info(f"work_dir: {work_dir}")

            # Execute download command
            await shell._aio_run_process_and_communicate(
                cmd.format(dl_uri=url).split(),
                cwd=work_dir
            )

            # Get downloaded files
            tree_list = tree(pathlib.Path(work_dir))
            # import bpdb
            # bpdb.set_trace()
            files = [str(p) for p in tree_list]
            media_files = filter_media(files)

            logger.info(f"tree_list: {tree_list}")
            logger.info(f"files: {files}")
            logger.info(f"media_files: {media_files}")

            # import bpdb
            # bpdb.set_trace()

            # Parse metadata from info.json
            # metadata = _parse_tweet_metadata(work_dir, url=url)
            metadata = await _a_parse_tweet_metadata(work_dir, url=url)

            logger.info(f"metadata: {metadata}")

            return DownloadResult(
                success=True,
                metadata=metadata,
                local_files=media_files,
                error=None
            )

        except Exception as e:
            logger.exception(f"Error downloading tweet: {e}")
            print(f"{e}")
            exc_type, exc_value, exc_traceback = sys.exc_info()
            print(f"Error Class: {e.__class__}")
            output = f"[UNEXPECTED] {type(e).__name__}: {e}"
            print(output)
            print(f"exc_type: {exc_type}")
            print(f"exc_value: {exc_value}")
            traceback.print_tb(exc_traceback)
            return DownloadResult(
                success=False,
                metadata={
                    "id": "",
                    "url": url,
                    "author": "",
                    "content": "",
                    "media_urls": [],
                    "created_at": "",
                },
                local_files=[],
                error=str(e)
            )


def _parse_tweet_metadata(work_dir: str, url: str | None = None) -> TweetMetadata:
    """
    Recursively search for and parse tweet metadata from gallery-dl info.json file.

    Args:
        work_dir: Directory to start searching for info.json file

    Returns:
        Dictionary containing tweet metadata
    """
    def find_info_json(directory: pathlib.Path) -> pathlib.Path | None:
        """Recursively search for info.json file.

        Args:
            directory: Starting directory for search

        Returns:
            Path to info.json if found, None otherwise
        """
        logger.info(f"Searching for info.json in: {directory}")

        # Check if directory exists
        if not directory.exists():
            logger.warning(f"Directory does not exist: {directory}")
            return None

        # Walk through directory tree
        for root, dirs, files in os.walk(directory):
            logger.debug(f"Checking directory: {root}")
            logger.debug(f"Found files: {files}")

            if 'info.json' in files:
                _info_path = pathlib.Path(root) / 'info.json'
                logger.info(f"Found info.json at: {_info_path}")
                return _info_path

        logger.warning(f"No info.json found in {directory} or its subdirectories")
        return None

    info_path = find_info_json(pathlib.Path(work_dir))

    logger.info(f"info_path: {info_path}")
    logger.info(f"info_path.exists(): {info_path.exists() if info_path else False}")
    logger.info(f"info_path is None: {info_path is None}")

    if info_path is None or not info_path.exists():
        logger.warning(f"No info.json found in {work_dir} or its subdirectories")
        return {
            "id": "",
            "url": "",
            "author": "",
            "content": "",
            "media_urls": [],
            "created_at": "",
        }

    try:
        logger.info("Attempting to load info.json")
        import json
        with open(f"{info_path}", encoding="utf-8") as f:
            data = json.load(f)

        logger.debug(f"data: {data}")
        data_model = TweetInfo(**data)
        media_files = _get_media_urls(work_dir)

        res = TweetMetadata(
            id=str(data_model.tweet_id),
            url=url,
            author= data_model.author.name, # pylint: disable=no-member
            content=data_model.content,
            media_urls=media_files,
            created_at=data_model.date,
        )

        logger.debug(f"res: {res}")

        return res


    except (json.decoder.JSONDecodeError, FileNotFoundError, KeyError) as ex:
        logger.exception(f"Error parsing tweet metadata: {ex}")
        # print(ex)
        # exc_type, exc_value, exc_traceback = sys.exc_info()
        # logger.error(f"Error Class: {ex.__class__!s}")
        # output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
        # logger.warning(output)
        # logger.error(f"exc_type: {exc_type}")
        # logger.error(f"exc_value: {exc_value}")
        # traceback.print_tb(exc_traceback)

        return {
            "id": "",
            "url": "",
            "author": "",
            "content": "",
            "media_urls": [],
            "created_at": "",
        }



async def _a_find_info_json(directory: pathlib.Path) -> pathlib.Path | None:
    """Recursively search for info.json file asynchronously.

    Args:
        directory: Starting directory for search

    Returns:
        Path to info.json if found, None otherwise
    """
    logger.info(f"Searching for info.json in: {directory}")

    # Check if directory exists
    if not directory.exists():
        logger.warning(f"Directory does not exist: {directory}")
        return None

    # Walk through directory tree
    for root, dirs, files in os.walk(directory):
        logger.debug(f"Checking directory: {root}")
        logger.debug(f"Found files: {files}")

        if 'info.json' in files:
            _info_path = pathlib.Path(root) / 'info.json'
            logger.info(f"Found info.json at: {_info_path}")
            return _info_path

    logger.warning(f"No info.json found in {directory} or its subdirectories")
    return None

async def _a_parse_tweet_metadata(work_dir: str, url: str | None = None) -> TweetMetadata:
    """Recursively search for and parse tweet metadata from gallery-dl info.json file asynchronously.

    Args:
        work_dir: Directory to start searching for info.json file
        url: Optional URL of the tweet

    Returns:
        Dictionary containing tweet metadata
    """
    info_path = await _a_find_info_json(pathlib.Path(work_dir))

    logger.info(f"info_path: {info_path}")
    logger.info(f"info_path.exists(): {info_path.exists() if info_path else False}")
    logger.info(f"info_path is None: {info_path is None}")

    if info_path is None:
        logger.warning(f"No info.json found in {work_dir} or its subdirectories")
        return {
            "id": "",
            "url": "",
            "author": "",
            "content": "",
            "media_urls": [],
            "created_at": "",
        }

    try:
        logger.info("Attempting to load info.json")
        async with aiofiles.open(str(info_path), encoding="utf-8") as f:
            content = await f.read()
            data = json.loads(content)

        logger.debug(f"data: {data}")
        data_model = TweetInfo(**data)
        media_files = _get_media_urls(work_dir)

        res = TweetMetadata(
            id=str(data_model.tweet_id),
            url=url,
            author=data_model.author.name,  # pylint: disable=no-member
            content=data_model.content,
            media_urls=media_files,
            created_at=data_model.date,
        )

        logger.debug(f"res: {res}")

        return res

    except (json.decoder.JSONDecodeError, FileNotFoundError, KeyError) as ex:
        logger.exception(f"Error parsing tweet metadata: {ex}")
        return {
            "id": "",
            "url": "",
            "author": "",
            "content": "",
            "media_urls": [],
            "created_at": "",
        }

</document_content>
</document>
<document index="67">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/twitter_utils/embed.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
"""Discord embed utilities for Twitter content."""
from __future__ import annotations

from typing import Any, Dict, Final, List, Optional, TypedDict, Union

import discord
import structlog


logger = structlog.get_logger(__name__)

from democracy_exe.utils.twitter_utils.types import DownloadResult, TweetDownloadMode


# Color constants
BLUE: Final[discord.Color] = discord.Color.blue()
GOLD: Final[discord.Color] = discord.Color.gold()
RED: Final[discord.Color] = discord.Color.red()


class TweetMetadata(TypedDict, total=False):
    """Type hints for tweet metadata."""

    id: str
    author: str
    content: str
    created_at: str
    url: str
    media_urls: list[str]
    retweet_count: int
    like_count: int
    reply_count: int
    card_url: str
    card_description: str
    card_image: str


def _add_optional_field(embed: discord.Embed, metadata: dict[str, Any], field: str, *, display_name: str | None = None, inline: bool = True) -> None:
    """Add field to embed if it exists in metadata.

    Args:
        embed: Discord embed to modify
        metadata: Metadata dictionary
        field: Field name to check and add
        display_name: Optional display name override
        inline: Whether the field should be inline
    """
    if field in metadata:
        name = display_name if display_name is not None else field.replace("_", " ").title()
        embed.add_field(name=name, value=metadata[field], inline=inline)


def _add_media_field(embed: discord.Embed, metadata: dict[str, Any]) -> None:
    """Add media URLs field to embed if present.

    Args:
        embed: Discord embed to modify
        metadata: Metadata dictionary
    """
    if metadata.get("media_urls"):
        media_list = "\n".join(metadata["media_urls"])
        embed.add_field(name="Media URLs", value=media_list, inline=False)


def create_tweet_embed(metadata: TweetMetadata) -> discord.Embed:
    """Create a Discord embed for a single tweet.

    Args:
        metadata: Tweet metadata dictionary containing author, content, etc.

    Returns:
        Discord embed object
    """
    embed = discord.Embed(title="Tweet", color=BLUE)
    embed.set_author(name=metadata["author"])
    embed.add_field(name="Created", value=metadata["created_at"], inline=True)

    _add_optional_field(embed, metadata, "url", display_name="URL")

    if "content" in metadata:
        embed.description = metadata["content"] # type: ignore

    _add_media_field(embed, metadata)

    return embed


def create_thread_embed(metadata_list: list[TweetMetadata]) -> discord.Embed:
    """Create a Discord embed for a tweet thread.

    Args:
        metadata_list: List of tweet metadata dictionaries

    Returns:
        Discord embed object
    """
    if not metadata_list:
        return discord.Embed(title="Empty Thread", description="No tweets found", color=BLUE)

    # Use first tweet as main content
    main_tweet = metadata_list[0]
    embed = discord.Embed(title="Tweet Thread", color=BLUE)
    embed.set_author(name=main_tweet["author"])

    # Add thread content
    thread_content = []
    for i, tweet in enumerate(metadata_list, 1):
        thread_content.append(f"**Tweet {i}:**\n{tweet.get('content', 'No content')}\n")

    embed.description = "\n".join(thread_content) # type: ignore

    # Add metadata
    embed.add_field(name="Thread Length", value=str(len(metadata_list)), inline=True)
    embed.add_field(name="Created", value=main_tweet["created_at"], inline=True)

    _add_optional_field(embed, main_tweet, "url", inline=False)

    return embed


def create_card_embed(metadata: TweetMetadata) -> discord.Embed:
    """Create a Discord embed for a tweet card preview.

    Args:
        metadata: Tweet metadata dictionary

    Returns:
        Discord embed object
    """
    embed = discord.Embed(title="Tweet Card", color=BLUE)
    embed.set_author(name=metadata["author"])

    if "content" in metadata:
        embed.description = metadata["content"] # type: ignore

    _add_optional_field(embed, metadata, "card_url", display_name="Card URL", inline=False)
    _add_optional_field(embed, metadata, "card_description", display_name="Description", inline=False)

    if metadata.get("card_image"):
        embed.set_image(url=metadata["card_image"])
    else:
        embed.set_image(url=None)

    return embed


def create_info_embed(metadata: TweetMetadata) -> discord.Embed:
    """Create a detailed info embed for a tweet.

    Args:
        metadata: Tweet metadata dictionary

    Returns:
        Discord embed object
    """
    embed = discord.Embed(title="Tweet Information", color=BLUE)

    # Add basic metadata
    embed.add_field(name="ID", value=metadata.get("id", "Unknown"), inline=True)
    embed.add_field(name="Author", value=metadata.get("author", "Unknown"), inline=True)
    embed.add_field(name="Created", value=metadata.get("created_at", "Unknown"), inline=True)

    _add_optional_field(embed, metadata, "url", display_name="URL", inline=False)

    if "content" in metadata:
        embed.description = metadata["content"] # type: ignore

    _add_media_field(embed, metadata)

    # Add engagement metrics
    for field in ["retweet_count", "like_count", "reply_count"]:
        _add_optional_field(embed, metadata, field)

    return embed


def create_download_progress_embed(url: str, mode: TweetDownloadMode) -> discord.Embed:
    """Create a progress embed for tweet download.

    Args:
        url: Tweet URL being downloaded
        mode: Download mode (single/thread/card)

    Returns:
        Discord embed object
    """
    embed = discord.Embed(
        title="Download in Progress",
        description=f"Downloading tweet {mode} from {url}...",
        color=GOLD
    )
    embed.set_footer(text="Please wait...")
    return embed


def create_error_embed(error: str) -> discord.Embed:
    """Create an error embed.

    Args:
        error: Error message

    Returns:
        Discord embed object
    """
    embed = discord.Embed(
        title="Error",
        description=str(error),
        color=RED
    )
    return embed

</document_content>
</document>
<document index="68">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/twitter_utils/models.py</source>
<document_content>
"""Data models for Twitter content."""
from __future__ import annotations

from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, TypedDict, Union

from pydantic import BaseModel, Field, field_validator
from pydantic._internal import _utils

from democracy_exe.utils.twitter_utils.types import TweetDownloadMode


class TweetDict(TypedDict, total=False):
    """Type hints for tweet dictionary representation."""

    id: str
    author: str
    content: str
    created_at: str
    url: str
    media_urls: list[str]
    retweet_count: int
    like_count: int
    reply_count: int
    card_url: str
    card_description: str
    card_image: str


class ThreadDict(TypedDict):
    """Type hints for thread dictionary representation."""

    author: str
    created_at: str
    tweets: list[TweetDict]


class DownloadDict(TypedDict, total=False):
    """Type hints for download result dictionary representation."""

    success: bool
    mode: str
    local_files: list[str]
    error: str
    metadata: TweetDict | ThreadDict


class MediaType(str, Enum):
    """Types of media that can be attached to tweets."""

    IMAGE = "image"
    VIDEO = "video"
    GIF = "gif"
    AUDIO = "audio"

    @classmethod
    def from_mime_type(cls, mime_type: str) -> MediaType:
        """Get media type from MIME type.

        Args:
            mime_type: MIME type string

        Returns:
            Corresponding media type

        Raises:
            ValueError: If MIME type is not supported
        """
        # Handle exact matches first
        if mime_type == "image/gif":
            return MediaType.GIF

        # Then handle prefix matches
        mime_map = {
            "image/": MediaType.IMAGE,
            "video/": MediaType.VIDEO,
            "audio/": MediaType.AUDIO,
        }
        for mime_prefix, media_type in mime_map.items():
            if mime_type.startswith(mime_prefix):
                return media_type
        raise ValueError(f"Unsupported MIME type: {mime_type}")


@dataclass
class MediaItem:
    """Represents a media item from a tweet.

    Attributes:
        url: Original URL of the media
        type: Type of media (image, video, etc.)
        local_path: Path where media is stored locally
        size: Size in bytes
        width: Width in pixels (if applicable)
        height: Height in pixels (if applicable)
        duration: Duration in seconds (if applicable)
    """

    url: str
    type: MediaType
    local_path: Path | None = None
    size: int | None = None
    width: int | None = None
    height: int | None = None
    duration: float | None = None

    @property
    def is_downloaded(self) -> bool:
        """Check if media has been downloaded.

        Returns:
            True if media has been downloaded
        """
        return self.local_path is not None and self.local_path.exists()

    @property
    def dimensions(self) -> tuple[int, int] | None:
        """Get media dimensions if available.

        Returns:
            Tuple of (width, height) if available
        """
        if self.width is not None and self.height is not None:
            return (self.width, self.height)
        return None


@dataclass
class TweetCard:
    """Represents a Twitter card (e.g. for links).

    Attributes:
        url: URL the card points to
        title: Card title
        description: Card description
        image_url: URL of card image
        local_image: Local path to downloaded card image
    """

    url: str
    title: str
    description: str | None = None
    image_url: str | None = None
    local_image: Path | None = None

    @property
    def has_image(self) -> bool:
        """Check if card has an image.

        Returns:
            True if card has a non-empty image URL
        """
        return bool(self.image_url)

    @property
    def is_downloaded(self) -> bool:
        """Check if card image has been downloaded.

        Returns:
            True if image has been downloaded
        """
        return self.local_image is not None and self.local_image.exists()


@dataclass
class Tweet:
    """Represents a single tweet.

    Attributes:
        id: Tweet ID
        author: Author's username
        content: Tweet text content
        created_at: Creation timestamp
        url: URL to the tweet
        media: List of media items
        card: Twitter card if present
        retweet_count: Number of retweets
        like_count: Number of likes
        reply_count: Number of replies
        quoted_tweet: Quoted tweet if present
    """

    id: str
    author: str
    content: str
    created_at: datetime
    url: str
    media: list[MediaItem] = field(default_factory=list)
    card: TweetCard | None = None
    retweet_count: int = 0
    like_count: int = 0
    reply_count: int = 0
    quoted_tweet: Tweet | None = None

    @property
    def has_media(self) -> bool:
        """Check if tweet has media attachments.

        Returns:
            True if tweet has media
        """
        return bool(self.media and len(self.media) > 0)

    @property
    def has_card(self) -> bool:
        """Check if tweet has a card.

        Returns:
            True if tweet has a card
        """
        return self.card is not None

    @property
    def is_quote(self) -> bool:
        """Check if tweet is a quote tweet.

        Returns:
            True if tweet is a quote
        """
        return self.quoted_tweet is not None

    def to_dict(self) -> TweetDict:
        """Convert tweet to dictionary format.

        Returns:
            Dictionary representation of tweet
        """
        result: TweetDict = {
            "id": self.id,
            "author": self.author,
            "content": self.content,
            "created_at": self.created_at.isoformat(),
            "url": self.url,
            "retweet_count": self.retweet_count,
            "like_count": self.like_count,
            "reply_count": self.reply_count,
        }

        if self.media:
            result["media_urls"] = [m.url for m in self.media]

        if self.card:
            result["card_url"] = self.card.url
            result["card_description"] = self.card.description
            if self.card.image_url:
                result["card_image"] = self.card.image_url

        return result


@dataclass
class TweetThread:
    """Represents a thread of tweets.

    Attributes:
        tweets: List of tweets in the thread
        author: Thread author
        created_at: Creation time of first tweet
    """

    tweets: list[Tweet]
    author: str
    created_at: datetime

    @property
    def length(self) -> int:
        """Get number of tweets in thread.

        Returns:
            Number of tweets
        """
        return len(self.tweets) if self.tweets else 0

    @property
    def first_tweet(self) -> Tweet | None:
        """Get first tweet in thread.

        Returns:
            First tweet if thread is not empty
        """
        return self.tweets[0] if self.tweets else None

    @property
    def last_tweet(self) -> Tweet | None:
        """Get last tweet in thread.

        Returns:
            Last tweet if thread is not empty
        """
        return self.tweets[-1] if self.tweets else None

    def to_dict(self) -> ThreadDict:
        """Convert thread to dictionary format.

        Returns:
            Dictionary representation of thread
        """
        return ThreadDict(
            author=self.author,
            created_at=self.created_at.isoformat(),
            tweets=[t.to_dict() for t in self.tweets],
        )


@dataclass
class DownloadedContent:
    """Represents downloaded Twitter content.

    Attributes:
        mode: Download mode used
        content: Downloaded tweet or thread
        local_files: List of local file paths
        error: Error message if download failed
    """

    mode: TweetDownloadMode
    content: Tweet | TweetThread | None = None
    local_files: list[Path] = field(default_factory=list)
    error: str | None = None

    @property
    def success(self) -> bool:
        """Check if download was successful.

        Returns:
            True if content was downloaded successfully
        """
        return self.content is not None and not self.error

    @property
    def has_files(self) -> bool:
        """Check if content has local files.

        Returns:
            True if there are local files, False if files list is empty or None
        """
        return bool(self.local_files and len(self.local_files) > 0)

    def to_dict(self) -> DownloadDict:
        """Convert download result to dictionary format.

        Returns:
            Dictionary representation of download result
        """
        result: DownloadDict = {
            "success": self.success,
            "mode": self.mode,
            "local_files": [str(f) for f in self.local_files],
        }

        if self.error:
            result["error"] = self.error
        elif self.content:
            result["metadata"] = self.content.to_dict()

        return result




class TwitterUser(BaseModel):
    """Model representing a Twitter user's profile data."""
    id: int | Any = Field(description="The unique identifier for the user")
    name: str = Field(description="The user's screen name/handle")
    nick: str = Field(description="The user's display name")
    location: str | None = Field(None, description="The user's location")
    date: datetime | str | Any = Field(description="The date the user's account was created")
    verified: bool = Field(False, description="Whether the user is verified")
    protected: bool = Field(False, description="Whether the user's tweets are protected")
    profile_banner: str | None = Field(None, description="URL to the user's profile banner image")
    profile_image: str = Field(description="URL to the user's profile image")
    favourites_count: int | Any = Field(description="Number of tweets the user has liked")
    followers_count: int | Any = Field(description="Number of followers the user has")
    friends_count: int | Any = Field(description="Number of users the user is following")
    listed_count: int | Any = Field(description="Number of lists the user is a member of")
    media_count: int | Any = Field(description="Number of media items the user has posted")
    statuses_count: int | Any = Field(description="Total number of tweets by the user")
    description: str = Field(description="The user's profile description/bio")
    url: str | None = Field(None, description="The user's website URL")

    @field_validator('date', mode='before')
    @classmethod
    def parse_date(cls, v: Any) -> Any:
        """Parse date strings that start with 'dt:' prefix.

        Args:
            v: The value to parse

        Returns:
            Parsed datetime or original value
        """
        if isinstance(v, str):
            if v.startswith('dt:'):
                return datetime.fromisoformat(v[3:])
        return v


class TwitterMention(BaseModel):
    """Model representing a user mention in a tweet."""
    id: int | Any = Field(description="The unique identifier of the mentioned user")
    name: str = Field(description="The screen name of the mentioned user")
    nick: str = Field(description="The display name of the mentioned user")


class TweetInfo(BaseModel):
    """Model representing a Twitter tweet info.json file ."""
    tweet_id: int | Any = Field(description="The unique identifier of the tweet")
    retweet_id: int | Any = Field(0, description="The ID of the original tweet if this is a retweet")
    quote_id: int | Any = Field(0, description="The ID of the quoted tweet if this is a quote tweet")
    reply_id: int | Any | None = Field(None, description="The ID of the tweet this is replying to")
    conversation_id: int | Any = Field(description="The ID of the conversation thread")
    date: datetime | str | Any = Field(description="The timestamp when the tweet was created")
    author: TwitterUser = Field(description="The user who authored the tweet")
    user: TwitterUser = Field(description="The user in whose timeline this tweet appears")
    lang: str = Field(description="The language code of the tweet")
    source: str = Field(description="The application used to post the tweet")
    sensitive: bool = Field(False, description="Whether the tweet contains sensitive content")
    favorite_count: int | Any = Field(0, description="Number of likes the tweet has received")
    quote_count: int | Any = Field(0, description="Number of times the tweet has been quoted")
    reply_count: int | Any = Field(0, description="Number of replies to the tweet")
    retweet_count: int | Any = Field(0, description="Number of times the tweet has been retweeted")
    bookmark_count: int | Any = Field(0, description="Number of times the tweet has been bookmarked")
    view_count: int | Any = Field(0, description="Number of times the tweet has been viewed")
    mentions: list[TwitterMention] | None = Field(None, description="List of users mentioned in the tweet")
    content: str = Field(description="The text content of the tweet")
    reply_to: str | None = Field(None, description="The screen name of the user being replied to")
    count: int | Any | None = Field(None, description="Number of media items in the tweet")
    category: str = Field("twitter", description="The category identifier for the tweet")
    subcategory: str = Field(description="The subcategory identifier for the tweet")
    type: str | None = Field(None, description="Type of media in the tweet")
    date_original: datetime | str | None = Field(None, description="Original tweet date for retweets")
    birdwatch: str | None = Field(None, description="Birdwatch note content if present")
    description: str | None = Field(None, description="Media description/alt text")

    @field_validator('date', 'date_original', mode='before')
    @classmethod
    def parse_dates(cls, v: Any) -> Any:
        """Parse date strings that start with 'dt:' prefix.

        Args:
            v: The value to parse

        Returns:
            Parsed datetime or original value
        """
        if isinstance(v, str):
            if v.startswith('dt:'):
                return datetime.fromisoformat(v[3:])
        return v

    class Config:
        """Pydantic config."""
        extra = "allow"  # Allow extra fields that might be present in test data


# from datetime import datetime
# from typing import List, Optional, Union

# from pydantic import BaseModel, Field


# class TweetMetadata(BaseModel):
#     """Model representing metadata for a tweet."""
#     id: Optional[str] = Field(None, description="The unique identifier of the tweet")
#     url: Optional[str] = Field(None, description="The URL of the tweet")
#     author: Optional[str] = Field(None, description="The author of the tweet")
#     content: Optional[str] = Field(None, description="The text content of the tweet")
#     media_urls: Optional[List[str]] = Field(default_factory=list, description="List of media URLs in the tweet")
#     created_at: Optional[str] = Field(None, description="The timestamp when the tweet was created")


# class TweetMetadataReturnModel(BaseModel):
#     """Model representing the return value for tweet metadata operations."""
#     success: bool = Field(True, description="Whether the operation was successful")
#     metadata: Optional[TweetMetadata] = Field(None, description="The tweet metadata if available")
#     local_files: Optional[List[str]] = Field(default_factory=list, description="List of local file paths")
#     error: Optional[str] = Field(None, description="Error message if operation failed")

#     class Config:
#         """Pydantic config."""
#         arbitrary_types_allowed = True

</document_content>
</document>
<document index="69">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/twitter_utils/storage.py</source>
<document_content>
"""Storage utilities for Twitter content."""
from __future__ import annotations

import asyncio
import os
import shutil
import tempfile

from collections.abc import AsyncGenerator
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Final, List, Optional, Set, TypedDict, Union

import structlog


logger = structlog.get_logger(__name__)

from democracy_exe.utils.twitter_utils.models import (
    DownloadedContent,
    MediaItem,
    MediaType,
    Tweet,
    TweetCard,
    TweetThread,
)


# Constants
MAX_CACHE_AGE: Final[timedelta] = timedelta(hours=24)
MAX_CACHE_SIZE: Final[int] = 1024 * 1024 * 1024  # 1GB
CHUNK_SIZE: Final[int] = 8192  # 8KB chunks for file operations


class StorageStats(TypedDict):
    """Type hints for storage statistics."""

    total_size: int
    file_count: int
    oldest_file: datetime | None
    newest_file: datetime | None


class StorageError(Exception):
    """Base exception for storage-related errors."""


class FileNotFoundError(StorageError):
    """Raised when a file is not found."""


class StorageFullError(StorageError):
    """Raised when storage is full."""


class TwitterMediaStorage:
    """Handles storage of Twitter media files.

    This class manages both temporary and persistent storage of downloaded
    media files, including cleanup of old files and directory structure.

    Attributes:
        base_dir: Base directory for media storage
        temp_dir: Directory for temporary files
        max_cache_age: Maximum age of cached files
        max_cache_size: Maximum size of cache in bytes
    """

    def __init__(
        self,
        base_dir: str | Path | None = None,
        max_cache_age: timedelta = MAX_CACHE_AGE,
        max_cache_size: int = MAX_CACHE_SIZE,
    ) -> None:
        """Initialize storage manager.

        Args:
            base_dir: Base directory for media storage
            max_cache_age: Maximum age of cached files
            max_cache_size: Maximum size of cache in bytes
        """
        if base_dir is None:
            base_dir = Path.home() / ".democracy_exe" / "twitter_media"
        self.base_dir = Path(base_dir)
        self.temp_dir = self.base_dir / "temp"
        self.max_cache_age = max_cache_age
        self.max_cache_size = max_cache_size

        # Create directories
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.temp_dir.mkdir(parents=True, exist_ok=True)

    async def get_storage_stats(self) -> StorageStats:
        """Get storage statistics.

        Returns:
            Dictionary with storage statistics
        """
        total_size = 0
        file_count = 0
        oldest_time: float | None = None
        newest_time: float | None = None

        for path in self.temp_dir.rglob("*"):
            if path.is_file():
                stat = path.stat()
                total_size += stat.st_size
                file_count += 1
                mtime = stat.st_mtime

                if oldest_time is None or mtime < oldest_time:
                    oldest_time = mtime
                if newest_time is None or mtime > newest_time:
                    newest_time = mtime

        return StorageStats(
            total_size=total_size,
            file_count=file_count,
            oldest_file=datetime.fromtimestamp(oldest_time) if oldest_time else None,
            newest_file=datetime.fromtimestamp(newest_time) if newest_time else None,
        )

    async def validate_storage(self) -> None:
        """Validate storage state and clean up if needed.

        Raises:
            StorageFullError: If storage is full and cleanup fails
        """
        stats = await self.get_storage_stats()
        if stats["total_size"] > self.max_cache_size:
            # Check size first before attempting cleanup
            if stats["total_size"] > self.max_cache_size * 2:  # If way over limit
                raise StorageFullError("Storage is critically full")

            await self.cleanup_old_files()
            # Check if cleanup was sufficient
            new_stats = await self.get_storage_stats()
            if new_stats["total_size"] > self.max_cache_size:
                raise StorageFullError("Storage is full after cleanup")

    async def cleanup_old_files(self) -> None:
        """Remove old files from cache."""
        cutoff = datetime.now() - self.max_cache_age
        total_size = 0
        files_by_time: list[tuple[float, Path]] = []

        # Collect file information
        for path in self.temp_dir.rglob("*"):
            if path.is_file():
                try:
                    mtime = path.stat().st_mtime
                    size = path.stat().st_size
                    total_size += size
                    files_by_time.append((mtime, path))
                except Exception as e:
                    logger.warning(f"Failed to stat {path}: {e}")

        # Sort by modification time
        files_by_time.sort()

        # Remove old files and enforce size limit
        for mtime, path in files_by_time:
            try:
                if (
                    datetime.fromtimestamp(mtime) < cutoff
                    or total_size > self.max_cache_size
                ):
                    size = path.stat().st_size
                    path.unlink()
                    total_size -= size
                    logger.debug(f"Removed old file: {path}")
            except Exception as e:
                logger.warning(f"Failed to remove {path}: {e}")

    def _get_media_dir(self, tweet_id: str) -> Path:
        """Get directory for storing media from a tweet.

        Args:
            tweet_id: ID of the tweet

        Returns:
            Path to media directory
        """
        return self.base_dir / tweet_id

    async def _copy_file_with_progress(
        self,
        src: Path,
        dst: Path,
        *,
        chunk_size: int = CHUNK_SIZE,
    ) -> None:
        """Copy file with progress tracking.

        Args:
            src: Source file path
            dst: Destination file path
            chunk_size: Size of chunks to copy

        Raises:
            StorageError: If copy fails
        """
        try:
            total_size = src.stat().st_size
            copied = 0

            with src.open("rb") as fsrc, dst.open("wb") as fdst:
                while True:
                    chunk = fsrc.read(chunk_size)
                    if not chunk:
                        break
                    fdst.write(chunk)
                    copied += len(chunk)
                    logger.debug(f"Copied {copied}/{total_size} bytes")

        except Exception as e:
            # Clean up partial file
            if dst.exists():
                dst.unlink()
            raise StorageError(f"Failed to copy file: {e}") from e

    async def save_media_item(
        self,
        media: MediaItem,
        tweet_id: str,
        *,
        temp: bool = True
    ) -> Path:
        """Save a media item to storage.

        Args:
            media: Media item to save
            tweet_id: ID of the tweet
            temp: Whether to use temporary storage

        Returns:
            Path where media was saved

        Raises:
            StorageError: If saving fails
            FileNotFoundError: If media file not found
        """
        if not media.local_path or not media.local_path.exists():
            raise FileNotFoundError(f"Media file not found: {media.local_path}")

        try:
            # Validate storage before saving
            await self.validate_storage()

            if temp:
                # Use temporary directory with tweet ID subdirectory
                target_dir = self.temp_dir / tweet_id
            else:
                target_dir = self._get_media_dir(tweet_id)

            target_dir.mkdir(parents=True, exist_ok=True)

            # Generate target filename
            suffix = media.local_path.suffix
            filename = f"{media.type.value}_{media.local_path.stem}{suffix}"
            target_path = target_dir / filename

            # Copy file with progress
            await self._copy_file_with_progress(media.local_path, target_path)
            media.local_path = target_path

            return target_path

        except Exception as e:
            raise StorageError(f"Failed to save media: {e}") from e

    async def save_tweet_content(
        self,
        content: DownloadedContent,
        *,
        temp: bool = True
    ) -> DownloadedContent:
        """Save all media from downloaded content.

        Args:
            content: Downloaded content to save
            temp: Whether to use temporary storage

        Returns:
            Updated content with new file paths

        Raises:
            StorageError: If saving fails
        """
        if not content.content:
            return content

        try:
            # Clean up old files first
            await self.cleanup_old_files()

            # Handle single tweet
            if isinstance(content.content, Tweet):
                tweet = content.content
                content.local_files = []

                # Save media items
                for media in tweet.media:
                    if media.local_path:
                        path = await self.save_media_item(
                            media,
                            tweet.id,
                            temp=temp
                        )
                        content.local_files.append(path)

                # Save card image if present
                if tweet.card and tweet.card.local_image:
                    card_path = await self.save_media_item(
                        MediaItem(
                            url=tweet.card.image_url or "",
                            type=MediaType.IMAGE,
                            local_path=tweet.card.local_image
                        ),
                        tweet.id,
                        temp=temp
                    )
                    tweet.card.local_image = card_path
                    content.local_files.append(card_path)

            # Handle thread
            elif isinstance(content.content, TweetThread):
                content.local_files = []
                for tweet in content.content.tweets:
                    # Save each tweet's media
                    for media in tweet.media:
                        if media.local_path:
                            path = await self.save_media_item(
                                media,
                                tweet.id,
                                temp=temp
                            )
                            content.local_files.append(path)

            return content

        except Exception as e:
            raise StorageError(f"Failed to save content: {e}") from e

    async def cleanup(self) -> None:
        """Clean up storage directories."""
        try:
            if self.temp_dir.exists():
                shutil.rmtree(self.temp_dir)
                self.temp_dir.mkdir(parents=True)
        except Exception as e:
            logger.warning(f"Failed to clean up storage: {e}")


async def get_storage() -> TwitterMediaStorage:
    """Get storage manager instance.

    Returns:
        Storage manager instance
    """
    return TwitterMediaStorage()

</document_content>
</document>
<document index="70">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/utils/twitter_utils/types.py</source>
<document_content>
"""Type definitions for Twitter utilities."""

from __future__ import annotations

from typing import Literal, TypedDict


class TweetMetadata(TypedDict):
    """Metadata for a downloaded tweet."""
    id: str
    url: str
    author: str
    content: str
    media_urls: list[str]
    created_at: str

class DownloadResult(TypedDict):
    """Result of a tweet download operation."""
    success: bool
    metadata: TweetMetadata
    local_files: list[str]
    error: str | None

TweetDownloadMode = Literal["single", "thread", "card"]

</document_content>
</document>
</documents>
