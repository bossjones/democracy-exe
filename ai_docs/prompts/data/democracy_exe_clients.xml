<documents>
<document index="1">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/clients/__init__.py</source>
<document_content>

</document_content>
</document>
<document index="2">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/clients/aio_gallery_dl.py</source>
<document_content>
"""Asynchronous wrapper around gallery-dl."""
from __future__ import annotations

import asyncio
import json
import os
import pathlib
import sys
import traceback

from collections.abc import AsyncIterator, Callable
from datetime import datetime
from functools import partial
from typing import Any, Dict, List, Optional, TypeVar, Union, cast, overload

import aiofiles
import bpdb
import gallery_dl
import structlog

from pydantic import BaseModel, ConfigDict, EmailStr, Field, SecretStr

from democracy_exe.aio_settings import aiosettings
from democracy_exe.utils.file_functions import expand_path_str, tilda


logger = structlog.get_logger(__name__)

T = TypeVar("T")
R = TypeVar("R")

class HttpConfig(BaseModel):
    """Configuration for HTTP downloader settings."""
    model_config = ConfigDict(populate_by_name=True)

    adjust_extensions: bool = Field(True, alias="adjust-extensions")
    mtime: bool = True
    rate: int | None = None
    retries: int = 4
    timeout: float = 30.0
    verify: bool = True

class YtdlConfig(BaseModel):
    """Configuration for youtube-dl downloader settings."""
    model_config = ConfigDict(populate_by_name=True)

    format: str | None = None
    forward_cookies: bool = Field(False, alias="forward-cookies")
    logging: bool = True
    mtime: bool = True
    outtmpl: str | None = None
    rate: int | None = None
    retries: int = 4
    timeout: float = 30.0
    verify: bool = True

class DownloaderConfig(BaseModel):
    """Configuration for downloader settings."""
    model_config = ConfigDict(populate_by_name=True)

    filesize_min: int | None = Field(None, alias="filesize-min")
    filesize_max: int | None = Field(None, alias="filesize-max")
    part: bool = True
    part_directory: str | None = Field(None, alias="part-directory")
    http: HttpConfig
    ytdl: YtdlConfig

class OutputConfig(BaseModel):
    """Configuration for output settings."""
    model_config = ConfigDict(populate_by_name=True)

    mode: str = "auto"
    progress: bool = True
    shorten: bool = True
    log: str = "[{name}][{levelname}][{extractor.url}] {message}"
    logfile: str | None = None
    unsupportedfile: str | None = None

class DirectoryConfig(BaseModel):
    """Configuration for directory settings."""
    model_config = ConfigDict(populate_by_name=True)

    directory: list[str]

class InstagramConfig(BaseModel):
    """Configuration for Instagram extractor."""
    model_config = ConfigDict(populate_by_name=True)

    highlights: bool = False
    videos: bool = True
    include: str = "all"
    directory: list[str]
    stories: DirectoryConfig
    channel: DirectoryConfig
    tagged: DirectoryConfig
    reels: DirectoryConfig
    filename: str
    date_format: str = Field(alias="date-format")
    cookies: str | None = None
    username: SecretStr | None = None
    password: SecretStr | None = None
    sleep_request: float = Field(8.0, alias="sleep-request")

class RedditConfig(BaseModel):
    """Configuration for Reddit extractor."""
    model_config = ConfigDict(populate_by_name=True)

    client_id: SecretStr = Field(alias="client-id")
    user_agent: str = Field(alias="user-agent")
    browser: str
    refresh_token: SecretStr | None = Field(None, alias="refresh-token")
    comments: int = 0
    morecomments: bool = False
    date_min: int = Field(0, alias="date-min")
    date_max: int = Field(253402210800, alias="date-max")
    date_format: str = Field(alias="date-format")
    id_min: str | None = Field(None, alias="id-min")
    id_max: str | None = Field(None, alias="id-max")
    recursion: int = 0
    videos: bool = True
    parent_directory: bool = Field(True, alias="parent-directory")
    directory: list[str]
    filename: str

class TwitterConfig(BaseModel):
    """Configuration for Twitter extractor."""
    model_config = ConfigDict(populate_by_name=True)

    quoted: bool = True
    replies: bool = True
    retweets: bool = True
    twitpic: bool = False
    videos: bool = True
    cookies: str | None = None
    filename: str

class DeviantartConfig(BaseModel):
    """Configuration for DeviantArt extractor."""
    model_config = ConfigDict(populate_by_name=True)

    extra: bool = False
    flat: bool = True
    folders: bool = False
    journals: str = "html"
    mature: bool = True
    metadata: bool = False
    original: bool = True
    quality: int = 100
    wait_min: int = Field(0, alias="wait-min")

class PixivConfig(BaseModel):
    """Configuration for Pixiv extractor."""
    model_config = ConfigDict(populate_by_name=True)

    username: SecretStr | None = None
    password: SecretStr | None = None
    avatar: bool = False
    ugoira: bool = True

class ExtractorConfig(BaseModel):
    """Configuration for extractors."""
    model_config = ConfigDict(populate_by_name=True)

    base_directory: str = Field("./gallery-dl/", alias="base-directory")
    postprocessors: Any | None = None
    archive: str | None = None
    cookies: str | None = None
    cookies_update: bool = Field(True, alias="cookies-update")
    proxy: str | None = None
    skip: bool = True
    sleep: int = 0
    sleep_request: int = Field(0, alias="sleep-request")
    sleep_extractor: int = Field(0, alias="sleep-extractor")
    path_restrict: str = Field("auto", alias="path-restrict")
    path_replace: str = Field("_", alias="path-replace")
    path_remove: str = Field("\\u0000-\\u001f\\u007f", alias="path-remove")
    user_agent: str = Field(alias="user-agent")
    path_strip: str = Field("auto", alias="path-strip")
    path_extended: bool = Field(True, alias="path-extended")
    extension_map: dict[str, str] = Field(alias="extension-map")
    instagram: InstagramConfig
    reddit: RedditConfig
    twitter: TwitterConfig
    deviantart: DeviantartConfig
    pixiv: PixivConfig

class GalleryDLConfig(BaseModel):
    """Root configuration model for gallery-dl."""
    model_config = ConfigDict(populate_by_name=True)

    extractor: ExtractorConfig
    downloader: DownloaderConfig
    output: OutputConfig
    netrc: bool = False


class AsyncGalleryDL:
    """Asynchronous wrapper around gallery-dl.

    This class provides an async interface to gallery-dl operations,
    running them in a thread pool to avoid blocking the event loop.

    Attributes:
        config: Gallery-dl configuration dictionary
        loop: Optional asyncio event loop

    Example:
        >>> async with AsyncGalleryDL() as client:
        ...     async for item in client.extract_from_url("https://example.com"):
        ...         print(item)
    """

    def __init__(
        self,
        config: dict[str, Any] | None = None,
        loop: asyncio.AbstractEventLoop | None = None,
        verbose: bool = False,
        write_info_json: bool = False,
        write_metadata: bool = False,
        no_mtime: bool = False,
        config_file: str | None = None,
    ) -> None:
        """Initialize AsyncGalleryDL.

        Args:
            config: Gallery-dl configuration dictionary
            loop: Optional asyncio event loop to use
            verbose: Enable verbose output
            write_info_json: Write info JSON files
            write_metadata: Write metadata files
            no_mtime: Don't set file modification times
            config_file: Path to gallery-dl config file (default: ~/.gallery-dl.conf)

        Example:
            >>> client = AsyncGalleryDL({"your": "config"})
        """
        self.config = config or {}
        logger.debug(f"Using self.config: {self.config}")

        if verbose:
            self.config["verbosity"] = 2
        if write_info_json:
            self.config["write-info-json"] = True
        if write_metadata:
            self.config["write-metadata"] = True
        if no_mtime:
            self.config["no-mtime"] = True
        self.loop = loop or asyncio.get_event_loop()

        # Load config file if specified
        if config_file:
            self.config_file = expand_path_str(config_file)
        else:
            self.config_file = expand_path_str("~/.gallery-dl.conf")
        logger.debug(f"Using self.config_file: {self.config_file}")


    @overload
    async def _run_in_executor(
        self, func: Callable[..., T], *args: Any, **kwargs: Any
    ) -> T:
        ...

    @overload
    async def _run_in_executor(
        self, func: Callable[..., R], *args: Any, **kwargs: Any
    ) -> R:
        ...

    async def _run_in_executor(
        self, func: Callable[..., Any], *args: Any, **kwargs: Any
    ) -> Any:
        """Run a function in the default executor.

        Args:
            func: Function to run
            *args: Positional arguments for the function
            **kwargs: Keyword arguments for the function

        Returns:
            Result from the function

        Example:
            >>> result = await client._run_in_executor(some_func, arg1, kwarg1="value")
        """
        partial_func = partial(func, *args, **kwargs)
        return await self.loop.run_in_executor(None, partial_func)

    async def extract_from_url(self, url: str) -> AsyncIterator[dict[str, Any]]:
        """Extract items from a URL asynchronously.

        Args:
            url: URL to extract from

        Yields:
            Extracted items from gallery-dl

        Raises:
            ValueError: If extraction fails
            RuntimeError: If gallery-dl encounters an error

        Example:
            >>> async for item in client.extract_from_url("https://example.com"):
            ...     print(item["title"])
        """
        try:
            extractor = await self._run_in_executor(
                gallery_dl.extractor.find,  # type: ignore[attr-defined] # pylint: disable=no-member
                url
            )

            # Create async iterator from sync iterator
            for item in extractor:
                yield cast(dict[str, Any], item)
                # Give control back to event loop
                await asyncio.sleep(0)

        except Exception as e:
            logger.error("Error in gallery-dl extraction")
            raise

    async def download(
        self,
        url: str,
        **options: Any
    ) -> AsyncIterator[dict[str, Any]]:
        """Download content from URL asynchronously.

        Args:
            url: URL to download from
            **options: Additional options to pass to gallery-dl

        Yields:
            Download progress and results

        Raises:
            ValueError: If download fails
            RuntimeError: If gallery-dl encounters an error

        Example:
            >>> async for status in client.download("https://example.com"):
            ...     print(status["progress"])
        """
        try:
            job = await self._run_in_executor(
                gallery_dl.job.DownloadJob,  # type: ignore[attr-defined]
                url,
                options
            )

            # Run download in executor and yield results
            for item in job.run():
                yield cast(dict[str, Any], item)
                # Give control back to event loop
                await asyncio.sleep(0)

        except Exception as e:
            print(f"{e}")
            exc_type, exc_value, exc_traceback = sys.exc_info()
            print(f"Error Class: {e.__class__}")
            output = f"[UNEXPECTED] {type(e).__name__}: {e}"
            print(output)
            print(f"exc_type: {exc_type}")
            print(f"exc_value: {exc_value}")
            traceback.print_tb(exc_traceback)
            # await logger.complete()
            if aiosettings.dev_mode:
                bpdb.pm()

            logger.error("Error in gallery-dl download")
            raise

    @classmethod
    async def extract_metadata(
        cls,
        url: str,
        config: dict[str, Any] | None = None
    ) -> AsyncIterator[dict[str, Any]]:
        """Extract metadata from URL asynchronously.

        This is a convenience class method that creates a temporary instance
        for metadata extraction.

        Args:
            url: URL to extract metadata from
            config: Optional gallery-dl configuration

        Yields:
            Metadata items from the URL

        Raises:
            ValueError: If metadata extraction fails
            RuntimeError: If gallery-dl encounters an error

        Example:
            >>> async for metadata in AsyncGalleryDL.extract_metadata("https://example.com"):
            ...     print(metadata["title"])
        """
        async with cls(config=config) as client:
            async for item in client.extract_from_url(url):
                yield item

    async def __aenter__(self) -> AsyncGalleryDL:
        """Enter async context.

        Returns:
            Self instance

        Example:
            >>> async with AsyncGalleryDL() as client:
            ...     # Use client here
            ...     pass
        """
        # Load config file if it exists
        if os.path.exists(self.config_file):
            try:
                async with aiofiles.open(self.config_file, encoding="utf-8") as f:
                    config_data = json.loads(await f.read())
                    self.config.update(config_data)
                logger.debug(f"Loaded gallery-dl config from {self.config_file}")
            except Exception as e:
                logger.error(f"Error loading gallery-dl config: {e}")

        return self

    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        """Exit async context.

        Args:
            exc_type: Exception type if an error occurred
            exc_val: Exception value if an error occurred
            exc_tb: Exception traceback if an error occurred

        Example:
            >>> async with AsyncGalleryDL() as client:
            ...     # Context is automatically cleaned up after this block
            ...     pass
        """
        # Cleanup if needed
        pass


if __name__ == "__main__":
    import asyncio

    import rich

    from langsmith import tracing_context


    async def main() -> None:
        """Run the AsyncGalleryDL tool asynchronously."""
        url = "https://x.com/Eminitybaba_/status/1868256259251863704"
        with tracing_context(enabled=False):

            # Test download
            try:
                # Test extraction with command line options
                client = AsyncGalleryDL(verbose=True, write_info_json=True, write_metadata=True, no_mtime=True)
                items = []
                async for item in client.extract_from_url(url):
                    items.append(item)
                rich.print(f"items: {items}")

            except Exception as ex:
                print(f"{ex}")
                exc_type, exc_value, exc_traceback = sys.exc_info()
                print(f"Error Class: {ex.__class__}")
                output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
                print(output)
                print(f"exc_type: {exc_type}")
                print(f"exc_value: {exc_value}")
                traceback.print_tb(exc_traceback)
                # await logger.complete()
                rich.print(f"aiosettings.dev_mode: {aiosettings.dev_mode}")
                if aiosettings.dev_mode:
                    bpdb.pm()


    asyncio.run(main())

</document_content>
</document>
<document index="3">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/clients/tweetpik.py</source>
<document_content>
# democracy_exe/clients/tweetpik.py
from __future__ import annotations

from typing import Any, Dict, Optional

import httpx
import structlog


logger = structlog.get_logger(__name__)

from democracy_exe.aio_settings import aiosettings


class TweetPikClient:
    """Client for interacting with TweetPik API.

    Handles screenshot capture of tweets with configurable styling options.
    Uses settings from aiosettings for default configuration values.
    """

    BASE_URL = "https://tweetpik.com/api/v2/images"

    def __init__(self, api_key: str):
        """Initialize TweetPik client with API key and default config.

        Args:
            api_key: TweetPik API authentication key
        """
        if not api_key:
            api_key = aiosettings.tweetpik_authorization.get_secret_value()  # pylint: disable=no-member

        self.api_key = api_key
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": api_key
        }

    def _prepare_payload(self, tweet_url: str, **kwargs) -> dict[str, Any]:
        """Prepare request payload with default and override settings.

        Args:
            tweet_url: URL of the tweet to capture
            **kwargs: Optional overrides for default settings

        Returns:
            Dictionary containing the complete request payload
        """
        payload = {
            "url": tweet_url,
            "theme": aiosettings.tweetpik_theme,
            "dimension": aiosettings.tweetpik_dimension,
            "backgroundColor": aiosettings.tweetpik_background_color,
            "textPrimaryColor": aiosettings.tweetpik_text_primary_color,
            "textSecondaryColor": aiosettings.tweetpik_text_secondary_color,
            "linkColor": aiosettings.tweetpik_link_color,
            "verifiedIconColor": aiosettings.tweetpik_verified_icon_color,
            "displayVerified": aiosettings.tweetpik_display_verified,
            "displayMetrics": aiosettings.tweetpik_display_metrics,
            "displayEmbeds": aiosettings.tweetpik_display_embeds,
            "contentScale": aiosettings.tweetpik_content_scale,
            "contentWidth": aiosettings.tweetpik_content_width,
            "twitterToken": aiosettings.tweetpik_api_key.get_secret_value(), # pylint: disable=no-member
        }
        payload.update(kwargs)
        return payload

    def screenshot_tweet(self, tweet_url: str, **kwargs) -> dict[str, Any]:
        """Capture a screenshot of a tweet synchronously.

        Args:
            tweet_url: URL of the tweet to capture
            **kwargs: Optional configuration overrides

        Returns:
            API response containing screenshot data

        Raises:
            httpx.HTTPError: If the API request fails
        """
        payload = self._prepare_payload(tweet_url, **kwargs)
        with httpx.Client() as client:
            response = client.post(self.BASE_URL, json=payload, headers=self.headers)
        response.raise_for_status()
        return response.json()

    async def screenshot_tweet_async(self, tweet_url: str, **kwargs) -> dict[str, Any]:
        """Capture a screenshot of a tweet asynchronously.

        Args:
            tweet_url: URL of the tweet to capture
            **kwargs: Optional configuration overrides

        Returns:
            API response containing screenshot data

        Raises:
            httpx.HTTPError: If the API request fails
        """
        payload = self._prepare_payload(tweet_url, **kwargs)
        async with httpx.AsyncClient() as client:
            response = await client.post(self.BASE_URL, json=payload, headers=self.headers)
        response.raise_for_status()
        return response.json()

</document_content>
</document>
<document index="4">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/clients/discord_client/__init__.py</source>
<document_content>

</document_content>
</document>
<document index="5">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/clients/discord_client/utils.py</source>
<document_content>
# pylint: disable=too-many-function-args
# mypy: disable-error-code="arg-type, var-annotated, list-item, no-redef, truthy-bool, return-value"
# pyright: reportPrivateImportUsage=false
# pyright: reportGeneralTypeIssues=false
# pyright: reportAttributeAccessIssue=false
# pyright: reportInvalidTypeForm=false
from __future__ import annotations

import logging

from typing import List, Optional

import discord
import structlog

from discord import Message as DiscordMessage


logger = structlog.get_logger(__name__)

from democracy_exe.base import DemocracyMessage
from democracy_exe.constants import INACTIVATE_THREAD_PREFIX, MAX_CHARS_PER_REPLY_MSG


def discord_message_to_message(message: discord.Message) -> DemocracyMessage | None:
    """Convert a Discord message to a DemocracyMessage.

    Args:
        message: The Discord message to convert.

    Returns:
        DemocracyMessage if conversion is successful, None otherwise.
    """
    if (
        message.type == discord.MessageType.thread_starter_message
        and message.reference.cached_message
        and len(message.reference.cached_message.embeds) > 0
        and len(message.reference.cached_message.embeds[0].fields) > 0
    ):
        field = message.reference.cached_message.embeds[0].fields[0]
        if field.value:
            return DemocracyMessage(user=field.name, text=field.value)
    elif message.content:
        return DemocracyMessage(user=message.author.name, text=message.content)
    return None


def split_into_shorter_messages(message: str) -> list[str]:
    """Split a message into shorter messages that fit within Discord's character limit.

    Args:
        message: The message to split.

    Returns:
        List of message chunks that fit within the character limit.
    """
    return [message[i : i + MAX_CHARS_PER_REPLY_MSG] for i in range(0, len(message), MAX_CHARS_PER_REPLY_MSG)]


def is_last_message_stale(interaction_message: DiscordMessage, last_message: DiscordMessage, bot_id: str) -> bool:
    """Check if the last message in a thread is stale.

    Args:
        interaction_message: The message that triggered the interaction.
        last_message: The last message in the thread.
        bot_id: The ID of the bot.

    Returns:
        True if the last message is stale, False otherwise.
    """
    return (
        last_message
        and last_message.id != interaction_message.id
        and last_message.author
        and last_message.author.id != bot_id
    )


async def close_thread(thread: discord.Thread) -> None:
    """Close a Discord thread.

    Args:
        thread: The thread to close.
    """
    await thread.edit(name=INACTIVATE_THREAD_PREFIX)
    await thread.send(
        embed=discord.Embed(
            description="**Thread closed** - Context limit reached, closing...",
            color=discord.Color.blue(),
        )
    )
    await thread.edit(archived=True, locked=True)

</document_content>
</document>
<document index="6">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/__init__.py</source>
<document_content>

</document_content>
</document>
<document index="7">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/discord_bot.py</source>
<document_content>
"""Main Discord bot module.

This module serves as the entry point for the Discord bot functionality.
It imports and uses the modular components defined in the submodules.
"""
from __future__ import annotations

import pathlib

from democracy_exe.chatbot.core import DemocracyBot


# Path to this module's directory
HERE = pathlib.Path(__file__).parent

__all__ = ["DemocracyBot"]

</document_content>
</document>
<document index="8">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/shared_bot.py</source>
<document_content>

</document_content>
</document>
<document index="9">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/terminal_bot.py</source>
<document_content>
from __future__ import annotations

import asyncio
import signal
import sys

from collections.abc import AsyncGenerator, Generator
from typing import Annotated, Any, Dict, List, Optional, TypedDict, Union

import structlog

from langchain_anthropic import ChatAnthropic
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph.state import CompiledStateGraph


logger = structlog.get_logger(__name__)
from rich import print as rprint

from democracy_exe.agentic import _utils as agentic_utils
from democracy_exe.agentic.workflows.react.graph import graph as memgraph
from democracy_exe.aio_settings import aiosettings


SEM = asyncio.Semaphore(1)


class FlushingStderr:
    """A class to handle flushing stderr output."""

    def write(self, message: str) -> None:
        """Write and flush a message to stderr.

        Args:
            message: The message to write to stderr
        """
        sys.stderr.write(message)
        sys.stderr.flush()


# logger.remove()
# logger.add(FlushingStderr(), enqueue=True)


async def go_terminal_bot(graph: CompiledStateGraph = memgraph) -> None:
    """Main function to run the LangGraph Chatbot in the terminal.

    This function handles user input and processes it through the AI pipeline.

    Args:
        graph: The compiled state graph to use for processing messages
    """
    logger.info("Starting the DemocracyExeAI Chatbot")
    rprint("[bold green]Welcome to the DemocracyExeAI Chatbot! Type 'quit' to exit.[/bold green]")
    logger.info("Welcome to the DemocracyExeAI Chatbot! Type 'quit' to exit.")

    config: RunnableConfig = {"configurable": {"thread_id": "1", "user_id": "1"}}

    # User input
    # await logger.complete()

    # Flush stderr before input prompt
    sys.stderr.flush()

    while True:
        user_input = await asyncio.to_thread(input, "You: ")

        if user_input.lower() == 'quit':
            rprint("[bold red]Goodbye![/bold red]")
            logger.info("Goodbye!")
            break

        # Create a HumanMessage from the user input
        message = HumanMessage(content=user_input)

        try:
            user_input_dict: dict[str, list[BaseMessage]] = {"messages": [message]}
            stream_terminal_bot(graph, user_input_dict, config)
        except Exception as e:
            logger.exception("Error processing message")
            rprint("[bold red]An error occurred while processing your message.[/bold red]")


def stream_terminal_bot(
    graph: CompiledStateGraph = memgraph,
    user_input: dict[str, list[BaseMessage]] | None = None,
    thread: dict[str, dict[str, str]] | None = None,
    interruptable: bool = False
) -> None:
    """Stream the LangGraph Chatbot in the terminal.

    Args:
        graph: The compiled state graph to use for processing messages
        user_input: Dictionary containing user messages
        thread: Thread configuration dictionary
        interruptable: Whether the stream can be interrupted for user approval
    """
    # Run the graph until the first interruption
    for event in graph.stream(user_input, thread, stream_mode="values"):
        logger.debug(event)
        chunk = event['messages'][-1]
        logger.error(f"chunk: {chunk}")
        logger.error(f"type: {type(chunk)}")

        chunk.pretty_print()

    if interruptable:
        # Get user feedback
        user_approval = input("Do you want to call the tool? (yes[y]/no[n]): ")

        # Check approval
        if user_approval.lower() in ("yes", "y"):
            # If approved, continue the graph execution
            for event in graph.stream(None, thread, stream_mode="values"):
                event['messages'][-1].pretty_print()
        else:
            print("Operation cancelled by user.")


def invoke_terminal_bot(
    graph: CompiledStateGraph = memgraph,
    user_input: dict[str, list[BaseMessage]] | None = None,
    thread: dict[str, dict[str, str]] | None = None
) -> str | None:
    """Invoke the LangGraph Chatbot in the terminal.

    Args:
        graph: The compiled state graph to use for processing messages
        user_input: Dictionary containing user messages
        thread: Thread configuration dictionary

    Returns:
        The AI response message as a string, or None if no response
    """
    messages = graph.invoke(user_input, thread)
    for m in messages['messages']:
        m.pretty_print()
    rprint(f"[bold blue]AI:[/bold blue] {messages['response']}")
    logger.info(f"AI: {messages['response']}")
    return messages['response']


if __name__ == "__main__":
    asyncio.run(go_terminal_bot())

</document_content>
</document>
<document index="10">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/textual_bot.py</source>
<document_content>
# from textual.app import App
# from textual.widgets import Input
# from textual.scroll_view import ScrollView

# class ChatApp(App):
#     async def on_mount(self):
#         self.chat_history = ScrollView()
#         self.input = Input()
#         await self.view.dock(self.chat_history, edge="top", size=20)
#         await self.view.dock(self.input, edge="bottom")

#     async def on_input_submitted(self, message: str):
#         # Add user message to chat history
#         self.chat_history.write(f"You: {message}\n")

#         # Send message to langgraph model and get response
#         response = await self.langgraph_model.send(message)

#         # Add model response to chat history
#         self.chat_history.write(f"Model: {response}\n")

#         # Clear input field
#         self.input.value = ""

# if __name__ == "__main__":
#     app = ChatApp()
#     app.run()

</document_content>
</document>
<document index="11">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/core/__init__.py</source>
<document_content>
"""Core bot module initialization."""
from __future__ import annotations

from democracy_exe.chatbot.core.bot import DemocracyBot


__all__ = ["DemocracyBot"]

</document_content>
</document>
<document index="12">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/core/bot.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false

"""Core DemocracyBot implementation.

This module contains the main DemocracyBot class and its core functionality.
"""
from __future__ import annotations

import asyncio
import datetime

from collections import Counter, defaultdict
from collections.abc import AsyncIterator, Callable, Coroutine, Iterable
from io import BytesIO
from typing import TYPE_CHECKING, Any, Dict, List, NoReturn, Optional, Tuple, TypeVar, Union, cast

import aiohttp
import discord
import pysnooper
import structlog

from discord import (
    Activity,
    AllowedMentions,
    AppInfo,
    DMChannel,
    Game,
    Guild,
    Intents,
    Message,
    Status,
    TextChannel,
    Thread,
    User,
)
from discord.abc import Messageable
from discord.ext import commands
from langchain_core.messages import AIMessage, HumanMessage
from langgraph.graph.state import CompiledStateGraph  # type: ignore[import]


logger = structlog.get_logger(__name__)
from pydantic import SecretStr

import democracy_exe

from democracy_exe import utils
from democracy_exe.agentic.workflows.react.graph import graph as memgraph
from democracy_exe.aio_settings import aiosettings
from democracy_exe.chatbot.handlers.attachment_handler import AttachmentHandler
from democracy_exe.chatbot.handlers.message_handler import MessageHandler
from democracy_exe.chatbot.utils.guild_utils import preload_guild_data
from democracy_exe.chatbot.utils.message_utils import format_inbound_message
from democracy_exe.constants import CHANNEL_ID
from democracy_exe.utils.bot_context import Context


if TYPE_CHECKING:
    from redis.asyncio import ConnectionPool as RedisConnectionPool

DESCRIPTION = """An example bot to showcase the discord.ext.commands extension
module.

There are a number of utility commands being showcased here."""

class ProxyObject(discord.Object):
    def __init__(self, guild: discord.abc.Snowflake | None):
        super().__init__(id=0)
        self.guild: discord.abc.Snowflake | None = guild

def _prefix_callable(bot: DemocracyBot, msg: discord.Message) -> list[str]:
    """
    Generate a list of command prefixes for the bot.

    This function generates a list of command prefixes for the bot based on the message context.
    If the message is from a direct message (DM) channel, it includes the bot's user ID mentions
    and default prefixes. If the message is from a guild (server) channel, it includes the bot's
    user ID mentions and the guild-specific prefixes.

    Args:
    ----
        bot (AsyncGoobBot): The instance of the bot.
        msg (discord.Message): The message object from Discord.

    Returns:
    -------
        List[str]: A list of command prefixes to be used for the bot.

    """
    user_id = bot.user.id
    base = [f"<@!{user_id}> ", f"<@{user_id}> "]
    if msg.guild is None:  # pyright: ignore[reportAttributeAccessIssue]
        base.extend(("!", "?"))
    else:
        base.extend(bot.prefixes.get(msg.guild.id, ["?", "!"]))  # pyright: ignore[reportAttributeAccessIssue]
    return base

class DemocracyBot(commands.Bot):
    """Discord bot for handling democratic interactions and AI processing.

    This bot integrates with various AI models and processing pipelines to handle
    user interactions in a democratic context.

    Attributes:
        session: aiohttp ClientSession for making HTTP requests
        command_stats: Counter for tracking command usage
        socket_stats: Counter for tracking socket events
        graph: LangGraph for processing messages
        message_handler: Handler for processing messages
        attachment_handler: Handler for processing attachments
        version: Bot version
        guild_data: Guild configuration data
        bot_app_info: Discord application info
        owner_id: Bot owner's Discord ID
        invite: Bot invite link
        uptime: Bot start time
    """
    user: discord.ClientUser
    old_tree_error = Callable[[discord.Interaction, discord.app_commands.AppCommandError], Coroutine[Any, Any, None]]

    def __init__(
        self,
        command_prefix: str | None = None,
        description: str | None = None,
        intents: discord.Intents | None = None,
        *args: Any,
        **kwargs: Any
    ) -> None:
        """Initialize the DemocracyBot with required intents and models.

        Args:
            command_prefix: Optional custom command prefix. Defaults to aiosettings.prefix.
            description: Optional bot description. Defaults to DESCRIPTION.
            intents: Optional custom intents. Defaults to standard intents configuration.
            *args: Additional positional arguments passed to commands.Bot
            **kwargs: Additional keyword arguments passed to commands.Bot
        """
        allowed_mentions = AllowedMentions(roles=False, everyone=False, users=True)

        # Set up default intents if not provided
        if intents is None:
            intents = Intents.default()
            intents.message_content = True
            intents.guilds = True
            intents.members = True
            intents.bans = True
            intents.emojis = True
            intents.voice_states = True
            intents.messages = True
            intents.reactions = True

        self._command_prefix = command_prefix or aiosettings.prefix
        self._user = None

        super().__init__(
            command_prefix=self._command_prefix,
            description=description or DESCRIPTION,
            pm_help=None,
            help_attrs=dict(hidden=True),
            chunk_guilds_at_startup=False,
            heartbeat_timeout=150.0,
            allowed_mentions=allowed_mentions,
            intents=intents,
            enable_debug_events=True,
            *args,
            **kwargs
        )

        # Initialize session and stats
        # self.session: aiohttp.ClientSession = aiohttp.ClientSession()
        self.command_stats: Counter = Counter()
        self.socket_stats: Counter = Counter()
        self.graph: CompiledStateGraph = memgraph

        # Initialize handlers
        self.message_handler = MessageHandler(self)
        self.attachment_handler = AttachmentHandler()

        # Initialize bot attributes
        self.version: str = democracy_exe.__version__
        self.guild_data: dict[int, dict[str, Any]] = {}
        self.bot_app_info: AppInfo | None = None
        self.owner_id: int | None = None
        self.invite: str | None = None
        self.uptime: datetime.datetime | None = None
        self.pool: RedisConnectionPool | None = None

        self.resumes: defaultdict[int, list[datetime.datetime]] = defaultdict(list)
        self.identifies: defaultdict[int, list[datetime.datetime]] = defaultdict(list)

        self.spam_control = commands.CooldownMapping.from_cooldown(10, 12.0, commands.BucketType.user)

        # A counter to auto-ban frequent spammers
        # Triggering the rate limit 5 times in a row will auto-ban the user from the bot.
        self._auto_spam_count = Counter()

        self.channel_list = [int(x) for x in CHANNEL_ID.split(",")]
        self.queue: asyncio.Queue = asyncio.Queue()
        self.tasks: list[Any] = []
        self.num_workers = 3

        self.total_sleep_time = 0

        self.start_time = datetime.datetime.now()
        self.typerCtx: dict | None = None
        self.job_queue: dict[Any, Any] = {}
        self.client_id: int | str = aiosettings.discord_client_id
        self.enable_ai = aiosettings.enable_ai

    async def get_context(self, origin: discord.Interaction | Message, /, *, cls=Context) -> Context:
        """Retrieve the context for a Discord interaction or message.

        Args:
            origin: The Discord interaction or message to get the context from
            cls: The class type for the context object

        Returns:
            The context object retrieved for the provided origin
        """
        ctx = await super().get_context(origin, cls=cls)
        ctx.prefix = self._command_prefix
        return ctx

    async def setup_hook(self) -> None:
        """Asynchronous setup hook for initializing the bot.

        This method is called to perform asynchronous setup tasks for the bot.
        It initializes the aiohttp session, sets up guild prefixes, retrieves
        bot application information, and loads extensions.
        """
        logger.debug("Starting setup_hook initialization")
        self.session: aiohttp.ClientSession = aiohttp.ClientSession()
        self.prefixes: list[str] = [aiosettings.prefix]

        self.version = democracy_exe.__version__
        self.guild_data = {}
        self.intents.members = True
        self.intents.message_content = True

        logger.debug("Retrieving bot application info")
        app_info = await self.application_info()
        self.bot_app_info = cast(AppInfo, app_info)
        if hasattr(self.bot_app_info, "owner") and self.bot_app_info.owner:
            self.owner_id = self.bot_app_info.owner.id

        # Load extensions will be moved to a separate utility function
        logger.info("Beginning extension loading process")
        await self._load_extensions()

        logger.info("Completed setup_hook initialization")
        # await logger.complete()

    async def on_command_error(self, ctx: commands.Context[commands.Bot], error: commands.CommandError) -> None:
        """Handle errors raised during command invocation.

        Args:
            ctx: The context in which the command was invoked
            error: The error that was raised during command invocation
        """
        if isinstance(error, commands.NoPrivateMessage):
            await ctx.author.send("This command cannot be used in private messages.")
        elif isinstance(error, commands.DisabledCommand):
            await ctx.author.send("Sorry. This command is disabled and cannot be used.")
        elif isinstance(error, commands.CommandInvokeError):
            if hasattr(error, "original"):
                original = error.original
                if not isinstance(original, discord.HTTPException):
                    logger.exception("In %s:", ctx.command.qualified_name, exc_info=original)
        elif isinstance(error, commands.ArgumentParsingError):
            await ctx.send(str(error))
        else:
            raise error

        # await logger.complete()


    def _clear_gateway_data(self) -> None:
        """
        Clear gateway data older than one week.

        This method removes entries from the `identifies` and `resumes` dictionaries
        that are older than one week. It iterates through each shard's list of dates
        and deletes the entries that are older than the specified time frame.

        Returns
        -------
            None

        """
        one_week_ago = discord.utils.utcnow() - datetime.timedelta(days=7)
        for shard_id, dates in self.identifies.items():
            to_remove = [index for index, dt in enumerate(dates) if dt < one_week_ago]
            for index in reversed(to_remove):
                del dates[index]

        for shard_id, dates in self.resumes.items():
            to_remove = [index for index, dt in enumerate(dates) if dt < one_week_ago]
            for index in reversed(to_remove):
                del dates[index]

    async def before_identify_hook(self, shard_id: int, *, initial: bool) -> None:  # type: ignore
        """
        Perform actions before identifying the shard.

        This method is called before the bot identifies the shard with the Discord gateway.
        It clears old gateway data and appends the current timestamp to the identifies list
        for the given shard ID.

        Args:
        ----
            shard_id (int): The ID of the shard that is about to identify.
            initial (bool): Whether this is the initial identification of the shard.

        Returns:
        -------
            None

        """
        self._clear_gateway_data()
        self.identifies[shard_id].append(discord.utils.utcnow())
        await super().before_identify_hook(shard_id, initial=initial)


    def get_guild_prefixes(self, guild: discord.abc.Snowflake | None, *, local_inject=_prefix_callable) -> list[str]:
        """
        Retrieve the command prefixes for a specific guild.

        This function generates a list of command prefixes for the bot based on the provided guild.
        If the guild is None, it returns the default prefixes. The function uses a proxy message
        to simulate a message from the guild and retrieves the prefixes using the local_inject function.

        Args:
        ----
            guild (Optional[discord.abc.Snowflake]): The guild for which to retrieve the command prefixes.
            local_inject (Callable): A callable function to inject the local context for prefix retrieval.

        Returns:
        -------
            list[str]: A list of command prefixes for the specified guild.

        """
        proxy_msg = ProxyObject(guild)
        return local_inject(self, proxy_msg)  # type: ignore  # lying

    async def query_member_named(
        self, guild: discord.Guild, argument: str, *, cache: bool = False
    ) -> discord.Member | None:
        """
        Query a member by their name, name + discriminator, or nickname.

        This asynchronous function searches for a member in the specified guild
        by their name, name + discriminator (e.g., username#1234), or nickname.
        It can optionally cache the results of the query.

        Args:
        ----
            guild (discord.Guild): The guild to query the member in.
            argument (str): The name, nickname, or name + discriminator combo to check.
            cache (bool): Whether to cache the results of the query. Defaults to False.

        Returns:
        -------
            Optional[discord.Member]: The member matching the query or None if not found.

        """
        if len(argument) > 5 and argument[-5] == "#":
            username, _, discriminator = argument.rpartition("#")
            members = await guild.query_members(username, limit=100, cache=cache)
            return discord.utils.get(members, name=username, discriminator=discriminator)
        else:
            members = await guild.query_members(argument, limit=100, cache=cache)

            return discord.utils.find(lambda m: m.name == argument or m.nick == argument, members)  # pylint: disable=consider-using-in # pyright: ignore[reportAttributeAccessIssue]

    async def get_or_fetch_member(self, guild: discord.Guild, member_id: int) -> discord.Member | None:
        """
        Retrieve a member from the cache or fetch from the API if not found.

        This asynchronous function attempts to retrieve a member from the cache
        in the specified guild using the provided member ID. If the member is not
        found in the cache, it fetches the member from the Discord API. The function
        handles rate limiting and returns the member if found, or None if not found.

        Args:
        ----
            guild (discord.Guild): The guild to look in.
            member_id (int): The member ID to search for.

        Returns:
        -------
            Optional[discord.Member]: The member if found, or None if not found.

        """
        member = guild.get_member(member_id)
        if member is not None:
            return member

        shard: discord.ShardInfo = self.get_shard(guild.shard_id)  # type: ignore  # will never be None
        if shard.is_ws_ratelimited():
            try:
                member = await guild.fetch_member(member_id)
            except discord.HTTPException:
                return None
            else:
                return member

        members = await guild.query_members(limit=1, user_ids=[member_id], cache=True)
        return members[0] if members else None


    def get_session_id(self, message: discord.Message | discord.Thread) -> str:
        """
        Generate a session ID for the given message.

        This function generates a session ID based on the message context.
        The session ID is used as a key for the history session and as an identifier for logs.

        Args:
        ----
            message (discord.Message): The message or event dictionary.

        Returns:
        -------
            str: The generated session ID.

        Notes:
        -----
            - If the message is a direct message (DM), the session ID is based on the user ID.
            - If the message is from a guild (server) channel, the session ID is based on the channel ID.

        """
        # ctx: Context = await self.get_context(message)  # type: ignore
        if isinstance(message, discord.Thread):
            is_dm: bool = str(message.starter_message.channel.type) == "private"  # pyright: ignore[reportAttributeAccessIssue]
            user_id: int = message.starter_message.author.id  # pyright: ignore[reportAttributeAccessIssue]
            channel_id = message.starter_message.channel.name  # pyright: ignore[reportAttributeAccessIssue]
        elif isinstance(message, discord.Message):
            is_dm: bool = str(message.channel.type) == "private"  # pyright: ignore[reportAttributeAccessIssue]
            user_id: int = message.author.id  # pyright: ignore[reportAttributeAccessIssue]
            channel_id = message.channel.name  # pyright: ignore[reportAttributeAccessIssue]

        return f"discord_{user_id}" if is_dm else f"discord_{channel_id}"  # pyright: ignore[reportAttributeAccessIssue] # pylint: disable=possibly-used-before-assignment

    async def resolve_member_ids(
        self, guild: discord.Guild, member_ids: Iterable[int]
    ) -> AsyncIterator[discord.Member]:
        """
        Bulk resolve member IDs to member instances, if possible.

        This asynchronous function attempts to resolve a list of member IDs to their corresponding
        member instances within a specified guild. Members that cannot be resolved are discarded
        from the list. The function yields the resolved members lazily using an asynchronous iterator.

        Note:
        ----
            The order of the resolved members is not guaranteed to be the same as the input order.

        Args:
        ----
            guild (discord.Guild): The guild to resolve members from.
            member_ids (Iterable[int]): An iterable of member IDs to resolve.

        Yields:
        ------
            discord.Member: The resolved members.

        """
        needs_resolution = []
        for member_id in member_ids:
            member = guild.get_member(member_id)
            if member is not None:
                yield member
            else:
                needs_resolution.append(member_id)

        total_need_resolution = len(needs_resolution)
        if total_need_resolution == 1:
            shard: discord.ShardInfo = self.get_shard(guild.shard_id)  # type: ignore  # will never be None
            if shard.is_ws_ratelimited():
                try:
                    member = await guild.fetch_member(needs_resolution[0])
                except discord.HTTPException:
                    pass
                else:
                    yield member
            else:
                members = await guild.query_members(limit=1, user_ids=needs_resolution, cache=True)
                if members:
                    yield members[0]
        elif total_need_resolution <= 100:
            # Only a single resolution call needed here
            resolved = await guild.query_members(limit=100, user_ids=needs_resolution, cache=True)
            for member in resolved:
                yield member
        else:
            # We need to chunk these in bits of 100...
            for index in range(0, total_need_resolution, 100):
                to_resolve = needs_resolution[index : index + 100]
                members = await guild.query_members(limit=100, user_ids=to_resolve, cache=True)
                for member in members:
                    yield member

    async def on_ready(self) -> None:
        """Handle the event when the bot is ready."""
        if not self.user:
            logger.error("Bot user is not initialized")
            return

        print(f"Logged in as {self.user} (ID: {self.user.id})")
        print("------")
        self.invite = f"https://discordapp.com/api/oauth2/authorize?client_id={self.user.id}&scope=bot&permissions=0"
        self.guild_data = await preload_guild_data()
        print(
            f"""Logged in as {self.user}..
            Serving {len(self.users)} users in {len(self.guilds)} guilds
            Invite: {self.invite}
        """
        )
        game = Game("DemocracyExe")
        await self.change_presence(status=Status.online, activity=game)

        if not hasattr(self, "uptime"):
            self.uptime = discord.utils.utcnow()

        logger.info(f"Ready: {self.user} (ID: {self.user.id})")
        # await logger.complete()

    async def on_shard_resumed(self, shard_id: int) -> None:
        """
        Handle the event when a shard resumes.

        This method is called when a shard successfully resumes its connection
        to the Discord gateway. It logs the shard ID and the timestamp of the
        resume event.

        Args:
        ----
            shard_id (int): The ID of the shard that resumed.

        Returns:
        -------
            None

        """
        logger.info("Shard ID %s has resumed...", shard_id)
        self.resumes[shard_id].append(discord.utils.utcnow())
        # await logger.complete()

    @property
    def owner(self) -> discord.User:
        """
        Retrieve the owner of the bot.

        This property returns the owner of the bot as a discord.User object.
        The owner information is retrieved from the bot's application info.

        Returns
        -------
            discord.User: The owner of the bot.

        """
        return self.bot_app_info.owner  # pyright: ignore[reportAttributeAccessIssue]

    async def close(self) -> None:
        """Close the bot and its associated resources."""
        try:
            await super().close()
        finally:
            if hasattr(self, "session") and not self.session.closed:
                await self.session.close()

    async def start(self, *args: Any, **kwargs: Any) -> None:
        """Start the bot and connect to Discord."""
        token = aiosettings.discord_token.get_secret_value() # pylint: disable=no-member
        await super().start(token, reconnect=True)

    async def _load_extensions(self) -> None:
        """Load bot extensions.

        This method loads all extensions from the cogs directory.
        It uses the extension_utils module to discover and load extensions.
        """
        from democracy_exe.chatbot.utils.extension_utils import extensions, load_extensions

        logger.debug("Looking for extensions in cogs directory")
        extensions_found = list(extensions())
        logger.info(f"Found extensions: {extensions_found}")

        try:
            await load_extensions(self, extensions_found)
        except Exception as e:
            logger.error(f"Failed to load extensions: {e}")
            logger.exception("Extension loading failed")
            raise


    async def my_background_task(self) -> None:
        """Run a background task that sends a counter message to a specific channel every 60 seconds.

        This asynchronous method waits until the bot is ready, then continuously increments a counter
        and sends its value to a predefined Discord channel every 60 seconds.
        """
        await self.wait_until_ready()
        counter = 0

        channel_id = aiosettings.discord_general_channel
        channel = self.get_channel(channel_id)
        while not self.is_closed():
            counter += 1
            if channel and isinstance(channel, (TextChannel, DMChannel, Thread)):
                await channel.send(str(counter))
            await asyncio.sleep(60)  # task runs every 60 seconds

    async def on_worker_monitor(self) -> None:
        """Monitor and log the status of worker tasks.

        This asynchronous method waits until the bot is ready, then continuously
        logs the status of worker tasks every 10 seconds.
        """
        await self.wait_until_ready()
        counter = 0

        while not self.is_closed():
            counter += 1
            logger.info(f"Worker monitor iteration: {counter}")
            await asyncio.sleep(10)

    # # NOTE: attempting to refactor the on_message method to be more readable and maintainable
    # async def on_message(self, message: discord.Message) -> None:
    #     """Process incoming messages and route them through the AI pipeline."""
    #     if not self._should_process_message(message):
    #         return

    #     if message.reference is not None:
    #         await self._handle_reply(message, message.reference.resolved, message.content)
    #         return

    #     if self.enable_ai and self._is_gpt_channel(message.channel):
    #         await self._process_ai_message(message)
    #     else:
    #         logger.info("AI is disabled, skipping message processing... with llm")
    #         await self.process_commands(message)




    # # NOTE: enhanced version of the on_message method
    # # see: https://github.com/bossjones/goob_ai/blob/a63f43ab3592542097e762349d53955bcc97ad1d/src/goob_ai/goob_bot.py
    # # @pysnooper.snoop(thread_info=True, max_variable_length=None, depth=10)
    # # Process incoming messages and route them through the AI pipeline
    # async def on_message(self, message: discord.Message) -> None:
    #     """Process incoming messages and route them through the AI pipeline.

    #     Args:
    #         message: The Discord message to process
    #     """

    #     # Check if bot user is properly initialized
    #     if not self.user:
    #         logger.error("Bot user is not initialized")
    #         return

    #     # Ignore messages from the bot itself to prevent feedback loops
    #     if message.author == self.user:
    #         logger.info("Skipping message from bot itself")
    #         return

    #     # Ignore @everyone and @here
    #     if "@here" in message.content or "@everyone" in message.content:
    #         logger.info("Skipping message with @everyone or @here")
    #         return


    #     # TODO: This is where all the AI logic is going to go
    #     logger.info(f"Thread message to process - {message.author}: {message.content[:50]}")  # pyright: ignore[reportAttributeAccessIssue] # type: ignore
    #     if message.author.bot:
    #         logger.info(f"Skipping message from bot itself, message.author.bot = {message.author.bot}")
    #         return

    #     # skip messages that start w/ bot's prefix
    #     if message.content.startswith(aiosettings.prefix):  # pyright: ignore[reportAttributeAccessIssue]
    #         logger.info(f"Skipping message that starts with {aiosettings.prefix}")
    #         return


    #     # NOTE: on discord.message.reference, see: https://discordpy.readthedocs.io/en/stable/api.html#discord.Message.reference
    #     # #     reference: Optional[:class:`~discord.MessageReference`]
    #     # The message that this message references. This is only applicable to messages of
    #     # type :attr:`MessageType.pins_add`, crossposted messages created by a
    #     # followed channel integration, or message replies.

    #     # Check if this message is a reply to another message
    #     if message.reference is not None:
    #         # Get the message being replied to
    #         ref_message = message.reference.resolved

    #         # Check if the replied-to message was from this bot
    #         if ref_message.author.id == message.author.bot:
    #             # Remove any @ mentions of the bot from the message content
    #             content = message.content.replace(f"<@{message.author.bot}>", "").strip()
    #             # Update the message content with cleaned version
    #             message.content = content.strip()

    #             # Show typing indicator while generating response
    #             await message.channel.trigger_typing()
    #             # Generate AI response to the user's message
    #             response = await self.__generate_response(message)

    #             # Send the response as a reply to maintain thread context
    #             await message.reply(response)
    #         # Exit message handling since we've processed the reply
    #         return

    #     # if AI is enabled and message is in the GPT channel, process the message
    #     if self.enable_ai and (str(message.channel.type) == "text" and message.channel.id == 1240294186201124929):
    #         logger.info("AI is enabled, processing message...")
    #         # Check if the bot is mentioned in the message
    #         if self.user.mentioned_in(message):
    #             # Get or create a thread for this conversation
    #             thread = await self.message_handler._get_thread(message)
    #             if thread is None:
    #                 return

    #             # Extract thread and user IDs and convert to strings for consistency
    #             thread_id = thread.id
    #             user_id = message.author.id

    #             if isinstance(thread_id, int):
    #                 thread_id = str(thread_id)
    #             if isinstance(user_id, int):
    #                 user_id = str(user_id)

    #             # Format the input data for the AI processing pipeline
    #             input_data = {
    #                 "messages": [format_inbound_message(message)],
    #                 "configurable": {"thread_id": thread_id, "user_id": user_id}
    #             }

    #             # Process message through AI pipeline and handle any errors
    #             try:
    #                 response = await self.message_handler.stream_bot_response(self.graph, input_data)
    #             except Exception as e:
    #                 logger.exception(f"Error streaming bot response: {e}")
    #                 response = "An error occurred while processing your message."

    #             # Log that we're about to send the response
    #             logger.debug("Sending response to thread...")

    #             # Split response into chunks if it exceeds Discord's message length limit
    #             chunks = [response[i:i+2000] for i in range(0, len(response), 2000)]

    #             # Send each chunk as a separate message in the thread
    #             for chunk in chunks:
    #                 await thread.send(chunk)
    #     else:
    #         logger.info("AI is disabled, skipping message processing... with llm")

    #         # This function processes the commands that have been registered to the bot and other groups. Without this coroutine, none of the commands will be triggered.
    #         await self.process_commands(message)

    # NOTE: original version of the on_message method
    # TODO: figure out how to refactor this to be more readable and maintainable and make tests pass
    # Process incoming messages and route them through the AI pipeline
    async def on_message(self, message: Message) -> None:
        """Process incoming messages and route them through the AI pipeline.

        Args:
            message: The Discord message to process
        """

        # Check if bot user is properly initialized
        if not self.user:
            logger.error("Bot user is not initialized")
            return

        # Ignore messages from the bot itself to prevent feedback loops
        if message.author == self.user:
            return


        if self.enable_ai:
            logger.info("AI is enabled, processing message...")
            # Check if the bot is mentioned in the message
            if self.user.mentioned_in(message):
                # Get or create a thread for this conversation
                thread = await self.message_handler._get_thread(message)
                if thread is None:
                    return

                # Extract thread and user IDs and convert to strings for consistency
                thread_id = thread.id
                user_id = message.author.id

                if isinstance(thread_id, int):
                    thread_id = str(thread_id)
                if isinstance(user_id, int):
                    user_id = str(user_id)

                # Format the input data for the AI processing pipeline
                input_data = {
                    "messages": [format_inbound_message(message)],
                    "configurable": {"thread_id": thread_id, "user_id": user_id}
                }

                # Process message through AI pipeline and handle any errors
                try:
                    response = await self.message_handler.stream_bot_response(self.graph, input_data)
                except Exception as e:
                    logger.exception(f"Error streaming bot response: {e}")
                    response = "An error occurred while processing your message."

                # Log that we're about to send the response
                logger.debug("Sending response to thread...")

                # Split response into chunks if it exceeds Discord's message length limit
                chunks = [response[i:i+2000] for i in range(0, len(response), 2000)]

                # Send each chunk as a separate message in the thread
                for chunk in chunks:
                    await thread.send(chunk)
        else:
            logger.info("AI is disabled, skipping message processing... with llm")
            await self.process_commands(message)


    # NOTE: attempting to refactor the on_message method to be more readable and maintainable
    def _should_process_message(self, message: discord.Message) -> bool:
        """Determine if a Discord message should be processed by the bot's AI pipeline.

        This method implements filtering logic to determine whether a message should be
        processed by the bot's AI pipeline. It checks multiple conditions to prevent
        unwanted message processing and potential feedback loops.

        Args:
            message: The Discord message to evaluate. Contains metadata about the
                message including its author, content, and channel information.

        Returns:
            bool: True if the message should be processed by the bot's AI pipeline,
                False if the message should be ignored.

        Note:
            Messages are ignored if any of these conditions are met:
            - Bot user is not initialized
            - Message is from the bot itself
            - Message contains @here or @everyone mentions
            - Message is from another bot
            - Message starts with the bot's command prefix

        Example:
            ```python
            if self._should_process_message(message):
                await self._process_ai_message(message)
            ```
        """
        if not self.user:
            logger.error("Bot user is not initialized")
            return False

        if message.author == self.user:
            logger.info("Skipping message from bot itself")
            return False

        if "@here" in message.content or "@everyone" in message.content:
            logger.info("Skipping message with @everyone or @here")
            return False

        if message.author.bot:
            logger.info(f"Skipping message from bot itself, message.author.bot = {message.author.bot}")
            return False

        if message.content.startswith(aiosettings.prefix):
            logger.info(f"Skipping message that starts with {aiosettings.prefix}")
            return False

        return True

    async def _handle_reply(
        self,
        message: discord.Message,
        reply_to: discord.Message,
        content: str
    ) -> bool:
        """Handle a reply to a message.

        Processes a reply to a message, checking if it's a reply to the bot and
        handling the response appropriately.

        Args:
            message: The current message being processed
            reply_to: The message being replied to
            content: The content of the current message

        Returns:
            bool: True if the message was handled as a reply, False otherwise

        Raises:
            discord.DiscordException: If there's an error sending the response
        """
        # Skip if not replying to bot
        if reply_to.author.id != self.user.id:
            logger.info("Skipping reply to non-bot message")
            return False

        try:
            # Process reply and send response
            response = await self._process_message(message, content)
            await message.reply(response)
            return True
        except Exception as e:
            logger.exception(f"Error handling reply: {e!s}")
            raise

        return False

    def _is_gpt_channel(self, channel: discord.TextChannel) -> bool:
        """Check if the given channel is the designated GPT channel.

        This method determines if a channel is the designated GPT channel by checking
        its type and ID. The GPT channel is where AI-powered interactions are allowed
        to take place.

        Args:
            channel: The Discord text channel to evaluate. Must be a TextChannel
                instance containing channel metadata and properties.

        Returns:
            bool: True if the channel is the designated GPT channel, False otherwise.

        Note:
            A channel is considered a GPT channel if:
            - It is a text channel (channel.type == "text")
            - Its ID matches the predefined GPT channel ID (1240294186201124929)
        """
        return str(channel.type) == "text" and channel.id == 1240294186201124929

    async def _process_ai_message(self, message: discord.Message) -> None:
        """Process a message through the bot's AI pipeline.

        This method handles the AI processing workflow for a Discord message. It checks if the bot
        is mentioned, creates or retrieves a thread for the conversation, and processes the message
        through the AI pipeline to generate a response.

        Args:
            message: The Discord message to process. Must contain the message content,
                author information, and channel metadata.

        Returns:
            None

        Raises:
            Exception: If there's an error during the AI processing pipeline or response streaming.
                The error is logged and a generic error message is sent to the user.

        Note:
            The method performs the following steps:
            1. Checks if the bot is mentioned in the message
            2. Creates or retrieves a thread for the conversation
            3. Formats the message for AI processing
            4. Processes the message through the AI pipeline
            5. Splits and sends the response in chunks if necessary (Discord's 2000 char limit)
        """
        # Exit early if the bot isn't explicitly mentioned in the message
        if not self.user.mentioned_in(message):
            return

        # Get or create a thread for this conversation using the message handler
        thread = await self.message_handler._get_thread(message)
        # Exit if thread creation/retrieval failed
        if thread is None:
            return

        # Convert thread and user IDs to strings for consistent handling in the AI pipeline
        thread_id = str(thread.id)
        user_id = str(message.author.id)

        # Prepare the input data structure for the AI pipeline
        # format_inbound_message converts the Discord message to a format the AI can process
        input_data = {
            "messages": [format_inbound_message(message)],  # Convert message to AI-readable format
            "configurable": {"thread_id": thread_id, "user_id": user_id}  # Add metadata for context
        }

        try:
            # Process the message through the AI pipeline and get the response
            # Uses the LangGraph instance (self.graph) to generate the response
            response = await self.message_handler.stream_bot_response(self.graph, input_data)
        except Exception as e:
            # Log any errors that occur during processing and return a generic error message
            logger.exception(f"Error streaming bot response: {e}")
            response = "An error occurred while processing your message."

        # Log that we're about to send the response
        logger.debug("Sending response to thread...")

        # Split response into chunks of 2000 characters to comply with Discord's message length limit
        chunks = [response[i:i+2000] for i in range(0, len(response), 2000)]

        # Send each chunk as a separate message in the thread
        for chunk in chunks:
            await thread.send(chunk)

</document_content>
</document>
<document index="13">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/core/types.py</source>
<document_content>
"""Type definitions for the chatbot core."""
from __future__ import annotations

from typing import TYPE_CHECKING, Protocol


if TYPE_CHECKING:
    from discord.ext import commands
    from redis.asyncio import ConnectionPool as RedisConnectionPool

class BotProtocol(Protocol):
    """Protocol defining the interface for the DemocracyBot."""

    pool: RedisConnectionPool | None
    command_prefix: str | list[str]

</document_content>
</document>
<document index="14">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/utils/__init__.py</source>
<document_content>
"""Discord utility functions initialization."""
from __future__ import annotations

from democracy_exe.chatbot.utils.discord_utils import (
    aio_extensions,
    create_embed,
    extensions,
    format_user_info,
    get_member_roles_hierarchy,
    get_or_create_role,
    has_required_permissions,
    safe_delete_messages,
    send_chunked_message,
    setup_channel_permissions,
)
from democracy_exe.chatbot.utils.message_utils import format_inbound_message


__all__ = [
    "extensions",
    "aio_extensions",
    "has_required_permissions",
    "send_chunked_message",
    "create_embed",
    "get_or_create_role",
    "safe_delete_messages",
    "get_member_roles_hierarchy",
    "setup_channel_permissions",
    "format_user_info",
    "format_inbound_message",
]

</document_content>
</document>
<document index="15">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/utils/discord_utils.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
"""Discord utility functions.

This module provides utility functions for Discord bot operations,
including message processing, permission checking, and file handling.
"""
from __future__ import annotations

import asyncio
import os
import pathlib
import sys
import uuid

from collections.abc import AsyncIterator
from pathlib import Path
from typing import Any, Dict, List, NoReturn, Optional, Set, Tuple, Union, cast

import aiofiles
import aiohttp
import discord
import rich
import structlog

from codetiming import Timer
from discord import Attachment, Client, File, Guild, Member, Message, PermissionOverwrite, Role, TextChannel, User
from discord.ext import commands
from logging_tree import printout


logger = structlog.get_logger(__name__)

from democracy_exe.bot_logger import generate_tree, get_lm_from_tree
from democracy_exe.models.loggers import LoggerModel
from democracy_exe.utils import async_, shell


def extensions() -> list[str]:
    """Get list of extension paths.

    Returns:
        List of extension paths as strings
    """
    cogs_dir = Path(__file__).parent.parent / "cogs"
    if not cogs_dir.exists():
        raise FileNotFoundError(f"Cogs directory not found: {cogs_dir}")
    return [
        f"chatbot.cogs.{f.stem}"
        for f in cogs_dir.glob("**/*.py")
        if not f.name.startswith("_") and f.name != "__init__.py"
    ]

async def aio_extensions() -> list[str]:
    """Get list of extension paths asynchronously.

    Returns:
        List of extension paths as strings
    """
    return extensions()


def has_required_permissions(
    member: Member, channel: TextChannel, required_perms: set[str]
) -> bool:
    """Check if a member has the required permissions in a channel.

    Args:
        member: The member to check permissions for
        channel: The channel to check permissions in
        required_perms: Set of permission names to check

    Returns:
        bool: True if member has all required permissions, False otherwise

    Raises:
        ValueError: If invalid permission names are provided
    """
    try:
        channel_perms = channel.permissions_for(member)
        return all(
            getattr(channel_perms, perm, None) is True for perm in required_perms
        )
    except AttributeError as e:
        logger.error(f"Invalid permission name in {required_perms}: {e}")
        raise ValueError(f"Invalid permission name: {e}") from e
    except Exception as e:
        logger.error(f"Error checking permissions: {e}")
        raise


async def send_chunked_message(
    channel: TextChannel, content: str, chunk_size: int = 2000
) -> list[Message]:
    """Send a long message in chunks to avoid Discord's message length limit.

    Args:
        channel: The channel to send the message to
        content: The message content to send
        chunk_size: Maximum size of each message chunk (default: 2000)

    Returns:
        List of sent messages

    Raises:
        ValueError: If chunk_size is invalid
        discord.HTTPException: If message sending fails
    """
    try:
        if chunk_size <= 0 or chunk_size > 2000:
            raise ValueError("chunk_size must be between 1 and 2000")

        messages: list[Message] = []
        for i in range(0, len(content), chunk_size):
            chunk = content[i:i + chunk_size]
            try:
                msg = await channel.send(chunk)
                messages.append(msg)
                await asyncio.sleep(0.5)  # Rate limiting prevention
            except discord.HTTPException as e:
                logger.error(f"Failed to send message chunk: {e}")
                raise

        return messages
    except Exception as e:
        logger.error(f"Error in send_chunked_message: {e}")
        raise


def create_embed(
    title: str,
    description: str,
    color: discord.Color | None = None,
    fields: list[dict[str, str]] | None = None,
    footer: str | None = None,
    thumbnail_url: str | None = None,
) -> discord.Embed:
    """Create a Discord embed with the specified parameters.

    Args:
        title: The embed title
        description: The embed description
        color: The embed color (default: None)
        fields: List of field dictionaries with 'name' and 'value' keys (default: None)
        footer: Footer text (default: None)
        thumbnail_url: URL for thumbnail image (default: None)

    Returns:
        discord.Embed: The created embed

    Raises:
        ValueError: If required parameters are missing or invalid
    """
    try:
        embed = discord.Embed(
            title=title,
            description=description,
            color=color or discord.Color.blue()
        )

        if fields:
            for field in fields:
                if "name" not in field or "value" not in field:
                    raise ValueError("Field must contain 'name' and 'value' keys")
                embed.add_field(
                    name=field["name"],
                    value=field["value"],
                    inline=bool(field.get("inline", True))
                )

        if footer:
            embed.set_footer(text=footer)

        if thumbnail_url:
            embed.set_thumbnail(url=thumbnail_url)

        return embed
    except Exception as e:
        logger.error(f"Error creating embed: {e}")
        raise


async def get_or_create_role(
    guild: Guild, role_name: str, **role_params: Any
) -> Role:
    """Get an existing role or create a new one if it doesn't exist.

    Args:
        guild: The guild to get/create the role in
        role_name: Name of the role
        **role_params: Additional role parameters (color, permissions, etc.)

    Returns:
        The found or created role

    Raises:
        discord.Forbidden: If bot lacks permission to manage roles
        discord.HTTPException: If role creation fails
    """
    try:
        existing_role = discord.utils.get(guild.roles, name=role_name)
        if existing_role:
            return existing_role

        logger.info(f"Creating new role: {role_name}")
        return await guild.create_role(name=role_name, **role_params)
    except discord.Forbidden as e:
        logger.error(f"Insufficient permissions to manage roles: {e}")
        raise
    except Exception as e:
        logger.error(f"Error in get_or_create_role: {e}")
        raise


async def safe_delete_messages(
    messages: list[Message], delay: float | None = None
) -> None:
    """Safely delete messages with error handling and optional delay.

    Args:
        messages: List of messages to delete
        delay: Optional delay before deletion in seconds

    Raises:
        discord.Forbidden: If bot lacks permission to delete messages
        discord.HTTPException: If message deletion fails
    """
    try:
        if delay:
            await asyncio.sleep(delay)

        for msg in messages:
            try:
                await msg.delete()
                await asyncio.sleep(0.5)  # Rate limiting prevention
            except discord.NotFound:
                logger.debug(f"Message {msg.id} already deleted")
            except discord.Forbidden as e:
                logger.error(f"No permission to delete message {msg.id}: {e}")
                raise
            except Exception as e:
                logger.error(f"Error deleting message {msg.id}: {e}")
                continue
    except Exception as e:
        logger.error(f"Error in safe_delete_messages: {e}")
        raise


def get_member_roles_hierarchy(member: Member) -> list[Role]:
    """Get member's roles sorted by hierarchy position.

    Args:
        member: The member to get roles for

    Returns:
        List of roles sorted by position (highest first)

    Raises:
        ValueError: If member has no roles
    """
    try:
        roles = sorted(member.roles, key=lambda r: r.position, reverse=True)
        if not roles:
            raise ValueError(f"Member {member.name} has no roles")
        return roles
    except Exception as e:
        logger.error(f"Error getting member roles hierarchy: {e}")
        raise


async def setup_channel_permissions(
    channel: TextChannel,
    role_overwrites: dict[Role, PermissionOverwrite],
) -> None:
    """Set up channel permission overwrites for roles.

    Args:
        channel: The channel to set permissions for
        role_overwrites: Dictionary mapping roles to their permission overwrites

    Raises:
        discord.Forbidden: If bot lacks permission to manage channel permissions
        discord.HTTPException: If setting permissions fails
    """
    try:
        await channel.edit(overwrites=role_overwrites)
    except discord.Forbidden as e:
        logger.error(f"No permission to edit channel overwrites: {e}")
        raise
    except Exception as e:
        logger.error(f"Error setting channel permissions: {e}")
        raise


def format_user_info(user: User | Member) -> str:
    """Format user information into a readable string.

    Args:
        user: The user or member to format information for

    Returns:
        Formatted string containing user information

    Raises:
        ValueError: If user object is invalid
    """
    try:
        info = [
            f"Username: {user.name}",
            f"ID: {user.id}",
            f"Created: {user.created_at.strftime('%Y-%m-%d %H:%M:%S')}",
        ]

        if isinstance(user, Member):
            info.extend([
                f"Nickname: {user.nick or 'None'}",
                f"Joined: {user.joined_at.strftime('%Y-%m-%d %H:%M:%S') if user.joined_at else 'Unknown'}",
                f"Top Role: {user.top_role.name}",
            ])

        return "\n".join(info)
    except Exception as e:
        logger.error(f"Error formatting user info: {e}")
        raise ValueError(f"Invalid user object: {e}")

def unlink_orig_file(a_filepath: str) -> str:
    """Delete the specified file and return its path.

    Args:
        a_filepath: The path to the file to be deleted

    Returns:
        The path of the deleted file

    Raises:
        OSError: If file deletion fails
    """
    try:
        rich.print(f"deleting ... {a_filepath}")
        os.unlink(f"{a_filepath}")
        return a_filepath
    except OSError as e:
        logger.error(f"Error deleting file {a_filepath}: {e}")
        raise

async def aunlink_orig_file(a_filepath: str) -> str:
    """Delete the specified file asynchronously and return its path.

    Args:
        a_filepath: The path to the file to be deleted

    Returns:
        The path of the deleted file

    Raises:
        OSError: If file deletion fails
    """
    try:
        rich.print(f"deleting ... {a_filepath}")
        await aiofiles.os.unlink(f"{a_filepath}")
        return a_filepath
    except OSError as e:
        logger.error(f"Error deleting file {a_filepath}: {e}")
        raise


async def details_from_file(
    path_to_media_from_cli: str, cwd: str | None = None
) -> tuple[str, str, str]:
    """Generate input and output file paths and retrieve the timestamp of the input file.

    Args:
        path_to_media_from_cli: The path to the media file provided via command line
        cwd: The current working directory (default: None)

    Returns:
        Tuple containing:
            - Input file path
            - Output file path
            - File timestamp

    Raises:
        FileNotFoundError: If input file doesn't exist
        OSError: If file stats cannot be retrieved
    """
    try:
        p = pathlib.Path(path_to_media_from_cli)
        full_path_input_file = f"{p.stem}{p.suffix}"
        full_path_output_file = f"{p.stem}_smaller.mp4"
        rich.print(full_path_input_file)
        rich.print(full_path_output_file)

        if sys.platform == "darwin":
            get_timestamp = await shell._aio_run_process_and_communicate(
                ["stat", "-f", "%Sm", "-t", "%Y-%m-%d %H:%M:%S", str(p.absolute())], cwd=cwd
            )
        elif sys.platform == "linux":
            get_timestamp = await shell._aio_run_process_and_communicate(
                ["stat", "-c", "%y", f"{p.stem}{p.suffix}"], cwd=cwd
            )
        else:
            raise OSError(f"Unsupported platform: {sys.platform}")

        return full_path_input_file, full_path_output_file, get_timestamp
    except Exception as e:
        logger.error(f"Error getting file details: {e}")
        raise


def filter_empty_string(a_list: list[str]) -> list[str]:
    """Filter out empty strings from a list of strings.

    Args:
        a_list: The list of strings to be filtered

    Returns:
        A new list containing only non-empty strings
    """
    return list(filter(lambda x: x != "", a_list))


async def worker(name: str, queue: asyncio.Queue) -> NoReturn:
    """Process tasks from the queue.

    Args:
        name: The name of the worker
        queue: The queue from which tasks are retrieved

    Raises:
        Exception: If task execution fails
    """
    logger.info(f"starting worker ... {name}")

    while True:
        try:
            co_cmd_task = await queue.get()
            logger.debug(f"co_cmd_task = {co_cmd_task}")

            await shell.run_coroutine_subprocess(cmd=co_cmd_task.cmd, uri=co_cmd_task.uri)
            queue.task_done()
            logger.info(f"{name} ran {co_cmd_task.name} with arguments {co_cmd_task}")
        except Exception as e:
            logger.error(f"Error in worker {name}: {e}")
            queue.task_done()
            continue


async def co_task(name: str, queue: asyncio.Queue) -> AsyncIterator[None]:
    """Process tasks from the queue with timing.

    Args:
        name: The name of the task
        queue: The queue from which tasks are retrieved

    Yields:
        None after each task is processed

    Raises:
        Exception: If task execution fails
    """
    logger.info(f"starting task ... {name}")

    timer = Timer(text=f"Task {name} elapsed time: {{:.1f}}")
    while not queue.empty():
        try:
            co_cmd_task = await queue.get()
            logger.info(f"Task {name} running")
            timer.start()
            await shell.run_coroutine_subprocess(cmd=co_cmd_task.cmd, uri=co_cmd_task.uri)
            timer.stop()
            yield
            # await logger.complete()
        except Exception as e:
            logger.error(f"Error in task {name}: {e}")
            continue


@async_.to_async
def get_logger_tree_printout() -> None:
    """Print the logger tree structure."""
    printout()


def dump_logger_tree() -> None:
    """Dump the logger tree structure."""
    rootm = generate_tree()
    logger.debug(rootm)


def dump_logger(logger_name: str) -> Any:
    """Dump the logger tree structure for a specific logger.

    Args:
        logger_name: The name of the logger to retrieve the tree structure for

    Returns:
        The logger metadata for the specified logger name

    Raises:
        KeyError: If logger name is not found
    """
    try:
        logger.debug(f"getting logger {logger_name}")
        rootm: LoggerModel = generate_tree()
        return get_lm_from_tree(rootm, logger_name)
    except Exception as e:
        logger.error(f"Error dumping logger {logger_name}: {e}")
        raise

</document_content>
</document>
<document index="16">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/utils/extension_utils.py</source>
<document_content>
"""Extension utility functions.

This module provides utilities for managing Discord bot extensions/cogs.
"""
from __future__ import annotations

import os
import pathlib

from collections.abc import AsyncIterator, Iterable
from typing import TYPE_CHECKING, Any, List

import aiofiles
import pysnooper
import structlog

from discord.ext import commands


logger = structlog.get_logger(__name__)

from democracy_exe.aio_settings import aiosettings


HERE = os.path.dirname(os.path.dirname(__file__))
print(f"HERE: {HERE}")
print(f"HERE: {HERE}")
print(f"HERE: {HERE}")
print(f"HERE: {HERE}")

if TYPE_CHECKING:
    from democracy_exe.chatbot.core.bot import DemocracyBot

def extensions() -> Iterable[str]:
    """Yield extension module paths.

    This function searches for Python files in the 'cogs' directory relative to the current file's directory.
    It constructs the module path for each file and yields it.

    Yields:
        The module path for each Python file in the 'cogs' directory

    Raises:
        FileNotFoundError: If cogs directory doesn't exist
    """
    cogs_dir = pathlib.Path(HERE) / "cogs"
    logger.error(f"Cogs directory: {cogs_dir}")
    if not cogs_dir.exists():
        raise FileNotFoundError(f"Cogs directory not found: {cogs_dir}")

    for file in cogs_dir.rglob("*.py"):
        logger.error(f"file: {file}")
        logger.error(f"file.name: {file.name}")
        logger.error(f"aiosettings.extension_allowlist: {aiosettings.extension_allowlist}")
        module_name = file.name.replace(".py", "")
        logger.error(f"module_name: {module_name}")
        logger.error(f"file.name != '__init__.py': {file.name != '__init__.py'}")
        logger.error(f"module_name in aiosettings.extension_allowlist: {module_name in aiosettings.extension_allowlist}")
        is_allowed = any(module_name in item for item in aiosettings.extension_allowlist)
        logger.error(f"is_allowed: {is_allowed}")
        if file.name != "__init__.py" and is_allowed:
            # Get path relative to the module root
            base_module_dir = pathlib.Path(HERE).parent.parent

            # relative_path = file.relative_to(pathlib.Path(HERE))
            relative_path = file.relative_to(base_module_dir)
            logger.error(f"Relative path: {relative_path}")
            extension_path = str(relative_path)[:-3].replace(os.sep, ".")
            logger.error(f"Extension path: {extension_path}")
            logger.debug(f"Found extension file: {file}")
            logger.debug(f"Converting to module path: {extension_path}")
            yield extension_path


class AsyncExtensionIterator:
    """Async iterator for discovering extensions."""

    def __init__(self) -> None:
        """Initialize the iterator."""
        self.cogs_path = pathlib.Path(HERE) / "cogs"
        logger.error(f"Cogs path: {self.cogs_path}")
        self.files = None
        self.current_index = 0

    def __aiter__(self) -> AsyncExtensionIterator:
        """Return self as the iterator.

        Returns:
            Self as the async iterator
        """
        return self

    # @pysnooper(thread_info=True, max_variable_length=None, depth=10)
    async def __anext__(self) -> str:
        """Get the next extension path.

        Returns:
            The next extension path

        Raises:
            StopAsyncIteration: When no more extensions are available
            FileNotFoundError: If cogs directory doesn't exist
        """
        if not self.cogs_path.exists():
            logger.error(f"Cogs directory not found: {self.cogs_path}")
            raise FileNotFoundError(f"Cogs directory not found: {self.cogs_path}")

        if self.files is None:
            # Initialize the file list on first iteration
            self.files = list(self.cogs_path.rglob("*.py"))
            logger.debug(f"Found files: {self.files}")
            logger.debug("Successfully initialized async file search")

        while self.current_index < len(self.files):
            file = self.files[self.current_index]
            self.current_index += 1

            # Skip __init__.py files
            logger.error(f"file: {file}")
            logger.error(f"file.name: {file.name}")
            logger.error(f"aiosettings.extension_allowlist: {aiosettings.extension_allowlist}")
            module_name = file.name.replace(".py", "")
            logger.error(f"module_name: {module_name}")
            logger.error(f"file.name != '__init__.py': {file.name != '__init__.py'}")
            logger.error(f"module_name in aiosettings.extension_allowlist: {module_name in aiosettings.extension_allowlist}")
            is_allowed = any(module_name in item for item in aiosettings.extension_allowlist)
            logger.error(f"is_allowed: {is_allowed}")

            # if file is __init__.py or not in allowlist, skip
            if file.name == "__init__.py" or not is_allowed:
                continue

            try:
                # Verify file exists and is readable
                async with aiofiles.open(file) as f:
                    # Just check if we can open it
                    await f.read(1)



                # # For testing:
                # >>> HERE = "/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot"
                # >>> cogs_path = pathlib.Path("/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/cogs")
                # >>> file = pathlib.Path("/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/cogs/twitter.py")
                # >>> base_module_dir = pathlib.Path(HERE).parent.parent
                # >>> relative_path = file.relative_to(base_module_dir)
                # >>> extension_path = str(relative_path).replace(os.sep, ".")[:-3]
                # >>> extension_path
                # 'democracy_exe.chatbot.cogs.twitter'
                # >>> print(f"HERE: {HERE}")
                # HERE: /Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot
                # >>> print(f"cogs_path: {cogs_path}")
                # cogs_path: /Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/cogs
                # >>> print(f"file: {file}")
                # file: /Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/cogs/twitter.py
                # >>> print(f"base_module_dir: {base_module_dir}")
                # base_module_dir: /Users/malcolm/dev/bossjones/democracy-exe
                # >>> print(f"relative_path: {relative_path}")
                # relative_path: democracy_exe/chatbot/cogs/twitter.py
                # >>> print(f"extension_path: {extension_path}")
                # extension_path: democracy_exe.chatbot.cogs.twitter
                # >>>


                # Get path relative to the cogs directory
                base_module_dir = pathlib.Path(HERE).parent.parent

                logger.error(f"base_module_dir: {base_module_dir}")

                relative_path = file.relative_to(base_module_dir)
                logger.error(f"relative_path: {relative_path}")

                extension_path = str(relative_path).replace(os.sep, ".")[:-3]
                logger.error(f"extension_path: {extension_path}")
                logger.error(f"file: {file}")
                logger.error(f"HERE: {HERE}")
                logger.error(f"relative_path: {relative_path}")
                # import bpdb; bpdb.set_trace()


                logger.debug(f"Found extension file: {file}")
                logger.debug(f"Converting to module path: {extension_path}")
                # await logger.complete()
                return extension_path

            except OSError as e:
                logger.warning(f"Skipping inaccessible extension file {file}: {e}")
                continue

        logger.debug("Completed async extension discovery")
        # await logger.complete()
        raise StopAsyncIteration


async def aio_extensions() -> AsyncIterator[str]:
    """Yield extension module paths asynchronously.

    This function asynchronously searches for Python files in the 'cogs' directory
    relative to the current file's directory. It constructs the module path for each
    file and yields it. Uses aiofiles for asynchronous file operations.

    Yields:
        The module path for each Python file in the 'cogs' directory

    Raises:
        FileNotFoundError: If the cogs directory doesn't exist
        OSError: If there's an error accessing the cogs directory
    """
    try:
        async for extension in AsyncExtensionIterator():
            yield extension
    except Exception as e:
        logger.error(f"Error discovering extensions: {e}")
        logger.exception("Extension discovery failed")
        raise


async def load_extensions(bot: DemocracyBot, extension_list: list[str]) -> None:
    """Load a list of extensions into the bot.

    Args:
        bot: The Discord bot instance
        extension_list: List of extension module paths to load

    Raises:
        Exception: If loading an extension fails
    """
    for extension in extension_list:
        try:
            await bot.load_extension(extension)
            logger.info(f"Loaded extension: {extension}")
            # await logger.complete()
        except Exception as e:
            logger.error(f"Failed to load extension {extension}: {e}")
            # await logger.complete()
            raise


async def reload_extension(bot: DemocracyBot, extension: str) -> None:
    """Reload a specific extension.

    Args:
        bot: The Discord bot instance
        extension: Extension module path to reload

    Raises:
        Exception: If reloading the extension fails
    """
    try:
        await bot.reload_extension(extension)
        logger.info(f"Reloaded extension: {extension}")
        # await logger.complete()
    except Exception as e:
        logger.error(f"Failed to reload extension {extension}: {e}")
        # await logger.complete()
        raise


async def unload_extension(bot: DemocracyBot, extension: str) -> None:
    """Unload a specific extension.

    Args:
        bot: The Discord bot instance
        extension: Extension module path to unload

    Raises:
        Exception: If unloading the extension fails
    """
    try:
        await bot.unload_extension(extension)
        logger.info(f"Unloaded extension: {extension}")
        # await logger.complete()
    except Exception as e:
        logger.error(f"Failed to unload extension {extension}: {e}")
        # await logger.complete()
        raise

</document_content>
</document>
<document index="17">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/utils/guild_utils.py</source>
<document_content>
"""Guild utility functions for Discord bot.

This module handles guild-related functionality like data preloading and configuration.
"""
from __future__ import annotations

from typing import Any, Dict

import structlog


logger = structlog.get_logger(__name__)

from democracy_exe.factories import guild_factory


async def preload_guild_data() -> dict[int, dict[str, Any]]:
    """Preload guild data.

    This function initializes and returns a dictionary containing guild data.
    Each guild is represented by its ID and contains a dictionary with the guild's prefix.

    Returns:
        A dictionary where the keys are guild IDs and the values are dictionaries
        containing guild-specific data, such as the prefix.
    """
    logger.info("Preloading guild data...")
    try:
        guilds = [guild_factory.Guild() for _ in range(3)]  # Create 3 guilds for testing
        # await logger.complete()
        return {guild.id: {"prefix": guild.prefix} for guild in guilds}
    except Exception as e:
        logger.error(f"Error preloading guild data: {e}")
        return {}

</document_content>
</document>
<document index="18">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/utils/message_utils.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
"""Message handling utilities for Discord bot.

This module provides utilities for handling Discord messages, including
formatting, thread management, and session handling.
"""
from __future__ import annotations

from typing import Any, Optional, Union, cast

import discord
import structlog

from discord import DMChannel, Guild, Member, Message, TextChannel, Thread, User
from discord.abc import Messageable
from langchain_core.messages import AIMessage, HumanMessage


logger = structlog.get_logger(__name__)


def format_inbound_message(message: Message) -> HumanMessage:
    """Format a Discord message into a HumanMessage for LangGraph processing.

    Args:
        message: The Discord message to format

    Returns:
        A formatted message ready for LangGraph processing

    Raises:
        ValueError: If message formatting fails
    """
    try:
        guild = cast(Optional[Guild], message.guild)
        channel = cast(Union[TextChannel, DMChannel], message.channel)
        author = cast(Union[Member, User], message.author)

        # Build metadata string
        guild_str = f"guild={guild.name}" if guild else ""
        content = f"""<discord {guild_str} channel={channel} author={author!r}>
        {message.content}
        </discord>"""

        logger.debug(f"Formatted message content: {content}")

        if author.global_name is None:
            raise ValueError("Failed to format message: author has no global name")

        return HumanMessage(
            content=content,
            name=str(author.global_name),
            id=str(message.id)
        )
    except Exception as e:
        logger.error(f"Error formatting message: {e}")
        raise ValueError(f"Failed to format message: {e}")


async def get_or_create_thread(message: Message) -> Thread | DMChannel | None:
    """Get or create a thread for the message.

    Args:
        message: The message to get/create a thread for

    Returns:
        Either a Thread object for server channels or DMChannel for direct messages

    Raises:
        discord.HTTPException: If thread creation fails
    """
    try:
        channel = message.channel

        # If this is a DM channel, just return it directly
        if isinstance(channel, DMChannel):
            return channel

        # For regular channels, create a thread
        if isinstance(channel, (TextChannel, Thread)):
            try:
                return await channel.create_thread(name="Response", message=message)
            except discord.HTTPException as e:
                logger.error(f"Failed to create thread: {e}")
                if not hasattr(e, 'status'):
                    e.status = 400
                raise

        return None
    except Exception as e:
        logger.error(f"Error getting/creating thread: {e}")
        raise


def get_session_id(message: Message | Thread) -> str:
    """Generate a session ID for the given message.

    Args:
        message: The message or event dictionary

    Returns:
        The generated session ID

    Notes:
        - If the message is a direct message (DM), the session ID is based on the user ID
        - If the message is from a guild channel, the session ID is based on the channel ID
    """
    try:
        if isinstance(message, Thread):
            starter_message = cast(Message, message.starter_message)
            if starter_message is None:
                raise ValueError("Thread has no starter message")
            channel = cast(Union[DMChannel, Any], starter_message.channel)
            is_dm = str(channel.type) == "private"
            user_id = starter_message.author.id
            channel_id = channel.name if isinstance(channel, Thread) else channel.id
        else:
            channel = cast(Union[DMChannel, Any], message.channel)
            is_dm = str(channel.type) == "private"
            user_id = message.author.id
            channel_id = channel.id

        if user_id is None or channel_id is None:
            raise ValueError("Could not determine user_id or channel_id")

        return f"discord_{user_id}" if is_dm else f"discord_{channel_id}"
    except Exception as e:
        logger.error(f"Error generating session ID: {e}")
        return f"discord_fallback_{discord.utils.utcnow().timestamp()}"


def prepare_agent_input(
    message: Message | Thread,
    user_real_name: str,
    surface_info: dict[str, Any]
) -> dict[str, Any]:
    """Prepare the agent input from the incoming Discord message.

    Args:
        message: The Discord message containing the user input
        user_real_name: The real name of the user who sent the message
        surface_info: The surface information related to the message

    Returns:
        The input dictionary to be sent to the agent

    Raises:
        ValueError: If message processing fails
    """
    try:
        if isinstance(message, Thread):
            starter_message = cast(Message, message.starter_message)
            if starter_message is None:
                raise ValueError("Thread has no starter message")
            content = starter_message.content
            attachments = starter_message.attachments
        else:
            content = message.content
            attachments = message.attachments

        if content is None:
            raise ValueError("Message has no content")

        agent_input = {
            "user name": user_real_name,
            "message": content,
            "surface_info": surface_info
        }

        if attachments:
            for attachment in attachments:
                logger.debug(f"Processing attachment: {attachment}")
                agent_input["file_name"] = attachment.filename
                if attachment.content_type and attachment.content_type.startswith("image/"):
                    agent_input["image_url"] = attachment.url

        return agent_input
    except Exception as e:
        logger.error(f"Error preparing agent input: {e}")
        raise ValueError(f"Failed to prepare agent input: {e}")

</document_content>
</document>
<document index="19">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/utils/prefix_utils.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
"""Prefix handling utilities for Discord bot.

This module handles command prefix management for the Discord bot.
"""
from __future__ import annotations

from typing import Any, List, Optional, Union, cast

import discord
import structlog

from discord import DMChannel, Guild, Message
from discord.ext import commands


logger = structlog.get_logger(__name__)

from democracy_exe.aio_settings import aiosettings


def get_guild_prefix(bot: Any, guild_id: int) -> str:
    """Get the prefix for a specific guild.

    Args:
        bot: The bot instance
        guild_id: The guild ID

    Returns:
        The guild's prefix or default prefix
    """
    try:
        if not hasattr(bot, 'prefixes'):
            raise AttributeError("Bot has no prefixes attribute")
        prefix = bot.prefixes.get(guild_id, [aiosettings.prefix])[0]
        logger.info("Getting guild prefix", guild_id=guild_id, prefix=prefix)
        return prefix
    except Exception as e:
        logger.error("Error getting guild prefix", error=str(e))
        return aiosettings.prefix


async def get_prefix(bot: Any, message: Message) -> Any:
    """Retrieve the command prefix for the bot based on the message context.

    Args:
        bot: The instance of the bot
        message: The message object from Discord

    Returns:
        The command prefix to be used for the bot
    """
    logger.info("Getting prefix for message", message_id=message.id)
    try:
        # Cast to proper types to satisfy type checker
        channel = cast(Union[DMChannel, Any], message.channel)
        prefix = (
            [aiosettings.prefix]
            if isinstance(channel, DMChannel)
            else [get_guild_prefix(bot, cast(Guild, message.guild).id)]
        )
        logger.debug("Using prefix", prefix=prefix)
        # await logger.complete()
        base = [f"<@!{bot.user.id}> ", f"<@{bot.user.id}> "]
        prefixes = [aiosettings.prefix] if isinstance(channel, DMChannel) else bot.prefixes.get(cast(Guild, message.guild).id, [aiosettings.prefix])
        base.extend(prefixes)
        return base
    except Exception as e:
        logger.error("Error getting prefix", error=str(e))
        # Fallback to default prefix
        return commands.when_mentioned_or(aiosettings.prefix)(bot, message)


def _prefix_callable(bot: Any, msg: Message) -> list[str]:
    """Generate a list of command prefixes for the bot.

    This function generates a list of command prefixes for the bot based on the
    message context. If the message is from a direct message (DM) channel, it
    includes the bot's user ID mentions and default prefixes. If the message is
    from a guild (server) channel, it includes the bot's user ID mentions and
    the guild-specific prefixes.

    Args:
        bot: The instance of the bot
        msg: The message object from Discord

    Returns:
        List of command prefixes to be used for the bot
    """
    try:
        user_id = bot.user.id
        base = [f"<@!{user_id}> ", f"<@{user_id}> "]

        # Cast to proper types to satisfy type checker
        guild = cast(Optional[Guild], msg.guild)
        if guild is None:
            base.extend(("!", "?"))
            logger.info("Getting prefixes for DM channel")
        else:
            base.extend(bot.prefixes.get(guild.id, ["?", "!"]))
            logger.info("Getting prefixes for guild channel", guild_id=guild.id)

        return base
    except Exception as e:
        logger.error("Error in prefix_callable", error=str(e))
        # Fallback to default prefixes
        return ["!", "?"]


async def update_guild_prefix(
    bot: Any, guild_id: int, new_prefix: str
) -> None:
    """Update the command prefix for a specific guild.

    Args:
        bot: The instance of the bot
        guild_id: The ID of the guild to update
        new_prefix: The new prefix to set

    Raises:
        ValueError: If the new prefix is invalid
    """
    try:
        if not new_prefix or len(new_prefix) > 10:
            raise ValueError("Invalid prefix length")

        if guild_id in bot.prefixes:
            bot.prefixes[guild_id] = [new_prefix]
            logger.info("Updated prefix for guild", guild_id=guild_id, new_prefix=new_prefix)
        else:
            logger.warning("Guild not found in prefix cache", guild_id=guild_id)

    except Exception as e:
        logger.error("Error updating guild prefix", error=str(e))
        raise


def get_prefix_display(
    bot: Any, guild: Guild | None = None
) -> str:
    """Get a display string for the current prefix(es).

    Args:
        bot: The instance of the bot
        guild: The guild to get prefixes for (None for DM prefixes)

    Returns:
        A formatted string showing the current prefix(es)
    """
    try:
        if guild is None:
            prefixes = ["!", "?"]
        else:
            prefixes = bot.prefixes.get(guild.id, ["?", "!"])

        if len(prefixes) == 1:
            return f"Current prefix is: {prefixes[0]}"
        else:
            return f"Current prefixes are: {', '.join(prefixes)}"
    except Exception as e:
        logger.error("Error getting prefix display", error=str(e))
        return "Default prefixes are: ! ?"

</document_content>
</document>
<document index="20">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/ai/__init__.py</source>
<document_content>

</document_content>
</document>
<document index="21">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/ai/langchain_utils.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
"""LangChain integration utilities for Discord bot.

This module handles AI/LangChain integration for the Discord bot.
"""
from __future__ import annotations

from typing import Any, Dict, Optional

import discord
import structlog

from discord import DMChannel, Message, Thread
from langchain_core.messages import HumanMessage


logger = structlog.get_logger(__name__)

from democracy_exe.agentic.workflows.react.graph import graph as memgraph
from democracy_exe.ai.graphs import AgentState
from democracy_exe.chatbot.utils.message_utils import format_inbound_message, get_session_id, prepare_agent_input


async def get_thread(message: Message) -> Thread | DMChannel:
    """Get or create a thread for the message.

    Args:
        message: The Discord message

    Returns:
        Thread or DMChannel for responses
    """
    if isinstance(message.channel, DMChannel):
        return message.channel

    thread = await message.channel.create_thread(
        name="Response",
        message=message
    )
    return thread


async def stream_bot_response(
    graph: Any = memgraph,
    user_input: dict[str, list[HumanMessage]] | None = None,
    thread: dict[str, Any] | None = None,
    interruptable: bool = False
) -> str:
    """Stream responses from the LangGraph Chatbot.

    Args:
        graph: The compiled state graph to use for generating responses
        user_input: Dictionary containing user messages
        thread: Dictionary containing thread state information
        interruptable: Flag to indicate if streaming can be interrupted

    Returns:
        The concatenated response from all chunks

    Raises:
        ValueError: If response generation fails
    """
    try:
        if user_input is None:
            raise ValueError("user_input must be provided")

        response = graph.invoke(user_input)
        if isinstance(response, dict) and "messages" in response:
            messages = response.get("messages", [])
            return "".join(msg.content for msg in messages if hasattr(msg, 'content'))
        raise ValueError("No response generated")
    except Exception as e:
        logger.error(f"Error streaming bot response: {e}")
        raise

</document_content>
</document>
<document index="22">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/handlers/__init__.py</source>
<document_content>
"""Message and attachment handlers initialization."""
from __future__ import annotations

from democracy_exe.chatbot.handlers.attachment_handler import AttachmentHandler
from democracy_exe.chatbot.handlers.message_handler import MessageHandler


__all__ = ["AttachmentHandler", "MessageHandler"]

</document_content>
</document>
<document index="23">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/handlers/attachment_handler.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
"""Attachment handling functionality.

This module contains functionality for handling Discord attachments,
including saving, downloading, and processing attachments.
"""
from __future__ import annotations

import asyncio
import base64
import io
import os
import pathlib
import uuid

from io import BytesIO
from typing import Any, Dict, List, Optional, Tuple, Union, cast

import aiohttp
import discord
import rich
import structlog

from discord import Attachment, File, HTTPException, Message


logger = structlog.get_logger(__name__)
from PIL import Image


class AttachmentHandler:
    """Handles processing and saving of Discord attachments."""

    @staticmethod
    def attachment_to_dict(attm: Attachment) -> dict[str, Any]:
        """Convert a discord.Attachment object to a dictionary.

        Args:
            attm: The attachment object to be converted

        Returns:
            A dictionary containing information about the attachment

        Raises:
            ValueError: If attachment is missing required attributes
        """
        try:
            if not all(hasattr(attm, attr) for attr in ["filename", "id", "url"]):
                raise ValueError("Attachment missing required attributes")

            result = {
                "filename": attm.filename,
                "id": attm.id,
                "proxy_url": attm.proxy_url,
                "size": attm.size,
                "url": attm.url,
                "spoiler": attm.is_spoiler(),
            }

            # Optional attributes
            if attm.height is not None:
                result["height"] = attm.height
            if attm.width is not None:
                result["width"] = attm.width
            if attm.content_type is not None:
                result["content_type"] = attm.content_type

            result["attachment_obj"] = attm
            return result

        except Exception as e:
            logger.error(f"Error converting attachment to dict: {e}")
            raise ValueError(f"Failed to convert attachment to dict: {e}") from e

    @staticmethod
    def file_to_local_data_dict(fname: str, dir_root: str) -> dict[str, Any]:
        """Convert a file to a dictionary with metadata.

        Args:
            fname: The name of the file to be converted
            dir_root: The root directory where the file is located

        Returns:
            A dictionary containing metadata about the file

        Raises:
            FileNotFoundError: If the file does not exist
            OSError: If there's an error accessing file stats
        """
        try:
            file_api = pathlib.Path(fname)
            if not file_api.exists():
                raise FileNotFoundError(f"File not found: {fname}")

            return {
                "filename": f"{dir_root}/{file_api.stem}{file_api.suffix}",
                "size": file_api.stat().st_size,
                "ext": f"{file_api.suffix}",
                "api": file_api,
            }
        except Exception as e:
            logger.error(f"Error creating file metadata dict: {e}")
            raise

    @staticmethod
    async def download_image(url: str) -> BytesIO | None:
        """Download an image from a given URL asynchronously.

        Args:
            url: The URL of the image to download

        Returns:
            A BytesIO object containing the downloaded image data, or None if download fails

        Raises:
            aiohttp.ClientError: If there's an error downloading the image
        """
        try:
            async with aiohttp.ClientSession() as session:
                response = await session.get(url)
                if response.status == 200:
                    data = await response.read()
                    return io.BytesIO(data)
                else:
                    logger.error(f"Failed to download image. Status: {response.status}")
                    return None
        except aiohttp.ClientError as e:
            logger.error(f"Error downloading image: {e}")
            raise

    @staticmethod
    async def file_to_data_uri(file: File) -> str:
        """Convert a discord.File object to a data URI.

        Args:
            file: The discord.File object to be converted

        Returns:
            A data URI representing the file content

        Raises:
            ValueError: If file is not readable
        """
        try:
            if not file.fp or not file.fp.readable():
                raise ValueError("File is not readable")

            with BytesIO(file.fp.read()) as f:
                file_bytes = f.read()
            base64_encoded = base64.b64encode(file_bytes).decode("ascii")
            return f"data:image;base64,{base64_encoded}"
        except Exception as e:
            logger.error(f"Error converting file to data URI: {e}")
            raise

    @staticmethod
    async def data_uri_to_file(data_uri: str, filename: str) -> File:
        """Convert a data URI to a discord.File object.

        Args:
            data_uri: The data URI to be converted
            filename: The name of the file to be created

        Returns:
            A discord.File object containing the decoded data

        Raises:
            ValueError: If data URI is invalid
        """
        try:
            if "," not in data_uri:
                raise ValueError("Invalid data URI format")

            metadata, base64_data = data_uri.split(",")
            file_bytes = base64.b64decode(base64_data)
            return discord.File(BytesIO(file_bytes), filename=filename, spoiler=False)
        except Exception as e:
            logger.error(f"Error converting data URI to file: {e}")
            raise

    @staticmethod
    def path_for(attm: Attachment, basedir: str = "./") -> pathlib.Path:
        """Generate a pathlib.Path object for an attachment.

        Args:
            attm: The attachment for which the path is generated
            basedir: The base directory path. Default is current directory

        Returns:
            A pathlib.Path object representing the path for the attachment file

        Raises:
            ValueError: If attachment filename is invalid
        """
        try:
            if not attm.filename:
                raise ValueError("Attachment has no filename")

            p = pathlib.Path(basedir).resolve() / str(attm.filename)
            logger.debug(f"path_for: p -> {p}")
            return p
        except Exception as e:
            logger.error(f"Error generating path for attachment: {e}")
            raise

    async def save_attachment(self, attm: Attachment, basedir: str = "./") -> None:
        """Save a Discord attachment to a specified directory.

        Args:
            attm: The attachment to be saved
            basedir: The base directory path where the file will be saved

        Raises:
            HTTPException: If there's an error saving the attachment
            OSError: If there's an error creating directories
        """
        try:
            path = self.path_for(attm, basedir=basedir)
            logger.debug(f"save_attachment: path -> {path}")
            path.parent.mkdir(parents=True, exist_ok=True)

            try:
                await attm.save(path, use_cached=True)
                await asyncio.sleep(5)
            except HTTPException:
                await attm.save(path)

            # await logger.complete()
        except Exception as e:
            logger.error(f"Error saving attachment: {e}")
            raise

    async def handle_save_attachment_locally(self, attm_data_dict: dict[str, Any], dir_root: str) -> str:
        """Save a Discord attachment locally.

        Args:
            attm_data_dict: A dictionary containing information about the attachment
            dir_root: The root directory where the file will be saved

        Returns:
            The path of the saved attachment file

        Raises:
            ValueError: If attachment data is invalid
            HTTPException: If there's an error saving the attachment
        """
        try:
            if not all(key in attm_data_dict for key in ["id", "filename", "attachment_obj"]):
                raise ValueError("Invalid attachment data dictionary")

            fname = f"{dir_root}/orig_{attm_data_dict['id']}_{attm_data_dict['filename']}"
            rich.print(f"Saving to ... {fname}")

            await attm_data_dict["attachment_obj"].save(fname, use_cached=True)
            await asyncio.sleep(1)
            return fname

        except Exception as e:
            logger.error(f"Error saving attachment locally: {e}")
            raise

    def get_attachments(
        self, message: Message
    ) -> tuple[list[dict[str, Any]], list[str], list[dict[str, Any]], list[str]]:
        """Retrieve attachment data from a Discord message.

        Args:
            message: The Discord message containing attachments

        Returns:
            A tuple containing:
                - A list of dictionaries with attachment data
                - A list of local attachment file paths
                - A list of dictionaries with local attachment data
                - A list of media filepaths

        Raises:
            ValueError: If message has no attachments
        """
        try:
            if not message.attachments:
                logger.warning("Message has no attachments")
                return [], [], [], []

            attachment_data_list_dicts = []
            local_attachment_file_list = []
            local_attachment_data_list_dicts = []
            media_filepaths = []

            for attm in message.attachments:
                try:
                    data = self.attachment_to_dict(attm)
                    attachment_data_list_dicts.append(data)
                except Exception as e:
                    logger.error(f"Error processing attachment: {e}")
                    continue

            return (
                attachment_data_list_dicts,
                local_attachment_file_list,
                local_attachment_data_list_dicts,
                media_filepaths,
            )

        except Exception as e:
            logger.error(f"Error getting attachments: {e}")
            return [], [], [], []

</document_content>
</document>
<document index="24">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/handlers/message_handler.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false

"""Message processing and LangGraph integration.

This module contains functionality for processing Discord messages and integrating
with LangGraph for AI responses.
"""
from __future__ import annotations

import re

from typing import Any, Optional, Union, cast

import discord
import structlog

from discord import DMChannel, Message, TextChannel, Thread
from discord.abc import Messageable
from discord.member import Member
from discord.user import User
from langchain_core.messages import AIMessage, HumanMessage
from langgraph.graph.state import CompiledStateGraph  # type: ignore


logger = structlog.get_logger(__name__)
from PIL import Image

from democracy_exe.chatbot.handlers.attachment_handler import AttachmentHandler
from democracy_exe.chatbot.utils.message_utils import format_inbound_message, get_session_id, prepare_agent_input


class MessageHandler:
    """Handler for processing Discord messages and integrating with LangGraph."""

    def __init__(self, bot: Any) -> None:
        """Initialize the message handler.

        Args:
            bot: The Discord bot instance
        """
        self.bot = bot
        self.attachment_handler = AttachmentHandler()

    async def check_for_attachments(self, message: discord.Message) -> str:
        """Check and process message attachments.

        Args:
            message: The Discord message

        Returns:
            The processed message content
        """
        try:
            content = cast(str, message.content)
            attachments = cast(list[discord.Attachment], message.attachments)

            # Handle Tenor GIFs
            if "https://tenor.com/view/" in content:
                return await self._handle_tenor_gif(message, content)

            # Handle image URLs
            image_pattern = r"https?://[^\s<>\"]+?\.(?:png|jpg|jpeg|gif|webp)"
            if re.search(image_pattern, content):
                url = re.findall(image_pattern, content)[0]
                return await self._handle_url_image(url)

            # Handle Discord attachments
            if attachments:
                return await self._handle_attachment_image(message)

            return content
        except Exception as e:
            logger.error(f"Error checking attachments: {e}")
            return cast(str, message.content) or ""

    async def stream_bot_response(
        self,
        graph: CompiledStateGraph,
        input_data: dict[str, Any]
    ) -> str:
        """Stream responses from the bot's LangGraph.

        Args:
            graph: The compiled state graph
            input_data: Input data for the graph

        Returns:
            The bot's response

        Raises:
            ValueError: If response generation fails
        """
        """Stream responses from the bot's LangGraph.

        Args:
            graph: The compiled state graph
            input_data: Input data for the graph

        Returns:
            The bot's response

        Raises:
            ValueError: If response generation fails
        """
        try:
            response = graph.invoke(input_data)
            if isinstance(response, dict) and "messages" in response:
                messages = response.get("messages", [])
                return "".join(msg.content for msg in messages if hasattr(msg, 'content'))
            raise ValueError("No response generated")
        except Exception as e:
            logger.error(f"Error streaming bot response: {e}")
            raise

    async def _get_thread(self, message: discord.Message) -> Thread | DMChannel:
        """Get or create a thread for the message.

        Args:
            message: The Discord message

        Returns:
            The thread or DM channel for the message

        Raises:
            ValueError: If thread creation fails
        """
        try:
            channel = cast(Union[TextChannel, DMChannel], message.channel)
            if isinstance(channel, DMChannel):
                return channel

            if not isinstance(channel, TextChannel):
                raise ValueError(f"Unsupported channel type: {type(channel)}")

            thread = await channel.create_thread(
                name="Response",
                message=message,
            )
            return thread
        except Exception as e:
            logger.error(f"Error getting thread: {e}")
            raise

    def _format_inbound_message(self, message: Message) -> HumanMessage:
        """Format a Discord message into a HumanMessage.

        Args:
            message: The Discord message to format

        Returns:
            Formatted HumanMessage
        """
        return format_inbound_message(message)

    def get_session_id(self, message: Message | Thread) -> str:
        """Generate a session ID for the given message.

        Args:
            message: The message or thread

        Returns:
            The generated session ID
        """
        return get_session_id(message)

    def prepare_agent_input(
        self,
        message: Message | Thread,
        user_real_name: str,
        surface_info: dict[str, Any]
    ) -> dict[str, Any]:
        """Prepare the agent input from the incoming Discord message.

        Args:
            message: The Discord message containing the user input
            user_real_name: The real name of the user who sent the message
            surface_info: The surface information related to the message

        Returns:
            The input dictionary to be sent to the agent

        Raises:
            ValueError: If message processing fails
        """
        return prepare_agent_input(message, user_real_name, surface_info)

    async def _handle_tenor_gif(self, message: discord.Message, content: str) -> str:
        """Handle Tenor GIF URLs in messages.

        Args:
            message: The Discord message
            content: The message content

        Returns:
            Updated message content with GIF description
        """
        try:
            start_index = content.index("https://tenor.com/view/")
            end_index = content.find(" ", start_index)
            tenor_url = content[start_index:] if end_index == -1 else content[start_index:end_index]

            parts = tenor_url.split("/")
            words = parts[-1].split("-")[:-1]
            sentence = " ".join(words)

            author = cast(Union[Member, User], message.author)
            return f"{content} [{author.display_name} posts an animated {sentence}]".replace(tenor_url, "")
        except Exception as e:
            logger.error(f"Error processing Tenor GIF: {e}")
            return content

    async def _handle_url_image(self, url: str) -> str:
        """Handle image URLs in messages.

        Args:
            url: The image URL

        Returns:
            The original URL
        """
        try:
            logger.info(f"Processing image URL: {url}")
            response = await self.attachment_handler.download_image(url)
            if response:
                image = Image.open(response).convert("RGB")
                # Process image if needed
            return url
        except Exception as e:
            logger.error(f"Error processing image URL: {e}")
            return url

    async def _handle_attachment_image(self, message: discord.Message) -> str:
        """Handle image attachments in messages.

        Args:
            message: The Discord message with attachments

        Returns:
            The message content
        """
        try:
            attachments = cast(list[discord.Attachment], message.attachments)
            if not attachments:
                return cast(str, message.content) or ""

            attachment = attachments[0]
            if not attachment.content_type or not attachment.content_type.startswith("image/"):
                return cast(str, message.content) or ""

            response = await self.attachment_handler.download_image(attachment.url)
            if response:
                image = Image.open(response).convert("RGB")
                # Process image if needed

            return cast(str, message.content) or ""
        except Exception as e:
            logger.error(f"Error processing attachment: {e}")
            return cast(str, message.content) or ""

</document_content>
</document>
<document index="25">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/cogs/__init__.py</source>
<document_content>

</document_content>
</document>
<document index="26">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/cogs/autocrop.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
# pyright: reportUninitializedInstanceVariable=false
# pyright: reportUndefinedVariable=false
# pyright: reportAttributeAccessIssue=false
# pyright: reportInvalidTypeForm=false

"""Autocrop cog for Discord bot.

This cog provides image auto-cropping functionality including smart detection
of important regions and automatic resizing/cropping to common aspect ratios.

Attributes:
    bot: The Discord bot instance
"""
from __future__ import annotations

import asyncio
import concurrent.futures
import functools
import pathlib
import sys
import tempfile
import traceback

from typing import Any, Dict, Final, List, Optional, Tuple, Union

import bpdb
import cv2
import discord
import numpy as np
import structlog

from discord.ext import commands
from discord.ext.commands import Context


logger = structlog.get_logger(__name__)
from PIL import Image
from rich.pretty import pprint

from democracy_exe.aio_settings import aiosettings
from democracy_exe.factories.guild_factory import Guild
from democracy_exe.utils import file_functions
from tests.internal.discord_test_utils import SlowAttachment


# Command constants
HELP_MESSAGE: Final[str] = f"""
Available commands:
- {aiosettings.prefix}crop square <attachment>: Crop image to 1:1 aspect ratio
- {aiosettings.prefix}crop portrait <attachment>: Crop image to 4:5 aspect ratio
- {aiosettings.prefix}crop landscape <attachment>: Crop image to 16:9 aspect ratio
- {aiosettings.prefix}crop story <attachment>: Crop image to 9:16 aspect ratio
"""

# Aspect ratio constants
ASPECT_RATIOS = {
    "square": (1, 1),
    "portrait": (4, 5),
    "landscape": (16, 9),
    "story": (9, 16)
}

class AutocropError(Exception):
    """Base exception for autocrop-related errors.

    This exception is raised when image processing operations fail in the Autocrop cog.
    """
    pass

class Autocrop(commands.Cog):
    """Autocrop functionality for Discord bot.

    This cog provides image auto-cropping functionality including smart detection
    of important regions and automatic resizing/cropping to common aspect ratios.
    It supports concurrent processing and handles timeouts gracefully.

    Attributes:
        bot (commands.Bot): The Discord bot instance this cog is attached to
    """

    def __init__(self, bot: commands.Bot) -> None:
        """Initialize the Autocrop cog.

        Args:
            bot: The Discord bot instance to attach this cog to
        """
        logger.debug("Initializing Autocrop cog")
        self.bot = bot
        logger.debug("Autocrop cog initialized")

    @commands.Cog.listener()
    async def on_ready(self) -> None:
        """Handle the cog ready event.

        This method is called when the cog is fully loaded and ready to process commands.
        It logs the ready status and ensures all logging is complete.
        """
        logger.debug(f"{type(self).__name__} Cog ready.")
        print(f"{type(self).__name__} Cog ready.")
        # await logger.complete()

    @commands.Cog.listener()
    async def on_guild_join(self, guild):
        """Add new guilds to the database"""
        logger.debug(f"Adding new guild to database: {guild.id}")
        guild_obj = Guild(id=guild.id)
        logger.debug(f"Successfully added guild {guild.id} to database")

    async def _process_image(
        self,
        image_path: str,
        aspect_ratio: tuple[int, int],
        output_path: str
    ) -> bool:
        """Process image with smart cropping.

        This method handles the core image processing functionality, including loading,
        conversion, and cropping operations. All CPU-bound operations are executed in
        a thread pool to prevent blocking the event loop.

        Args:
            image_path: Path to input image file
            aspect_ratio: Target width/height ratio tuple (e.g., (1, 1) for square)
            output_path: Path where the processed image should be saved

        Returns:
            bool: True if processing was successful, False otherwise

        Raises:
            AutocropError: If any step of the image processing fails
        """
        logger.debug(f"Starting image processing - Input: {image_path}, Target ratio: {aspect_ratio}")
        try:
            # Run CPU-bound operations in thread pool
            loop = asyncio.get_running_loop()
            pool = concurrent.futures.ThreadPoolExecutor()

            def process_in_thread():
                # Load image
                logger.debug(f"Loading image from {image_path}")
                img = Image.open(image_path)
                logger.debug(f"Original image dimensions: {img.size}, mode: {img.mode}")

                # Convert to RGB if needed
                if img.mode != 'RGB':
                    logger.debug(f"Converting image from {img.mode} to RGB")
                    img = img.convert('RGB')

                # Calculate target dimensions
                target_ratio = aspect_ratio[0] / aspect_ratio[1]
                current_ratio = img.width / img.height
                logger.debug(f"Target ratio: {target_ratio:.2f}, Current ratio: {current_ratio:.2f}")

                if current_ratio > target_ratio:
                    # Image is too wide
                    new_width = int(img.height * target_ratio)
                    offset = (img.width - new_width) // 2
                    crop_box = (offset, 0, offset + new_width, img.height)
                    logger.debug(f"Image too wide - New width: {new_width}, Offset: {offset}")
                else:
                    # Image is too tall
                    new_height = int(img.width / target_ratio)
                    offset = (img.height - new_height) // 2
                    crop_box = (0, offset, img.width, offset + new_height)
                    logger.debug(f"Image too tall - New height: {new_height}, Offset: {offset}")

                logger.debug(f"Applying crop with box: {crop_box}")
                # Crop and save
                cropped = img.crop(crop_box)
                logger.debug(f"Saving processed image to {output_path} with quality 95")
                cropped.save(output_path, quality=95)
                return True

            logger.debug("Dispatching image processing to thread pool")
            try:
                result = await loop.run_in_executor(pool, process_in_thread)
                if result:
                    logger.debug("Image processed successfully")
                return result
            finally:
                pool.shutdown(wait=False)

        except Exception as e:
            logger.exception(f"Failed to process image: {e}")
            raise AutocropError(f"Image processing failed: {e}")

    async def _handle_crop(
        self,
        ctx: Context,
        ratio_name: str,
        attachment: discord.Attachment | SlowAttachment
    ) -> tuple[bool, str | None]:
        """Handle image cropping workflow.

        This method orchestrates the complete image cropping process, including:
        - Validating the input attachment
        - Creating a progress message
        - Downloading the attachment with timeout
        - Processing the image with timeout
        - Sending the result back to Discord

        All operations are properly logged and error-handled.

        Args:
            ctx: Discord command context containing message and channel info
            ratio_name: Name of aspect ratio to use (e.g., "square", "portrait")
            attachment: Discord attachment containing the image to process

        Returns:
            A tuple containing:
                bool: Success status of the operation
                str | None: Error message if operation failed, None if successful

        Note:
            This method uses temporary directories with unique names per request to
            ensure thread safety during concurrent operations.
        """
        logger.info(f"Starting crop operation - User: {ctx.author}, Guild: {ctx.guild}, Ratio: {ratio_name}")
        logger.debug(f"Attachment details - Name: {attachment.filename}, Size: {attachment.size}, Type: {attachment.content_type}")

        if not attachment.content_type or not attachment.content_type.startswith('image/'):
            logger.warning(f"Invalid attachment type: {attachment.content_type}")
            await ctx.send("Please provide a valid image file")
            return False, "Please provide a valid image file"

        # Create progress message
        try:
            progress = await ctx.send(f"Processing image to {ratio_name} format...")
            logger.debug("Progress message created successfully")
        except discord.HTTPException as e:
            logger.error(f"Failed to send progress message: {e}")
            return False, "Failed to send progress message"
        except Exception as e:
            logger.error(f"Failed to send progress message: {e}")
            return False, "Failed to send progress message"

        try:
            # Use unique temporary directory for each request
            tmp_prefix = f"autocrop_{ctx.message.id}_{ctx.author.id}_"
            logger.debug(f"Creating temporary directory with prefix: {tmp_prefix}")
            with tempfile.TemporaryDirectory(prefix=tmp_prefix) as tmpdirname:
                # Download attachment with timeout
                input_path = f"{tmpdirname}/input{pathlib.Path(attachment.filename).suffix}"
                try:
                    logger.debug(f"Downloading attachment with {aiosettings.autocrop_download_timeout}s timeout")
                    async with asyncio.timeout(aiosettings.autocrop_download_timeout):
                        await attachment.save(input_path)
                    logger.debug("Attachment downloaded successfully")
                except TimeoutError:
                    logger.error("Image download timed out")
                    await progress.edit(content="Image download timed out")
                    return False, "Image download timed out"
                except discord.HTTPException as e:
                    logger.error(f"Failed to download attachment: {e}")
                    await progress.edit(content=f"Failed to download image: {e}")
                    return False, f"Failed to download image: {e}"

                # Process image
                output_path = f"{tmpdirname}/output{pathlib.Path(attachment.filename).suffix}"
                try:
                    logger.debug(f"Processing image with {aiosettings.autocrop_processing_timeout}s timeout")
                    async with asyncio.timeout(aiosettings.autocrop_processing_timeout):
                        await self._process_image(
                            input_path,
                            ASPECT_RATIOS[ratio_name],
                            output_path
                        )
                    logger.debug("Image processed successfully")
                except TimeoutError:
                    logger.error("Image processing timed out")
                    await progress.edit(content="Image processing timed out")
                    return False, "Image processing timed out"

                # Send processed image
                try:
                    logger.debug(f"Sending processed image: {output_path}")
                    await ctx.send(file=discord.File(output_path))
                    await progress.edit(content="Processing complete!")
                    logger.info(f"Crop operation completed successfully for {ctx.author} in {ctx.guild}")
                    return True, None
                except discord.HTTPException as e:
                    error_msg = f"Failed to send processed image: {e}"
                    logger.error(error_msg)
                    await progress.edit(content=error_msg)
                    return False, error_msg

        except Exception as e:
            error_msg = f"Failed to process image: {e}"
            logger.exception(error_msg)
            try:
                await progress.edit(content=error_msg)
            except discord.HTTPException:
                logger.error(f"Failed to edit progress message: {e}")
            return False, error_msg

    @commands.group(name="crop")
    async def crop(self, ctx: commands.Context) -> None:
        """Autocrop command group.

        This is the base command for all cropping operations. If no subcommand is
        specified, it displays the help message showing available options.

        Args:
            ctx: Discord command context
        """
        logger.debug(f"Crop command invoked by {ctx.author} in {ctx.guild}")
        if ctx.invoked_subcommand is None:
            logger.debug("No subcommand specified, sending help message")
            await ctx.send(HELP_MESSAGE)

    @crop.command(name="square")
    async def square(self, ctx: commands.Context) -> None:
        """Crop image to 1:1 aspect ratio.

        This command processes an attached image to create a square crop,
        maintaining the central portion of the image.

        Args:
            ctx: Discord command context containing the message with image attachment

        Note:
            The image must be attached to the command message.
            The resulting image will have equal width and height.
        """
        logger.debug(f"Square crop command invoked by {ctx.author}")
        if not ctx.message.attachments:
            logger.warning(f"No attachment provided by {ctx.author}")
            await ctx.send("Please attach an image to crop")
            return

        await self._handle_crop(ctx, "square", ctx.message.attachments[0])

    @crop.command(name="portrait")
    async def portrait(self, ctx: commands.Context) -> None:
        """Crop image to 4:5 aspect ratio.

        This command processes an attached image to create a portrait crop
        with a 4:5 aspect ratio, suitable for platforms like Instagram.

        Args:
            ctx: Discord command context containing the message with image attachment

        Note:
            The image must be attached to the command message.
            The resulting image will have a height 1.25 times its width.
        """
        logger.debug(f"Portrait crop command invoked by {ctx.author}")
        if not ctx.message.attachments:
            logger.warning(f"No attachment provided by {ctx.author}")
            await ctx.send("Please attach an image to crop")
            return

        await self._handle_crop(ctx, "portrait", ctx.message.attachments[0])

    @crop.command(name="landscape")
    async def landscape(self, ctx: commands.Context) -> None:
        """Crop image to 16:9 aspect ratio.

        This command processes an attached image to create a landscape crop
        with a 16:9 aspect ratio, suitable for HD video formats.

        Args:
            ctx: Discord command context containing the message with image attachment

        Note:
            The image must be attached to the command message.
            The resulting image will have a width 1.78 times its height.
        """
        logger.debug(f"Landscape crop command invoked by {ctx.author}")
        if not ctx.message.attachments:
            logger.warning(f"No attachment provided by {ctx.author}")
            await ctx.send("Please attach an image to crop")
            return

        await self._handle_crop(ctx, "landscape", ctx.message.attachments[0])

    @crop.command(name="story")
    async def story(self, ctx: commands.Context) -> None:
        """Crop image to 9:16 aspect ratio.

        This command processes an attached image to create a vertical crop
        with a 9:16 aspect ratio, suitable for stories/reels on social media.

        Args:
            ctx: Discord command context containing the message with image attachment

        Note:
            The image must be attached to the command message.
            The resulting image will have a height 1.78 times its width.
        """
        logger.debug(f"Story crop command invoked by {ctx.author}")
        if not ctx.message.attachments:
            logger.warning(f"No attachment provided by {ctx.author}")
            await ctx.send("Please attach an image to crop")
            return

        await self._handle_crop(ctx, "story", ctx.message.attachments[0])

async def setup(bot: commands.Bot) -> None:
    """Add Autocrop cog to bot.

    Args:
        bot: Discord bot instance
    """
    logger.debug("Setting up Autocrop cog")
    await bot.add_cog(Autocrop(bot))
    logger.debug("Autocrop cog setup complete")

</document_content>
</document>
<document index="27">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/cogs/imagecaption.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false

"""Image captioning cog for Discord bot.

This cog provides image captioning functionality using the BLIP model.
It can process images from URLs, attachments, and Tenor GIFs.
"""
from __future__ import annotations

import re

from io import BytesIO
from re import Pattern
from typing import TYPE_CHECKING, Optional, Union
from urllib.parse import urlparse

import discord
import requests
import structlog
import torch

from discord.ext import commands
from discord.ext.commands import BucketType, cooldown


logger = structlog.get_logger(__name__)
from PIL import Image
from transformers import BlipForConditionalGeneration, BlipProcessor  # type: ignore

from democracy_exe.factories import guild_factory


if TYPE_CHECKING:
    from discord import Message
    from PIL.Image import Image as PILImage

# URL validation patterns
URL_PATTERN: Pattern = re.compile(
    r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
)
IMAGE_EXTENSIONS: tuple[str, ...] = ('.jpg', '.jpeg', '.png', '.gif', '.webp')


class ImageCaptionCog(commands.Cog, name="image_caption"):
    """Cog for handling image captioning using BLIP model.

    This cog provides commands for generating captions for images shared in Discord,
    whether they are direct attachments, URLs, or Tenor GIFs.

    Attributes:
        bot: The Discord bot instance
        processor: BLIP image processor
        model: BLIP captioning model
    """

    def __init__(self, bot: commands.Bot) -> None:
        """Initialize the image captioning cog.

        Args:
            bot: The Discord bot instance
        """
        self.bot = bot
        logger.info("Initializing BLIP model and processor...")

        try:
            self.processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
            self.model = BlipForConditionalGeneration.from_pretrained(
                "Salesforce/blip-image-captioning-base",
                torch_dtype=torch.float32
            ).to("cpu")
            logger.info("Successfully initialized BLIP model and processor")
        except Exception as e:
            logger.error(f"Failed to initialize BLIP model: {e!s}")
            raise

    @commands.Cog.listener()
    async def on_ready(self) -> None:
        """Handle cog ready event."""
        logger.info(f"{type(self).__name__} Cog ready.")
        # await logger.complete()

    @commands.Cog.listener()
    async def on_guild_join(self, guild: discord.Guild) -> None:
        """Handle guild join event.

        Args:
            guild: The guild that was joined
        """
        logger.info(f"Joined new guild: {guild.id}")
        guild_obj = guild_factory.Guild(id=guild.id)
        # await logger.complete()

    def _validate_image_url(self, url: str) -> bool:
        """Validate if a URL points to an image.

        Args:
            url: URL to validate

        Returns:
            bool: True if URL is valid image URL, False otherwise
        """
        try:
            # Check URL format
            if not URL_PATTERN.match(url):
                return False

            # Parse URL and check extension
            parsed = urlparse(url)
            return parsed.path.lower().endswith(IMAGE_EXTENSIONS)
        except Exception as e:
            logger.error(f"Error validating URL {url}: {e!s}")
            return False

    async def _process_tenor_gif(self, message: discord.Message, url: str) -> str:
        """Process a Tenor GIF URL to extract descriptive text.

        Args:
            message: The Discord message
            url: The Tenor GIF URL

        Returns:
            Processed message content with GIF description
        """
        try:
            # Extract the relevant part of the URL
            start_index = url.index("https://tenor.com/view/")
            end_index = url.find(" ", start_index)
            tenor_url = url[start_index:] if end_index == -1 else url[start_index:end_index]

            # Extract descriptive words from URL
            words = tenor_url.split("/")[-1].split("-")[:-1]
            description = " ".join(words)

            return f"{message.content} [{message.author.display_name} posts an animated {description}]".replace(tenor_url, "") # type: ignore
        except Exception as e:
            logger.error(f"Error processing Tenor GIF: {e!s}")
            # await logger.complete()
            return message.content # type: ignore

    async def _download_image(self, url: str) -> PILImage | None:
        """Download and process an image from a URL.

        Args:
            url: The image URL

        Returns:
            PIL Image object or None if download fails

        Raises:
            requests.RequestException: If image download fails
        """
        try:
            # Validate URL before downloading
            if not self._validate_image_url(url):
                logger.warning(f"Invalid image URL: {url}")
                return None

            async with self.bot.session.get(url) as response: # type: ignore
                if response.status != 200:
                    raise requests.RequestException(f"Failed to download image: {response.status}")
                data = await response.read()
                return Image.open(BytesIO(data)).convert("RGB")
        except Exception as e:
            logger.error(f"Error downloading image from {url}: {e!s}")
            # await logger.complete()
            return None

    def caption_image(self, image: PILImage) -> str:
        """Generate a caption for an image using BLIP model.

        Args:
            image: PIL Image to caption

        Returns:
            Generated caption text

        Raises:
            Exception: If caption generation fails
        """
        try:
            inputs = self.processor(image.convert("RGB"), return_tensors="pt").to("cpu", torch.float32)
            out = self.model.generate(**inputs, max_new_tokens=50)
            return self.processor.decode(out[0], skip_special_tokens=True) # type: ignore
        except Exception as e:
            logger.error(f"Error generating caption: {e!s}")
            raise

    @commands.command(
        name="caption",
        aliases=["cap", "describe"],
        help="Generate a caption for an image from URL or attachment"
    )
    @commands.cooldown(1, 30, BucketType.user)  # One use per user every 30 seconds
    async def image_caption(self, ctx: commands.Context, url: str | None = None) -> None:
        """Generate a caption for an image.

        This command can process:
        - Image attachments
        - Image URLs
        - Tenor GIFs

        Args:
            ctx: Command context
            url: Optional image URL
        """

        try:
            # Handle message with no image
            if not url and not ctx.message.attachments: # type: ignore
                await ctx.send("Please provide an image URL or attachment!")
                return

            # Process URL if provided
            if url:
                if "tenor.com/view/" in url:
                    response = await self._process_tenor_gif(ctx.message, url) # type: ignore
                    await ctx.send(response)
                    return

                # Validate URL
                if not self._validate_image_url(url):
                    await ctx.send("Invalid image URL! Please provide a valid image URL.")
                    return

                image = await self._download_image(url)
                if not image:
                    await ctx.send("Failed to download image from URL!")
                    return

            # Process attachment if present
            elif ctx.message.attachments: # type: ignore
                attachment = ctx.message.attachments[0] # type: ignore
                if not attachment.content_type or not attachment.content_type.startswith("image/"):
                    await ctx.send("The attachment must be an image!")
                    return

                image = await self._download_image(attachment.url)
                if not image:
                    await ctx.send("Failed to process image attachment!")
                    return

            # Generate and send caption if image was loaded
            if image:
                try:
                    caption = self.caption_image(image)
                    await ctx.send(f"I see {caption}")
                finally:
                    try:
                        image.close()
                    except Exception as e:
                        logger.warning(f"Error closing image: {e!s}")
            else:
                await ctx.send("Failed to process image!")

        except commands.CommandOnCooldown as e:
            await ctx.send(f"This command is on cooldown. Try again in {e.retry_after:.1f} seconds.") # type: ignore
        except Exception as e:
            logger.exception("Error in image_caption command")
            # await logger.complete()
            await ctx.send(f"An error occurred while processing the image: {e!s}")


async def setup(bot: commands.Bot) -> None:
    """Add ImageCaptionCog to bot.

    Args:
        bot: The Discord bot instance
    """
    await bot.add_cog(ImageCaptionCog(bot))

</document_content>
</document>
<document index="28">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/chatbot/cogs/twitter.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false

"""Twitter cog for Discord bot.

This cog provides Twitter-related functionality including downloading tweets,
threads, cards and displaying tweet metadata.

Attributes:
    bot: The Discord bot instance
"""
from __future__ import annotations

import asyncio
import pathlib
import sys
import tempfile
import traceback

from typing import Any, Dict, Final, List, Optional, Tuple, Union, cast

import bpdb
import discord
import structlog

from discord.ext import commands
from discord.ext.commands import Context


logger = structlog.get_logger(__name__)
from rich.pretty import pprint

from democracy_exe.aio_settings import aiosettings
from democracy_exe.factories.guild_factory import Guild
from democracy_exe.utils.twitter_utils.download import download_tweet
from democracy_exe.utils.twitter_utils.embed import (
    TweetMetadata,
    create_card_embed,
    create_download_progress_embed,
    create_error_embed,
    create_info_embed,
    create_thread_embed,
    create_tweet_embed,
)
from democracy_exe.utils.twitter_utils.types import DownloadResult, TweetDownloadMode


# Command constants
HELP_MESSAGE: Final[str] = f"""
Available commands:
- {aiosettings.prefix}tweet download <url>: Download tweet media and metadata
- {aiosettings.prefix}tweet thread <url>: Download full tweet thread
- {aiosettings.prefix}tweet card <url>: Download tweet card preview
- {aiosettings.prefix}tweet info <url>: Show tweet metadata
"""


class TwitterError(Exception):
    """Base exception for Twitter-related errors."""


class Twitter(commands.Cog):
    """Twitter functionality for Discord bot.

    Handles downloading and displaying tweets, threads and cards.

    Attributes:
        bot: The Discord bot instance
    """

    def __init__(self, bot: commands.Bot):
        logger.debug("Initializing Twitter cog")
        self.bot = bot
        logger.debug("Twitter cog initialized")


    @commands.Cog.listener()
    async def on_ready(self):
        logger.debug(f"{type(self).__name__} Cog ready.")
        print(f"{type(self).__name__} Cog ready.")
        # await logger.complete()

    @commands.Cog.listener()
    async def on_guild_join(self, guild):
        """Add new guilds to the database"""
        logger.debug(f"Adding new guild to database: {guild.id}")
        guild_obj = Guild(id=guild.id)
        logger.debug(f"Successfully added guild {guild.id} to database")

    def _cleanup_temp_dir(self, file_path: str) -> None:
        """Delete temporary directory created for tweet downloads.

        Args:
            file_path: Path to a file in the temporary directory

        Note:
            Silently fails if directory doesn't exist or can't be deleted to avoid
            disrupting the main download flow.
        """
        try:
            # Get the parent directory of the file
            temp_dir = pathlib.Path(file_path).parent
            logger.debug(f"temp_dir: {temp_dir}")

            # Verify that we're deleting a gallery-dl directory
            if temp_dir.exists():
                logger.debug(f"Cleaning up temporary directory: {temp_dir}")
                import shutil
                shutil.rmtree(temp_dir)
                logger.debug("Temporary directory cleanup complete")
            else:
                logger.warning(f"Skipping cleanup - directory {temp_dir} doesn't exist")
        except Exception as e:
            logger.warning(f"Failed to cleanup temporary directory: {e}")

    async def _handle_download(
        self,
        ctx: Context,
        url: str,
        mode: TweetDownloadMode
    ) -> tuple[bool, str | None]:
        """Handle tweet download and send response.

        Args:
            ctx: Command context
            url: Tweet URL
            mode: Download mode (single/thread/card)

        Returns:
            Tuple containing:
                - Success status (bool)
                - Error message if any (Optional[str])

        Raises:
            TwitterError: If download fails
        """
        logger.info(f"{type(self).__name__} -> _handle_download -> ctx = {ctx}, url = {url}, mode = {mode}")
        logger.debug(f"Starting download handler - URL: {url}, Mode: {mode}")

        # Create progress message with embed
        progress_embed = create_download_progress_embed(url, mode)
        progress = await ctx.send(embed=progress_embed)
        logger.debug("Created progress embed")

        try:
            # Download tweet content
            logger.debug("Initiating tweet download")
            result = await download_tweet(url, mode=mode)
            logger.error(f"result: {result}")
            logger.debug(f"Download result: success={result['success']}")

            if not result["success"]:
                logger.debug(f"Download failed: {result['error']}")
                error_embed = create_error_embed(str(result.get("error", "Unknown error")))
                await progress.edit(embed=error_embed)
                return False, result["error"]


            # Create appropriate embed based on mode
            logger.debug("Creating response embed")
            if mode == "thread":
                metadata_list = [cast(TweetMetadata, result["metadata"])]
                embed = create_thread_embed(metadata_list)
                logger.debug(f"Created thread embed with {len(metadata_list)} tweets")
            elif mode == "card":
                metadata = cast(TweetMetadata, result["metadata"])
                embed = create_card_embed(metadata)
                logger.debug("Created card embed")
            else:
                metadata = cast(TweetMetadata, result["metadata"])
                embed = create_tweet_embed(metadata)
                logger.debug("Created single tweet embed")

            # Upload media files if any
            files = []
            logger.debug(f"Processing {len(result['local_files'])} media files")
            # import bpdb; bpdb.set_trace()
            # {
            # 'success': True,
            # 'metadata': {'id': '', 'url': '', 'author': '', 'content': '', 'media_urls': [], 'created_at': ''},
            # 'local_files': ['/private/var/folders/q_/d5r_s8wd02zdx6qmc5f_96mw0000gp/T/tmpsu3j8yhy/gallery-dl/twitter/UAPJames/UAPJames-1869141126051217764-(20241217_220226)-img1.mp4'],
            # 'error': None
            # }

            for file_path in result["local_files"]:
                try:
                    files.append(discord.File(file_path))
                    logger.debug(f"Added file to upload: {file_path}")
                except Exception as ex:
                    logger.warning(f"Failed to create discord.File for {file_path}: {ex}")
                    print(f"{ex}")
                    exc_type, exc_value, exc_traceback = sys.exc_info()
                    print(f"Error Class: {ex.__class__}")
                    output = f"[UNEXPECTED] {type(ex).__name__}: {ex}"
                    print(output)
                    print(f"exc_type: {exc_type}")
                    print(f"exc_value: {exc_value}")
                    traceback.print_tb(exc_traceback)
                    if aiosettings.dev_mode:
                        bpdb.pm()

            # Send completion message first
            await ctx.send("Download complete!")
            # Then update progress embed
            await progress.edit(embed=embed)
            # Finally send any files
            if files:
                logger.debug(f"Uploading {len(files)} media files")
                await ctx.send(files=files)
                # Clean up temp directory after files are sent
                if result["local_files"]:
                    logger.debug("Cleaning up temp directory after files are sent")
                    try:
                        self._cleanup_temp_dir(result["local_files"][0])
                    except Exception as e:
                        logger.warning(f"Failed to cleanup temp directory: {e}")
                        return False, "Failed to cleanup temp directory"

            logger.debug("Download handler completed successfully")
            return True, None

        except Exception as e:
            logger.exception("Error in download handler")
            error_embed = create_error_embed(str(e))
            await progress.edit(embed=error_embed)
            error_msg = f"Failed to download tweet: {e}"
            raise TwitterError(error_msg) from e


    @commands.group(name="tweet")
    async def tweet(self, ctx: commands.Context) -> None:
        """Twitter command group."""
        logger.debug(f"Tweet command invoked by {ctx.author} in {ctx.guild}")
        if ctx.invoked_subcommand is None: # type: ignore
            logger.debug("No subcommand specified, sending help message")
            await ctx.send(HELP_MESSAGE)

    @tweet.command(name="download", aliases=["dlt", "t", "twitter"])
    async def download(self, ctx: commands.Context, url: str, *args: Any, **kwargs: Any) -> None:
        """Download tweet media and metadata.

        Args:
            ctx: Command context
            url: Tweet URL to download
        """
        try:
            logger.info(f"{type(self).__name__} -> ctx = {ctx}, url = {url}")
            logger.debug(f"Download command invoked - URL: {url}")
            await self._handle_download(ctx, url, mode="single")
            logger.debug("Download command completed")
        except Exception as e:
            logger.exception("Error in download command")
            error_embed = create_error_embed(str(e))
            await ctx.send(embed=error_embed)
            raise TwitterError(f"Failed to download tweet: {e}") from e

    @download.error
    async def download_error_handler(self, ctx, error):
        if isinstance(error, commands.MissingPermissions):
            await ctx.send(
                embed=discord.Embed(description="Sorry, you need `MANAGE SERVER` permissions to change the download!")
            )

    @tweet.command(name="thread", aliases=["dt"])
    async def thread(self, ctx: commands.Context, url: str) -> None:
        """Download full tweet thread.

        Args:
            ctx: Command context
            url: Tweet thread URL to download
        """
        logger.debug(f"Thread command invoked - URL: {url}")
        await self._handle_download(ctx, url, mode="thread")
        logger.debug("Thread command completed")

    @thread.error
    async def thread_error_handler(self, ctx, error):
        if isinstance(error, commands.MissingPermissions):
            await ctx.send(
                embed=discord.Embed(description="Sorry, you need `MANAGE SERVER` permissions to change the thread!")
            )

    @tweet.command(name="card")
    async def card(self, ctx: commands.Context, url: str) -> None:
        """Download tweet card preview.

        Args:
            ctx: Command context
            url: Tweet URL to download card from
        """
        logger.debug(f"Card command invoked - URL: {url}")
        await self._handle_download(ctx, url, mode="card")
        logger.debug("Card command completed")

    @card.error
    async def card_error_handler(self, ctx, error):
        if isinstance(error, commands.MissingPermissions):
            await ctx.send(
                embed=discord.Embed(description="Sorry, you need `MANAGE SERVER` permissions to change the card!")
            )

    @tweet.command(name="info")
    async def info(self, ctx: commands.Context, url: str) -> None:
        """Show tweet metadata.

        Args:
            ctx: Command context
            url: Tweet URL to get info for

        Raises:
            TwitterError: If getting info fails
        """
        logger.debug(f"Info command invoked - URL: {url}")

        try:
            logger.debug("Fetching tweet metadata")
            result = await download_tweet(url, mode="single")
            logger.debug(f"Metadata fetch result: success={result['success']}")

            if not result["success"]:
                logger.debug(f"Metadata fetch failed: {result['error']}")
                error_embed = create_error_embed(result["error"])
                await ctx.send(embed=error_embed)
                return

            # Create detailed info embed
            metadata = cast(TweetMetadata, result["metadata"])
            embed = create_info_embed(metadata)
            logger.debug("Created info embed")
            await ctx.send(embed=embed)
            logger.debug("Info command completed successfully")

        except Exception as e:
            logger.exception("Error in info command")
            error_embed = create_error_embed(str(e))
            await ctx.send(embed=error_embed)
            raise TwitterError(f"Failed to get tweet info: {e}") from e

    @info.error
    async def info_error_handler(self, ctx, error):
        if isinstance(error, commands.MissingPermissions):
            await ctx.send(
                embed=discord.Embed(description="Sorry, you need `MANAGE SERVER` permissions to change the info!")
            )


async def setup(bot: commands.Bot) -> None:
    """Add Twitter cog to bot.

    Args:
        bot: Discord bot instance
    """
    logger.debug("Setting up Twitter cog")
    await bot.add_cog(Twitter(bot))
    logger.debug("Twitter cog setup complete")

</document_content>
</document>
<document index="29">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/bot_logger/__init__.py</source>
<document_content>
# #!/usr/bin/env python3

# """
# Logging utilities for the SandboxAgent project.

# This module provides functions and utilities for configuring and managing logging in the SandboxAgent project.
# It includes functions for setting up the global logger, handling exceptions, and filtering log messages.

# Functions:
#     global_log_config(log_level: LOG_LEVEL, json: bool = False) -> None:
#         Configure the global logger with the specified log level and format.

#     get_logger(name: str = "democracy_exe") -> Logger:
#         Get a logger instance with the specified name.

#     _log_exception(exc: BaseException, dev_mode: bool = False) -> None:
#         Log an exception with the appropriate level based on the dev_mode setting.

#     _log_warning(exc: BaseException, dev_mode: bool = False) -> None:
#         Log a warning with the appropriate level based on the dev_mode setting.

#     filter_out_serialization_errors(record: dict[str, Any]) -> bool:
#         Filter out log messages related to serialization errors.

#     filter_out_modules(record: dict[str, Any]) -> bool:
#         Filter out log messages from the standard logging module.

# Constants:
#     LOGURU_FILE_FORMAT: str
#         The log format string for file logging.

#     NEW_logger_FORMAT: str
#         The new log format string for console logging.

#     LOG_LEVEL: Literal
#         The available log levels.

#     MASKED: str
#         A constant string used to mask sensitive data in logs.

# Classes:
#     Pii(str):
#         A custom string class that masks sensitive data in logs based on the log_pii setting.
# """
# # pylint: disable=no-member
# # pylint: disable=consider-using-tuple
# # pylint: disable=eval-used,no-member
# # pyright: ignore[reportOperatorIssue]
# # pyright: ignore[reportOptionalIterable]
# # SOURCE: https://betterstack.com/community/guides/logging/loguru/

# # FIXME: https://github.com/sweepai/sweep/blob/7d93c612568b8febd4aaf3c75810794bc10c09ae/sweepai/utils/event_logger.py#L7
# # FIXME: https://github.com/sweepai/sweep/blob/7d93c612568b8febd4aaf3c75810794bc10c09ae/sweepai/utils/event_logger.py#L7
# # FIXME: https://github.com/sweepai/sweep/blob/7d93c612568b8febd4aaf3c75810794bc10c09ae/sweepai/utils/event_logger.py#L7
# # FIXME: https://github.com/sweepai/sweep/blob/7d93c612568b8febd4aaf3c75810794bc10c09ae/sweepai/utils/event_logger.py#L7
# # FIXME: https://github.com/sweepai/sweep/blob/7d93c612568b8febd4aaf3c75810794bc10c09ae/sweepai/utils/event_logger.py#L7
# # FIXME: https://github.com/sweepai/sweep/blob/7d93c612568b8febd4aaf3c75810794bc10c09ae/sweepai/utils/event_logger.py#L7
# # FIXME: https://github.com/sweepai/sweep/blob/7d93c612568b8febd4aaf3c75810794bc10c09ae/sweepai/utils/event_logger.py#L7
# # FIXME: https://github.com/sweepai/sweep/blob/7d93c612568b8febd4aaf3c75810794bc10c09ae/sweepai/utils/event_logger.py#L7
# # FIXME: https://github.com/sweepai/sweep/blob/7d93c612568b8febd4aaf3c75810794bc10c09ae/sweepai/utils/event_logger.py#L7

# from __future__ import annotations

# import contextvars
# import functools
# import gc
# import inspect
# import logging
# import multiprocessing
# import os
# import re
# import sys
# import time

# from datetime import UTC, datetime, timezone
# from logging import Logger, LogRecord
# from pathlib import Path
# from pprint import pformat
# from sys import stdout
# from time import process_time
# from types import FrameType
# from typing import TYPE_CHECKING, Any, Deque, Dict, Literal, Optional, Union, cast

# import loguru


from __future__ import annotations

import logging

import structlog


logger = structlog.get_logger(__name__)
# from loguru._defaults import LOGURU_FORMAT
# from loguru._logger import Core
# from tqdm import tqdm

# from democracy_exe.aio_settings import aiosettings
from democracy_exe.models.loggers import LoggerModel, LoggerPatch


# from democracy_exe.utils import get_or_create_event_loop


# if TYPE_CHECKING:
#     from better_exceptions.log import BetExcLogger
#     from loguru._logger import Logger as _Logger




# # --------------------------------------------------------------------------
# # NOTE: This might be the solution I need to get loguru to work with multiprocessing on linux and macos
# # https://github.com/Delgan/loguru/issues/912
# # from __future__ import annotations

# # import loguru
#  as _logger


# # class LoggerDelegator:
# #     def __init__(self, logger: loguru.Logger):
# #         self._logger: loguru.Logger = logger

# #     def update_logger(self, logger: loguru.Logger):
# #         self._logger = logger

# #     def __getattr__(self, attr):
# #         return getattr(self._logger, attr)

# # logger = LoggerDelegator(_logger)
# # --------------------------------------------------------------------------


# LOGLEVEL_MAPPING = {
#     50: "CRITICAL",
#     40: "ERROR",
#     30: "WARNING",
#     20: "INFO",
#     10: "DEBUG",
#     5: "VVVV",
#     0: "NOTSET",
# }

# LOGURU_CONSOLE_FORMAT = (
#     "<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
#     "<level>{level}</level> | "
#     "<cyan>{module}</cyan>:<cyan>{line}</cyan> | "
#     "<level>{extra[room_id]}</level> - "
#     "<level>{message}</level>"
# )

# LOGURU_FILE_FORMAT = (
#     "<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
#     "<level>{level}</level> | "
#     "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | "
#     "<level>{extra[room_id]}</level> - "
#     "<level>{message}</level>"
# )

# # NOTE: this is the default format for loguru
# _LOGURU_FORMAT = (
#     "LOGURU_FORMAT",
#     str,
#     "<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
#     "<level>{level: <8}</level> | "
#     "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
# )

# # NOTE: this is the new format for loguru
# NEW_logger_FORMAT = (
#     "<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
#     "<level>{level: <8}</level> | "
#     "<cyan>{name}</cyan>:<cyan>{function}</cyan> - "
#     "<magenta>{file}:{line}</magenta> | "
#     "<level>{message}</level> | {extra}"
# )


# LOG_LEVEL = Literal[
#     "TRACE",
#     "VVVV",
#     "DEBUG",
#     "INFO",
#     "SUCCESS",
#     "WARNING",
#     "ERROR",
#     "CRITICAL",
# ]

# # --------------------------------------------------------------------------
# # SOURCE: https://github.com/ethyca/fides/blob/e9174b771155abc0ff3faf1fb7b8b76836788617/src/fides/api/util/logger.py#L116
# MASKED = "MASKED"


# class Pii(str):
#     """Mask pii data"""

#     def __format__(self, __format_spec: str) -> str:
#         """
#         Mask personally identifiable information (PII) in log messages.

#         This class is a subclass of str and overrides the __format__ method to mask
#         PII data in log messages. If the aiosettings.log_pii setting is True, the
#         original string value is returned. Otherwise, the string "MASKED" is returned.

#         Args:
#             __format_spec (str): The format specification string.

#         Returns:
#             str: The masked or unmasked string value.
#         """
#         if aiosettings.log_pii:
#             return super().__format__(__format_spec)
#         return MASKED


# # def _log_exception(exc: BaseException, dev_mode: bool = False) -> None:
# #     """
# #     Log an exception with the appropriate level based on the dev_mode setting.

# #     If dev_mode is True, the entire traceback will be logged using logger.opt(exception=True).error().
# #     Otherwise, only the exception message will be logged using logger.error().

# #     Args:
# #         exc (BaseException): The exception to be logged.
# #         dev_mode (bool, optional): Whether to log the entire traceback or just the exception message.
# #             Defaults to False.

# #     Returns:
# #         None
# #     """
# #     if dev_mode:
# #         logger.opt(exception=True).error(exc)
# #     else:
# #         logger.error(exc)


# # def _log_warning(exc: BaseException, dev_mode: bool = False) -> None:
# #     """
# #     Log a warning with the appropriate level based on the dev_mode setting.

# #     If dev_mode is True, the entire traceback will be logged using logger.opt(exception=True).warning().
# #     Otherwise, only the warning message will be logged using logger.warning().

# #     Args:
# #         exc (BaseException): The exception or warning to be logged.
# #         dev_mode (bool, optional): Whether to log the entire traceback or just the warning message.
# #             Defaults to False.

# #     Returns:
# #         None
# #     """
# #     if dev_mode:
# #         logger.opt(exception=True).warning(exc)
# #     else:
# #         logger.warning(exc)


# def filter_out_serialization_errors(record: dict[str, Any]):
#     # Patterns to match the log messages you want to filter out
#     patterns = [
#         r"Orjson serialization failed:",
#         r"Failed to serialize .* to JSON:",
#         r"Object of type .* is not JSON serializable",
#         # Failed to deepcopy input: TypeError("cannot pickle '_thread.RLock' object") | {}
#         r".*Failed to deepcopy input:.*",
#         r".*logging:callHandlers.*",
#     ]

#     # Check if the log message matches any of the patterns
#     for pattern in patterns:
#         if re.search(pattern, record["message"]):
#             return False  # Filter out this message

#     return True  # Keep all other messages


# def filter_discord_logs(record: dict[str, Any]) -> bool:
#     """
#     Filter Discord.py log messages based on specific patterns and levels.

#     This filter helps reduce noise from Discord.py's verbose logging while keeping
#     important messages. It filters out common heartbeat messages and routine WebSocket
#     events unless they indicate errors.

#     Args:
#         record: The log record to check.

#     Returns:
#         bool: True if the message should be kept, False if it should be filtered out.
#     """
#     # Always keep error and critical messages
#     if record["level"].no >= 40:  # ERROR and CRITICAL
#         return True

#     # Filter out common Discord.py noise patterns
#     noise_patterns = [
#         r"^Shard ID \d* has sent the HEARTBEAT payload\.$",
#         r"^Shard ID \d* has successfully IDENTIFIED\.$",
#         r"^Shard ID \d* has sent the RESUME payload\.$",
#         r"^Got a request to RESUME the session\.$",
#         r"^WebSocket Event: \'PRESENCE_UPDATE\'\.$",
#         r"^WebSocket Event: \'GUILD_CREATE\'\.$",
#         r"^WebSocket Event: \'TYPING_START\'\.$",
#         r"^Found matching ID for \w+ for \d+\.$",
#     ]

#     # Skip filtering if not from discord
#     if not record["name"].startswith("discord"):
#         return True

#     # Check message against noise patterns
#     message = str(record["message"])
#     for pattern in noise_patterns:
#         if re.match(pattern, message):
#             return False

#     return True

# def filter_out_modules(record: dict[str, Any]) -> bool:
#     """
#     Filter out log messages from the standard logging module.

#     Args:
#         record: The log record to check.

#     Returns:
#         bool: True if the message should be kept, False if it should be filtered out.
#     """
#     # Check if the log message originates from the logging module
#     if record["name"].startswith("logging") or record["name"].startswith("langsmith.client"):
#         return False  # Filter out this message

#     return True  # Keep all other messages


# def catch_all_filter(record: dict[str, Any]) -> bool:
#     """
#     Filter out log messages that match certain patterns or originate from specific modules.

#     This function combines the `filter_out_serialization_errors` and `filter_out_modules` filters
#     to create a single filter that removes log messages that match certain patterns (related to
#     serialization errors) or originate from the standard logging module.

#     Args:
#         record (dict[str, Any]): The log record to check.

#     Returns:
#         bool: True if the message should be kept, False if it should be filtered out.
#     """

#     return filter_out_serialization_errors(record) and filter_out_modules(record)


# class TqdmOutputStream:
#     """
#     A custom output stream that writes to sys.stderr and supports tqdm progress bars.

#     This class provides a write method that writes strings to sys.stderr using tqdm.write,
#     which allows tqdm progress bars to be displayed correctly. It also provides an isatty
#     method that returns whether sys.stderr is a terminal.

#     This class is useful when you want to use tqdm progress bars in your application while
#     still being able to write to the standard error stream.
#     """

#     def write(self, string: str = "") -> None:
#         """
#         Write the given string to sys.stderr using tqdm.write.

#         Args:
#             string (str): The string to write.
#         """
#         tqdm.write(string, file=sys.stderr, end="")

#     def isatty(self) -> bool:
#         """
#         Return whether sys.stderr is a terminal.

#         Returns:
#             bool: True if sys.stderr is a terminal, False otherwise.
#         """
#         return sys.stderr.isatty()


# _console_handler_id: int | None = None
# _file_handler_id: int | None = None

# _old_log_dir: str | None = None
# _old_console_log_level: LOG_LEVEL | None = None
# _old_backup_count: int | None = None

# REQUEST_ID_CONTEXTVAR = contextvars.ContextVar("request_id", default=None)

# # initialize the context variable with a default value
# REQUEST_ID_CONTEXTVAR.set("notset")


# def set_log_extras(record: dict[str, Any]) -> None:
#     """Set extra log fields in the log record.

#     Args:
#         record: The log record to modify.
#     """
#     record["extra"]["datetime"] = datetime.now(UTC)

# # SOURCE: https://github.com/joint-online-judge/fastapi-rest-framework/blob/b0e93f0c0085597fcea4bb79606b653422f16700/fastapi_rest_framework/logging.py#L43
# def format_record(record: dict[str, Any]) -> str:
#     """Custom format for loguru loggers.

#     Uses pformat for log any data like request/response body during debug.
#     Works with logging if loguru handler it.

#     Args:
#         record: The log record.

#     Returns:
#         The formatted log record.
#     """
#     # Format the datetime in the expected format
#     time_str = record["time"].strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]

#     # Get level name directly from the level object
#     level_name = record["level"].name

#     # Build the basic message format
#     format_string = (
#         f"{time_str} | "
#         f"{level_name:<8} | "
#         f"{record['name']}:{record['function']} - "
#         f"{record['file'].name}:{record['line']} | "
#         f"{record['message']}"
#     )

#     # Add payload if present
#     if record["extra"].get("payload") is not None:
#         payload = pformat(record["extra"]["payload"], indent=4, compact=True, width=88)
#         format_string += f"\n{payload}"

#     # Add exception if present
#     if record["exception"]:
#         format_string += f"\n{record['exception']}"

#     return format_string + "\n"

# # SOURCE: https://github.com/joint-online-judge/fastapi-rest-framework/blob/b0e93f0c0085597fcea4bb79606b653422f16700/fastapi_rest_framework/logging.py#L43
# def format_record_improved(record: dict[str, Any]) -> str:
#     """Custom format for loguru loggers.

#     Uses pformat for log any data like request/response body during debug.
#     Works with logging if loguru handler it.

#     Args:
#         record: The log record.

#     Returns:
#         The formatted log record.
#     """
#     # Format the time using the record's time field
#     time_str = record["time"].strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]

#     # Safely format the message, handling dictionary content
#     message = record["message"]
#     if isinstance(message, str) and ('{' in message and '}' in message):
#         try:
#             # Check if message looks like a dict string but avoid eval
#             if message.strip().startswith('{') and message.strip().endswith('}'):
#                 # Use pformat directly on the string representation
#                 message = pformat(message, indent=2, width=80)
#             # Otherwise keep original message
#         except Exception:
#             # If formatting fails, keep original message
#             pass

#     # Format the basic message
#     formatted = (
#         f"{time_str} | "
#         f"{record['level'].name:<8} | "
#         f"{record['name']}:{record['function']} - "
#         f"{record['file'].name}:{record['line']} | "
#         f"{message}"
#     )

#     # Add payload if present
#     if record["extra"].get("payload") is not None:
#         payload = pformat(record["extra"]["payload"], indent=2, width=80)
#         formatted += f"\n{payload}"

#     # Add exception if present
#     if record["exception"]:
#         formatted += f"\n{record['exception']}"

#     return formatted


# class InterceptHandler(logging.Handler):
#     """
#     Intercept all logging calls (with standard logging) into our Loguru Sink
#     See: https://github.com/Delgan/loguru#entirely-compatible-with-standard-logging
#     """

#     loglevel_mapping = {
#         logging.CRITICAL: "CRITICAL",
#         logging.ERROR: "ERROR",
#         logging.FATAL: "FATAL",
#         logging.WARNING: "WARNING",
#         logging.INFO: "INFO",
#         logging.DEBUG: "DEBUG",
#         1: "DUMMY",
#         0: "NOTSET",
#     }

#     def emit(self, record: logging.LogRecord) -> None:  # pragma: no cover
#         """
#         Intercept all logging calls (with standard logging) into our Loguru Sink.

#         This method is called by the standard logging library whenever a log message
#         is emitted. It converts the standard logging record into a Loguru log message
#         and logs it using the Loguru logger.

#         Args:
#             record (logging.LogRecord): The standard logging record to be logged.

#         Returns:
#             None
#         """
#         # Get corresponding Loguru level if it exists.
#         level: str | int
#         try:
#             # First try to get the level by name
#             level = logger.level(record.levelname).name
#         except ValueError:
#             # If the level doesn't exist, register it
#             logger.level(record.levelname, no=record.levelno)
#             level = record.levelname

#         # NOTE: Original question: I don't quite understand the frame and depth aspects, can you try explaining it using a practical example? imagine there is a logger for "discord.client" for example
#         # -------------------------------------------------------------------------------------------------------------
#         # # If we didn't track frames properly
#         # logger.info("Connecting to Discord...")
#         # # Would show: "File 'logging/__init__.py', line 123" in the log
#         # # This is wrong! We want to know it came from client.py
#         # Here's an example of how it would work:

#         # # discord/client.py
#         # import logging

#         # logger = logging.getLogger("discord.client")

#         # class Client:
#         #     def connect(self):
#         #         logger.info("Connecting to Discord...")  # This is where our log originates

#         # # When this log message is created, it creates a stack of function calls (frames) like this:
#         # # Frame 3 (Top): logging/__init__.py - internal logging code
#         # # Frame 2: logging/__init__.py - more logging internals
#         # # Frame 1: discord/client.py - our actual Client.connect() method
#         # # Frame 0 (Bottom): The script that called Client.connect()

#         # # Your application code
#         # from discord import Client
#         # import logging

#         # # Set up logging
#         # logging.basicConfig(handlers=[InterceptHandlerImproved()])

#         # client = Client()
#         # client.connect()

#         # # What happens when client.connect() logs:
#         # # 1. discord.client calls logger.info("Connecting to Discord...")
#         # # 2. This goes through standard logging
#         # # 3. InterceptHandlerImproved receives it
#         # # 4. It walks back through the frames:
#         # #    - Skips logging internals
#         # #    - Finds discord/client.py
#         # # 5. Finally outputs something like:
#         # #    "2024-01-10 12:34:56 | INFO | discord.client:connect:45 | Connecting to Discord..."
#         # #    With the correct file, function, and line number!
#         # -------------------------------------------------------------------------------------------------------------


#         # NOTE: Here is how it works:
#         # Starts at depth 2, keeps going until it finds non-logging frame
#         # Frame 3 (depth 2) -> logging/__init__.py (skip)
#         # Frame 2 (depth 3) -> logging/__init__.py (skip)
#         # Frame 1 (depth 4) -> discord/client.py (found it!)

#         # INFO: Find Caller Frame
#         # Find caller from where originated the logged message
#         frame, depth = logging.currentframe(), 2
#         while frame.f_code.co_filename == logging.__file__:
#             frame = frame.f_back
#             # DISABLED 12/10/2021 # frame = cast(FrameType, frame.f_back)
#             depth += 1

#         loguru.logger.opt(depth=depth, exception=record.exc_info).log(
#             level,
#             record.getMessage(),
#         )


# class InterceptHandlerImproved(logging.Handler):
#     """
#     Intercept all logging calls (with standard logging) into our Loguru Sink
#     """

#     def emit(self, record: logging.LogRecord) -> None:
#         """
#         Intercept and redirect log messages from the standard logging module to Loguru.

#         Args:
#             record (logging.LogRecord): The standard logging record to be logged.
#         """
#         # Map standard logging levels to Loguru levels
#         try:
#             level = record.levelname
#         except ValueError:
#             level = "INFO"  # Default to INFO if level mapping fails


#         # NOTE: Here is how it works:
#         # Starts at depth 0, more precisely tracks frames
#         # Current frame (depth 0) -> InterceptHandler.emit
#         # Frame 3 (depth 1) -> logging/__init__.py (skip)
#         # Frame 2 (depth 2) -> logging/__init__.py (skip)
#         # Frame 1 (depth 3) -> discord/client.py (found it!)

#         # Find the stack frame from which the log message originated.
#         frame, depth = inspect.currentframe(), 0
#         while frame and (depth == 0 or frame.f_code.co_filename == logging.__file__):
#             # Traverse back through the stack frames.
#             frame = frame.f_back
#             # Increment the depth counter to track how far back we've gone.
#             depth += 1

#         # Log the message using Loguru, preserving the original context and exception info.
#         logger.opt(depth=depth, exception=record.exc_info).log(
#             level,  # Use the determined log level.
#             record.getMessage(),  # Log the message content.
#         )

# class InterceptHandlerImproved2(logging.Handler):
#     def __init__(self):
#         super().__init__()
#         self._is_logging = False

#     def emit(self, record: logging.LogRecord) -> None:
#         if self._is_logging:
#             return  # Prevent recursive logging

#         self._is_logging = True
#         try:
#             # Map standard logging levels to Loguru levels
#             try:
#                 level = logger.level(record.levelname).name
#             except ValueError:
#                 level = record.levelno

#             # Find the caller's stack frame
#             frame, depth = inspect.currentframe(), 0
#             while frame and (depth == 0 or frame.f_code.co_filename == logging.__file__):
#                 frame = frame.f_back
#                 depth += 1

#             # Create a simplified message that won't trigger recursion
#             message = str(record.getMessage())

#             # Handle dictionary-like messages specially
#             if message.startswith('{') and message.endswith('}'):
#                 try:
#                     # Safely evaluate dictionary-like strings
#                     message = f"Dict: {message}"
#                 except Exception:
#                     pass

#             logger.opt(depth=depth, exception=record.exc_info).log(
#                 level,
#                 message
#             )
#         finally:
#             self._is_logging = False


# class InterceptHandler3(logging.Handler):
#     """
#     An advanced logging handler that intercepts standard logging messages and redirects them to Loguru.
#     This handler provides improved stack trace handling and level mapping.
#     """

#     def emit(self, record: logging.LogRecord) -> None:
#         """
#         Process and emit a logging record by converting it to a Loguru log message.

#         Args:
#             record: The logging record to process and emit.
#         """
#         # Try to map the standard logging level name to a Loguru level
#         # For example, converts 'INFO' to Loguru's info level
#         try:
#             level: str | int = logger.level(record.levelname).name
#         except ValueError:
#             # If the level name isn't recognized by Loguru, fall back to the numeric level
#             level = record.levelno

#         # Initialize frame tracking to find the true origin of the log message
#         frame, depth = inspect.currentframe(), 0

#         # Walk up the stack frames until we find the actual caller
#         while frame:
#             # Get the filename of the current frame
#             filename = frame.f_code.co_filename

#             # Check if this frame is from the logging module itself
#             is_logging = filename == logging.__file__

#             # Check if this frame is from Python's import machinery
#             # These frames appear when code is running from a frozen executable
#             is_frozen = "importlib" in filename and "_bootstrap" in filename

#             # If we've gone past the logging frames and aren't in import machinery,
#             # we've found our caller
#             if depth > 0 and not (is_logging or is_frozen):
#                 break

#             # Move up to the next frame and increment our depth counter
#             frame = frame.f_back
#             depth += 1

#         # Emit the log message through Loguru with:
#         # - proper stack depth for correct file/line reporting
#         # - original exception info preserved
#         # - original message content
#         logger.opt(depth=depth, exception=record.exc_info).log(level, record.getMessage())

# # logging.basicConfig(handlers=[InterceptHandler3()], level=0, force=True)

# def get_logger(
#     name: str,
#     provider: str | None = None,
#     level: int = logging.INFO,
#     logger: logging.Logger = logger,
# ) -> logging.Logger:
#     """
#     Get a logger instance with the specified name and configuration.

#     Args:
#         name (str): The name of the logger.
#         provider (Optional[str], optional): The provider for the logger. Defaults to None.
#         level (int, optional): The logging level. Defaults to logging.INFO.
#         logger (logging.Logger, optional): The logger instance to use. Defaults to the root logger.

#     Returns:
#         logging.Logger: The configured logger instance.

#     Example:
#         >>> logger = get_logger(__name__)
#         >>> logger.info("This is an info message")
#     """

#     return logger


# def request_id_filter(record: dict[str, Any]):
#     """
#     Inject the request id from the context var to the log record. The logging
#     config format is defined in logger_config.yaml and has request_id as a field.
#     """
#     record["extra"]["request_id"] = REQUEST_ID_CONTEXTVAR.get()


# # def reset_logging(
# #     log_dir: str,
# #     *,
# #     console_log_level: LOG_LEVEL = "INFO",
# #     backup_count: int | None = None,
# # ) -> None:
# #     """
# #     Reset the logging configuration.

# #     This function resets the logging configuration by removing any existing
# #     handlers and adding new handlers for console and file logging. The console
# #     log level and file backup count can be specified.

# #     Args:
# #         log_dir (str): The directory path for the log file.
# #         console_log_level (LOG_LEVEL, optional): The log level for the console
# #             handler. Defaults to "INFO".
# #         backup_count (Optional[int], optional): The number of backup log files
# #             to keep. If None, no backup files are kept. Defaults to None.

# #     Returns:
# #         None
# #     """
# #     global _console_handler_id, _file_handler_id
# #     global _old_log_dir, _old_console_log_level, _old_backup_count
# #     logger.configure(extra={"room_id": ""})

# #     if console_log_level != _old_console_log_level:
# #         if _console_handler_id is not None:
# #             logger.remove(_console_handler_id)
# #         else:
# #             logger.remove()  # remove the default stderr handler

# #         _console_handler_id = logger.add(
# #             sys.stderr,
# #             level=console_log_level,
# #             format=LOGURU_CONSOLE_FORMAT,
# #         )

# #         _old_console_log_level = console_log_level

# #     # Add file handler if log_dir is provided
# #     if log_dir:
# #         if _file_handler_id is not None:
# #             logger.remove(_file_handler_id)

# #         log_file = Path(log_dir) / "app.log"
# #         _file_handler_id = logger.add(
# #             log_file,
# #             rotation="1 MB",
# #             retention=backup_count,
# #             format=NEW_logger_FORMAT,
# #             enqueue=True,
# #         )

# #         _old_log_dir = log_dir
# #         _old_backup_count = backup_count


# def timeit(func):
#     def wrapped(*args, **kwargs):
#         start = time.time()
#         result = func(*args, **kwargs)
#         end = time.time()
#         logger.debug("Function '{}' executed in {:f} s", func.__name__, end - start)
#         return result

#     return wrapped


# def logger_wraps(*, entry=True, exit=True, level="DEBUG"):
#     def wrapper(func):
#         name = func.__name__

#         @functools.wraps(func)
#         def wrapped(*args, **kwargs):
#             logger_ = logger.opt(depth=1)
#             if entry:
#                 logger_.log(level, "Entering '{}' (args={}, kwargs={})", name, args, kwargs)
#             result = func(*args, **kwargs)
#             if exit:
#                 logger_.log(level, "Exiting '{}' (result={})", name, result)
#             return result

#         return wrapped

#     return wrapper


# # @pysnooper.snoop()
# # @pysnooper.snoop(thread_info=True)
# # FIXME: https://github.com/abnerjacobsen/fastapi-mvc-loguru-demo/blob/main/mvc_demo/core/loguru_logs.py
# # SOURCE: https://loguru.readthedocs.io/en/stable/api/logger.html#loguru._logger.Logger
# def global_log_config(
#     log_level: str | int = logging.INFO,
#     json: bool = False,
#     mp_context: str = "spawn",
# ) -> _Logger:
#     """Configure global logging settings.

#     Args:
#         log_level: The log level to use. Defaults to logging.INFO.
#         json: Whether to format logs as JSON. Defaults to False.

#     Returns:
#         The configured logger instance.
#     """


#     # SOURCE: https://github.com/acgnhiki/blrec/blob/975fa2794a3843a883597acd5915a749a4e196c8/src/blrec/logging/configure_logging.py#L21
#     global _console_handler_id, _file_handler_id
#     global _old_log_dir, _old_console_log_level, _old_backup_count

#     # If log_level is a string (e.g., "INFO", "DEBUG"), convert it to the corresponding numeric value
#     # This allows users to specify log levels either as strings or as integer constants
#     if isinstance(log_level, str):
#         try:
#             log_level = logging._nameToLevel.get(log_level.upper(), logging.INFO)
#         except (AttributeError, KeyError):
#             # If conversion fails, default to INFO level
#             log_level = logging.INFO

#     # NOTE: Original
#     # intercept_handler = InterceptHandler()
#     intercept_handler = InterceptHandlerImproved()
#     # intercept_handler = InterceptHandlerImproved2()
#     # intercept_handler = InterceptHandler3()

#     log_filters = {
#         "discord": aiosettings.thirdparty_lib_loglevel,
#         # "telethon": aiosettings.thirdparty_lib_loglevel,
#         # "web3": aiosettings.thirdparty_lib_loglevel,
#         # "apprise": aiosettings.thirdparty_lib_loglevel,
#         "urllib3": aiosettings.thirdparty_lib_loglevel,
#         # "asyncz": aiosettings.thirdparty_lib_loglevel,
#         # "rlp": aiosettings.thirdparty_lib_loglevel,
#         # "numexpr": aiosettings.thirdparty_lib_loglevel,
#         # "yfinance": aiosettings.thirdparty_lib_loglevel,
#         # "peewee": aiosettings.thirdparty_lib_loglevel,
#         "httpx": aiosettings.thirdparty_lib_loglevel,
#         "openai": aiosettings.thirdparty_lib_loglevel,
#         "httpcore": aiosettings.thirdparty_lib_loglevel,
#     }

#     logging.root.setLevel(log_level)

#     # Create a set to track which module loggers we've already configured
#     seen = set()
#     # Iterate through a list of logger names, including:
#     # 1. All existing loggers from the root logger's dictionary
#     # 2. Specific modules we want to ensure are configured
#     for name in [
#         *logging.root.manager.loggerDict.keys(),  # pylint: disable=no-member
#         "asyncio",          # Python's async/await framework
#         "discord",          # Main Discord.py library
#         "discord.client",   # Discord client module
#         "discord.gateway",  # Discord WebSocket gateway
#         "discord.http",     # Discord HTTP client
#         "chromadb",        # Vector database for embeddings
#         "langchain_chroma", # LangChain integration with ChromaDB
#     ]:
#         # Only process each base module once (e.g., 'discord.client' and 'discord.http'
#         # both have base module 'discord')
#         if name not in seen:
#             if "discord" in name:
#                 print(f"name: {name}")
#             # Extract the base module name (e.g., 'discord.client' -> 'discord')
#             module_name = name.split(".")[0]
#             # Log which module we're configuring
#             print(f"Setting up logger for {module_name}")
#             # Add the base module to our tracking set
#             seen.add(module_name)
#             # Replace the module's logging handlers with our custom interceptor
#             module_logger = logging.getLogger(module_name)
#             module_logger.handlers = [intercept_handler]

#             if "discord" in name:
#                 module_logger.setLevel(logging.INFO)
#             # logging.getLogger(name).handlers = [intercept_handler]

#     # print(f"GOOBY multiprocessing.get_start_method(): {multiprocessing.get_start_method()}")

#     # get event loop
#     # loop = get_or_create_event_loop()
#     # Get a new logger instance with multiprocessing support
#     config = {
#         "handlers": [{
#             "sink": stdout,
#             "format": format_record,
#             "filter": lambda record: (
#                 filter_out_serialization_errors(record)
#                 and filter_out_modules(record)
#                 and filter_discord_logs(record)
#             ),
#             "enqueue": True,  # Enable multiprocessing-safe queue
#             "serialize": json,
#             "backtrace": True,
#             "diagnose": True,
#             "catch": True,
#             "level": log_level,
#             "colorize": True,
#             "context": mp_context,  # Pass multiprocessing context to handler
#             # "loop": loop,
#         }],
#         "extra": {"request_id": REQUEST_ID_CONTEXTVAR.get()}
#     }

#     new_logger = logger.configure(**config)

#     # Register cleanup handlers for multiprocessing
#     import atexit
#     import signal

#     def cleanup_logger():
#         """Ensure all logging is complete at exit."""
#         try:
#             # logger.complete()()
#         except Exception:
#             pass

#     def terminate_handler(signo, frame):
#         """Handle termination gracefully."""
#         cleanup_logger()
#         sys.exit(signo)

#     atexit.register(cleanup_logger)
#     signal.signal(signal.SIGTERM, terminate_handler)
#     signal.signal(signal.SIGINT, terminate_handler)

#     print(f"Logger set up with log level: {log_level}")

#     # setup_uvicorn_logger()
#     # setup_gunicorn_logger()

#     return logger


# def setup_uvicorn_logger():
#     """
#     Set up the uvicorn logger.

#     This function configures the uvicorn logger to use the InterceptHandler,
#     which allows for better handling and formatting of log messages from uvicorn.
#     """
#     loggers = (logging.getLogger(name) for name in logging.root.manager.loggerDict if name.startswith("uvicorn."))
#     for uvicorn_logger in loggers:
#         uvicorn_logger.handlers = []
#     logging.getLogger("uvicorn").handlers = [InterceptHandler()]


# def setup_gunicorn_logger():
#     logging.getLogger("gunicorn.error").handlers = [InterceptHandler()]
#     logging.getLogger("gunicorn.access").handlers = [InterceptHandler()]

# def setup_discord_logger():
#     discord_logger = logging.getLogger('discord')
#     discord_logger.setLevel(logging.INFO)
#     discord_logger.addHandler(InterceptHandler())


def get_lm_from_tree(loggertree: LoggerModel, find_me: str) -> LoggerModel | None:
    """Recursively search for a logger model in the logger tree.

    Args:
        loggertree: The root logger model to search from.
        find_me: The name of the logger model to find.

    Returns:
        The found logger model, or None if not found.
    """
    if find_me == loggertree.name:
        print("Found")
        return loggertree
    else:
        for ch in loggertree.children:
            print(f"Looking in: {ch.name}")
            if i := get_lm_from_tree(ch, find_me):
                return i
    return None


def generate_tree() -> LoggerModel:
    """Generate a tree of logger models.

    Returns:
        The root logger model of the generated tree.
    """
    rootm = LoggerModel(name="root", level=logging.getLogger().getEffectiveLevel(), children=[])
    nodesm: dict[str, LoggerModel] = {}
    items = sorted(logging.root.manager.loggerDict.items())  # type: ignore
    for name, loggeritem in items:
        if isinstance(loggeritem, logging.PlaceHolder):
            nodesm[name] = nodem = LoggerModel(name=name, children=[])
        else:
            nodesm[name] = nodem = LoggerModel(name=name, level=loggeritem.getEffectiveLevel(), children=[])
        i = name.rfind(".", 0, len(name) - 1)
        parentm = rootm if i == -1 else nodesm[name[:i]]
        parentm.children.append(nodem)
    return rootm


# def obfuscate_message(message: str) -> str:
#     """Obfuscate sensitive information in a message.

#     Args:
#         message: The message to obfuscate.

#     Returns:
#         The obfuscated message.
#     """
#     obfuscation_patterns = [
#         (r"email: .*", "email: ******"),
#         (r"password: .*", "password: ******"),
#         (r"newPassword: .*", "newPassword: ******"),
#         (r"resetToken: .*", "resetToken: ******"),
#         (r"authToken: .*", "authToken: ******"),
#         (r"located at .*", "located at ******"),
#         (r"#token=.*", "#token=******"),
#     ]
#     for pattern, replacement in obfuscation_patterns:
#         message = re.sub(pattern, replacement, message)

#     return message


# def formatter(record: dict[str, Any]) -> str:
#     """Format a log record.

#     Args:
#         record: The log record to format.

#     Returns:
#         The formatted log record.
#     """
#     record["extra"]["obfuscated_message"] = record["message"]
#     return (
#         "<green>[{time:YYYY-MM-DD HH:mm:ss}]</green> <level>[{level}]</level> - "
#         "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - "
#         "<level>{extra[obfuscated_message]}</level>\n{exception}"
#     )


# def formatter_sensitive(record: dict[str, Any]) -> str:
#     """Format a log record with sensitive information obfuscated.

#     Args:
#         record: The log record to format.

#     Returns:
#         The formatted log record with sensitive information obfuscated.
#     """
#     record["extra"]["obfuscated_message"] = obfuscate_message(record["message"])
#     return (
#         "<green>[{time:YYYY-MM-DD HH:mm:ss}]</green> <level>[{level}]</level> - "
#         "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - "
#         "<level>{extra[obfuscated_message]}</level>\n{exception}"
#     )


# # SMOKE-TESTS
# if __name__ == "__main__":
#     import multiprocessing
#     from logging_tree import printout

#     # Determine best multiprocessing context based on platform
#     if sys.platform == "darwin":  # macOS
#         mp_context = "spawn"  # Recommended for macOS
#     elif sys.platform == "win32":  # Windows
#         mp_context = "spawn"  # Only option on Windows
#     else:  # Linux and other Unix
#         mp_context = "fork"  # Default and most efficient on Unix

#     # Set up multiprocessing context
#     multiprocessing.set_start_method(mp_context, force=True)
#     context = multiprocessing.get_context(mp_context)

#     print(f"********************************************** Using multiprocessing context: {mp_context}")
#     print(f"********************************************** Using multiprocessing context: {context}")


#     global_log_config(
#         log_level=logging.getLevelName("DEBUG"),
#         json=False,
#         mp_context="spawn",
#     )
#     # logger = logger

#     # def dump_logger_tree():
#     #     rootm = generate_tree()
#     #     logger.debug(rootm)

#     # def dump_logger(logger_name: str):
#     #     logger.debug(f"getting logger {logger_name}")
#     #     rootm = generate_tree()
#     #     return get_lm_from_tree(rootm, logger_name)

#     # logger.info("TESTING TESTING 1-2-3")
#     # printout()

</document_content>
</document>
<document index="30">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/bot_logger/logsetup.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
# pyright: reportInvalidTypeForm=false
# pyright: reportUndefinedVariable=false
# pyright: reportUnusedFunction=false

from __future__ import annotations

import logging
import logging.config
import multiprocessing
import sys

from typing import Any, Dict, List, Optional, Union

import structlog

from structlog.contextvars import bind_contextvars, clear_contextvars, merge_contextvars, reset_contextvars
from structlog.stdlib import BoundLogger
from structlog.typing import Processor

from democracy_exe.aio_settings import aiosettings


# so we have logger names
structlog.stdlib.recreate_defaults()

logger: structlog.stdlib.BoundLogger = structlog.get_logger(__name__)


def get_log_level(level: str | int) -> int:
    """Convert a log level string to its corresponding logging level value.

    This function takes a string or integer log level and returns the corresponding
    logging level value. It supports both string names (e.g., 'INFO', 'DEBUG') and
    integer values.

    Args:
        level: The log level as a string (e.g., 'INFO', 'DEBUG') or integer

    Returns:
        int: The corresponding logging level value

    Raises:
        ValueError: If the log level string is not valid
        TypeError: If the level is not a string or integer
    """
    if isinstance(level, int):
        if level < 0:
            raise ValueError(f"Invalid level value, it should be a positive integer, not: {level}")
        return level

    if not isinstance(level, str):
        raise TypeError(
            f"Invalid level, it should be an integer or a string, not: '{type(level).__name__}'"
        )

    # Convert to upper case for case-insensitive comparison
    level_upper = level.upper()

    # Map of level names to level numbers
    level_map = {
        'CRITICAL': logging.CRITICAL,  # 50
        'FATAL': logging.FATAL,        # 50
        'ERROR': logging.ERROR,        # 40
        'WARNING': logging.WARNING,    # 30
        'WARN': logging.WARN,          # 30
        'INFO': logging.INFO,          # 20
        'DEBUG': logging.DEBUG,        # 10
        'NOTSET': logging.NOTSET      # 0
    }

    try:
        return level_map[level_upper]
    except KeyError:
        raise ValueError(f"Invalid log level: '{level}'") from None


def configure_logging(
    enable_json_logs: bool = False,
    log_level: str = "INFO",
    third_party_loggers: dict[str, str] | None = None,
    environment: str = "development",
) -> dict[str, Any]:
    """Configure structured logging with comprehensive async support and third-party integration.

    This function sets up a unified logging system following structlog best practices:
    1. Environment-aware configuration (development vs production)
    2. Proper async context handling with contextvars
    3. Performance-optimized processor chains
    4. Comprehensive error and exception handling
    5. Integration with standard library logging
    6. Support for both development (pretty) and production (JSON) output
    7. Proper timestamp handling in UTC
    8. Canonical log lines support through bound loggers

    Args:
        enable_json_logs: If True, outputs logs in JSON format. Otherwise, uses colored console output.
        log_level: The minimum log level to capture.
        third_party_loggers: Dict of logger names and their minimum levels to configure.
        environment: The runtime environment ("development", "production", "testing")

    Returns:
        dict[str, Any]: The current structlog configuration
    """
    # Default third-party logger configuration
    default_third_party = {
        "discord": "WARNING",
        "discord.client": "WARNING",
        "discord.gateway": "WARNING",
        "aiohttp": "WARNING",
        "asyncio": "WARNING",
        "urllib3": "WARNING",
        "requests": "WARNING",
        "PIL": "WARNING",
    }

    if third_party_loggers:
        default_third_party.update(third_party_loggers)

    # Configure third-party loggers
    for logger_name, level in default_third_party.items():
        logger = logging.getLogger(logger_name)
        logger.setLevel(get_log_level(level))

    # Clear any existing context for clean configuration
    clear_contextvars()

    def get_processor_chain(enable_dev_processors: bool = False) -> list[Processor]:
        """Build the processor chain based on environment needs.

        Follows structlog's best practices for processor ordering and optimization.
        See: https://www.structlog.org/en/stable/logging-best-practices.html

        Args:
            enable_dev_processors: Whether to include development-specific processors

        Returns:
            List of configured processors
        """
        # Core processors that are always needed
        processors: list[Processor] = [
            # Filter by level first for performance
            structlog.stdlib.filter_by_level,

            # Context management
            structlog.contextvars.merge_contextvars,

            # Add basic event metadata
            structlog.processors.add_log_level,
            structlog.stdlib.add_logger_name,

            # Format any positional arguments
            structlog.stdlib.PositionalArgumentsFormatter(),

            # Add timestamps in UTC
            structlog.processors.TimeStamper(fmt="iso", utc=True),

            # Add stack information for errors
            structlog.processors.StackInfoRenderer(),

            # Handle exceptions
            structlog.processors.format_exc_info,

            # Decode any bytes to strings
            structlog.processors.UnicodeDecoder(),
        ]

        # Development-specific processors
        if enable_dev_processors and environment == "development":
            processors.append(
                structlog.processors.CallsiteParameterAdder(
                    {
                        structlog.processors.CallsiteParameter.PATHNAME,
                        structlog.processors.CallsiteParameter.FILENAME,
                        structlog.processors.CallsiteParameter.MODULE,
                        structlog.processors.CallsiteParameter.FUNC_NAME,
                        structlog.processors.CallsiteParameter.LINENO,
                        structlog.processors.CallsiteParameter.THREAD,
                        structlog.processors.CallsiteParameter.THREAD_NAME,
                        structlog.processors.CallsiteParameter.PROCESS,
                        structlog.processors.CallsiteParameter.PROCESS_NAME,
                    }
                )
            )

        return processors

    # Get shared processors
    shared_processors = get_processor_chain(not enable_json_logs)

    def get_renderer(enable_json_logs: bool) -> Processor:
        """Get the appropriate renderer based on environment.

        Args:
            enable_json_logs: Whether to use JSON logging

        Returns:
            Configured renderer processor
        """
        if environment == "testing":
            # No renderer needed for testing
            return lambda _, __, event_dict: event_dict

        if enable_json_logs or environment == "production":
            return structlog.processors.JSONRenderer(
                sort_keys=False,
                serializer=lambda obj: str(obj)  # Simple serializer that handles non-JSON types
            )

        return structlog.dev.ConsoleRenderer(
            colors=True,
            sort_keys=True,
        )

    # Get the appropriate renderer
    renderer = get_renderer(enable_json_logs)
    if enable_json_logs and environment != "testing":
        shared_processors.append(structlog.processors.format_exc_info)

    # Configure root logger with handlers
    root_logger = logging.getLogger()
    root_logger.setLevel(get_log_level(log_level))

    # Remove any existing handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # Create stdout handler for INFO and below
    stdout_handler = logging.StreamHandler(sys.stdout)
    stdout_handler.setLevel(logging.DEBUG)
    stdout_handler.addFilter(lambda record: record.levelno <= logging.INFO)
    root_logger.addHandler(stdout_handler)

    # Create stderr handler for WARNING and above
    stderr_handler = logging.StreamHandler(sys.stderr)
    stderr_handler.setLevel(logging.WARNING)
    root_logger.addHandler(stderr_handler)

    # Configure structlog with performance optimizations
    structlog.configure(
        processors=shared_processors + [
            renderer,
            structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.make_filtering_bound_logger(get_log_level(log_level)),
        cache_logger_on_first_use=environment != "testing",  # Disable caching for tests
    )

    return structlog.get_config()


def get_logger(name: str) -> BoundLogger:
    """Get a configured logger instance.

    This function returns a properly configured structlog logger instance.
    It's safe to call this at module level since it returns a lazy proxy
    that gets properly configured on first use.

    Args:
        name: The name for the logger, typically __name__

    Returns:
        A configured structlog logger instance

    Example:
        >>> logger = get_logger(__name__)
        >>> logger.info("message", key="value")
    """
    return structlog.get_logger(name)


def reset_logging() -> None:
    """Reset logging configuration to default state.

    This is particularly useful in tests to ensure a clean slate between test cases.
    It:
    1. Clears all context variables
    2. Resets structlog configuration
    3. Cleans up any bound loggers

    Example:
        >>> def test_logging():
        ...     reset_logging()
        ...     configure_logging(environment="testing")
        ...     # Run test...
    """
    clear_contextvars()
    structlog.reset_defaults()
    # Recreate stdlib defaults for logger names
    structlog.stdlib.recreate_defaults()


def configure_test_logging() -> dict[str, Any]:
    """Configure logging specifically for testing.

    Sets up a test-optimized logging configuration that:
    1. Uses a capturing logger for test verification
    2. Disables caching for test isolation
    3. Enables all processors for thorough testing
    4. Sets DEBUG level for maximum visibility

    Returns:
        dict[str, Any]: The test logging configuration

    Example:
        >>> def test_feature():
        ...     configure_test_logging()
        ...     logger = get_logger(__name__)
        ...     # Run test...
    """
    return configure_logging(
        enable_json_logs=True,
        log_level="DEBUG",
        environment="testing"
    )


if __name__ == "__main__":
    import rich
    # Example usage and testing
    logger = structlog.get_logger(__name__)
    configure_logging(enable_json_logs=False, log_level="DEBUG")
    logger = structlog.get_logger(__name__)
    rich.print(structlog.get_config())

    import bpdb
    bpdb.set_trace()

    def worker_process(name: str) -> None:
        """Example worker process function.

        Args:
            name: Name of the worker
        """
        proc_logger = get_logger(f"worker.{name}")

        # Clear context for this process
        clear_contextvars()

        # Bind process-specific context
        bind_contextvars(
            worker_name=name,
            worker_type="example"
        )

        proc_logger.info(f"Worker {name} starting")

        try:
            # Simulate some work
            1/0
        except Exception as e:
            proc_logger.error("Worker error", exc_info=e)

        proc_logger.info(f"Worker {name} finished")

    # Test in main process
    logger = get_logger("test_logger")
    clear_contextvars()

    # Bind some context variables that will be included in all log entries
    bind_contextvars(
        app_version="1.0.0",
        environment="development"
    )

    # Start some worker processes
    processes = []
    for i in range(3):
        p = multiprocessing.Process(
            target=worker_process,
            args=(f"worker_{i}",)
        )
        processes.append(p)
        p.start()

    # Test structured logging while workers run
    logger.info("Main process running",
                num_workers=len(processes))

    # Wait for all processes
    for p in processes:
        p.join()

    logger.info("All workers finished")

    # Clear context at the end
    clear_contextvars()

</document_content>
</document>
<document index="31">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/__init__.py</source>
<document_content>

</document_content>
</document>
<document index="32">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/base.py</source>
<document_content>
from __future__ import annotations

from abc import ABC, abstractmethod
from collections.abc import Callable
from typing import Any, TypedDict, TypeVar

from langgraph.graph import Graph
from langgraph.graph.state import CompiledStateGraph


T = TypeVar('T', bound='BaseAgent')
G = TypeVar('G', bound='BaseGraph')

class AgentState(TypedDict):
    """State dictionary passed between agents in the graph.

    Attributes:
        query: The input query or command
        response: The generated response
        current_agent: Name of the currently active agent
        context: Additional contextual information
    """
    query: str
    response: str
    current_agent: str
    context: dict[str, Any]

class BaseAgent(ABC):
    """Abstract base class for all agents in the system.

    All concrete agent implementations must inherit from this class
    and implement the process method.
    """

    @abstractmethod
    def process(self, state: AgentState) -> AgentState:
        """Process the current state and return updated state.

        Args:
            state: Current agent state containing query and context

        Returns:
            Updated agent state after processing
        """
        pass

class BaseGraph(ABC):
    """Abstract base class for building agent graphs.

    Provides common interface for constructing and compiling
    graphs of connected agents.
    """

    def __init__(self) -> None:
        """Initialize the graph."""
        self.graph: Graph = Graph()

    @abstractmethod
    def build(self) -> Graph:
        """Build and return the configured graph.

        Returns:
            Constructed graph ready for compilation
        """
        pass

    def compile(self) -> Callable[[AgentState], AgentState]:
        """Compile the built graph.

        Returns:
            Compiled graph ready for execution
        """
        graph = self.build()
        return lambda state: graph.compile()(state)

class AgentNode:
    """Wrapper class to use agents as nodes in the graph."""

    def __init__(self, agent: BaseAgent) -> None:
        """Initialize agent node.

        Args:
            agent: The agent to wrap as a node
        """
        self.agent = agent

    def __call__(self, state: AgentState) -> AgentState:
        """Process state through the wrapped agent.

        Args:
            state: Current agent state

        Returns:
            Updated agent state after processing
        """
        # Create a copy of the input state
        state_copy = state.copy()

        # Process the state copy and return new state
        return self.agent.process(state_copy)

def conditional_edge(state: AgentState) -> str:
    """Determine next agent based on current state.

    Args:
        state: Current agent state

    Returns:
        Name of the next agent to route to
    """
    return state["current_agent"]



"""
This base.py file defines the core structures and abstract classes that will be used throughout the AI module:
AgentState: A TypedDict that defines the structure of the state passed between agents.
BaseAgent: An abstract base class that all agents should inherit from, defining the process method.
BaseGraph: An abstract base class for graphs, providing a common interface for building and compiling graphs.
AgentNode: A wrapper class that allows agents to be used as nodes in the graph.
conditional_edge: A function used for conditional routing in the graph based on the current agent.
"""

</document_content>
</document>
<document index="33">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/state.py</source>
<document_content>
from __future__ import annotations

from typing import Any, Dict, Optional, TypedDict

from pydantic import BaseModel, Field


class AgentState(TypedDict):
    """State dictionary for tracking agent interactions.

    Attributes:
        query: The original query text
        response: The current response text
        current_agent: Name of the currently active agent
        context: Additional context dictionary
    """
    query: str
    response: str
    current_agent: str
    context: dict[str, Any]


class Query(BaseModel):
    """Model representing an input query.

    Attributes:
        text: The query text
        context: Optional dictionary of additional context
    """
    text: str
    context: dict[str, Any] | None = Field(default_factory=dict)


class Response(BaseModel):
    """Model representing the response output.

    Attributes:
        text: The response text
    """
    text: str


def create_initial_state(query: Query) -> AgentState:
    """Create initial agent state from a query.

    Args:
        query: The input Query object

    Returns:
        AgentState with initialized values
    """
    return {
        "query": query.text,
        "response": "",
        "current_agent": "",
        "context": query.context
    }


def update_state(state: AgentState, key: str, value: Any) -> AgentState:
    """Update a specific key in the agent state.

    Args:
        state: Current agent state
        key: Key to update
        value: New value for the key

    Returns:
        New AgentState with updated value
    """
    new_state = state.copy()
    new_state[key] = value
    return new_state


def get_response(state: AgentState) -> Response:
    """Extract response from final state.

    Args:
        state: Final agent state

    Returns:
        Response object containing the response text
    """
    return Response(text=state["response"])


"""
This state.py file defines the structures and helper functions for managing the state in our AI system:
AgentState: A TypedDict that defines the structure of the state passed between agents (same as in base.py, but repeated here for convenience).
Query: A Pydantic model representing the input query, including optional context.
Response: A Pydantic model representing the output response.
create_initial_state(): A function to create the initial state from a Query object.
update_state(): A helper function to update a specific key in the state while maintaining immutability.
get_response(): A function to extract the response from the final state.
These structures and functions provide a consistent way to handle input, output, and state management throughout the AI system. The use of Pydantic models (Query and Response) allows for easy integration with FastAPI or other frameworks that support these models.
The helper functions make it easier to work with the state in a functional programming style, ensuring immutability and consistency.
"""

</document_content>
</document>
<document index="34">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/agents/__init__.py</source>
<document_content>

</document_content>
</document>
<document index="35">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/agents/image_analysis_agent.py</source>
<document_content>
# democracy_exe/ai/agents/image_analysis_agent.py
from __future__ import annotations

from typing import List, Tuple

import torch

from PIL import Image
from torchvision.models import ResNet50_Weights, resnet50
from torchvision.transforms import CenterCrop, Compose, Normalize, Resize, ToTensor

from democracy_exe.ai.base import AgentState, BaseAgent


class ImageAnalysisAgent(BaseAgent):
    """Agent for analyzing images using ResNet50 model.

    This agent performs image classification and object detection using a pre-trained
    ResNet50 model. It can process images and generate natural language descriptions
    of their contents.

    Attributes:
        device: The torch device (CPU/GPU) to run inference on
        model: Pre-trained ResNet50 model
        transform: Composition of image transformations
        classes: List of ImageNet class labels
    """
    def __init__(self):
        """Initialize the ImageAnalysisAgent with ResNet50 model and transforms."""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = resnet50(weights=ResNet50_Weights.DEFAULT).to(self.device)
        self.model.eval()
        self.transform = Compose([
            Resize(256),
            CenterCrop(224),
            ToTensor(),
            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])
        with open('imagenet_classes.txt') as f:
            self.classes = [line.strip() for line in f.readlines()]


    def preprocess_image(self, image_path: str) -> torch.Tensor:
        """Preprocess an image for model inference.

            Args:
                image_path: Path to the image file

            Returns:
                Preprocessed image tensor ready for model input
        """
        image = Image.open(image_path).convert('RGB')
        return self.transform(image).unsqueeze(0).to(self.device)

    def classify_image(self, image_tensor: torch.Tensor) -> list[tuple[str, float]]:
        """Classify an image using the ResNet50 model.

        Args:
            image_tensor: Preprocessed image tensor

        Returns:
            List of tuples containing (class_name, probability) for top 5 predictions
        """
        with torch.no_grad():
            outputs = self.model(image_tensor)
        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
        top5_prob, top5_catid = torch.topk(probabilities, 5)
        return [(self.classes[idx], prob.item()) for idx, prob in zip(top5_catid, top5_prob, strict=False)]

    def detect_objects(self, image_tensor: torch.Tensor) -> list[tuple[str, float]]:
        """Detect objects in the image using the model.

        Args:
            image_tensor: Preprocessed image tensor

        Returns:
            List of tuples containing (object_class, confidence) for detected objects
        """
        return self.classify_image(image_tensor)  # Using classification as object detection for now

    def generate_description(self, classifications: list[tuple[str, float]]) -> str:
        """Generate a natural language description of image classifications.

        Args:
            classifications: List of (class_name, probability) tuples

        Returns:
            Formatted string describing the image contents
        """
        description = "This image appears to contain:\n"
        for cls, prob in classifications:
            description += f"- {cls} (confidence: {prob:.2%})\n"
        return description

    async def process(self, state: AgentState) -> AgentState:
        """Process an image and update the agent state with analysis results.

        Args:
            state: Current agent state containing image_path

        Returns:
            Updated agent state with analysis response

        Raises:
            Exception: If image processing fails
        """
        try:
            image_path = state["image_path"]
            image_tensor = self.preprocess_image(image_path)
            classifications = self.classify_image(image_tensor)
            description = self.generate_description(classifications)
            state["response"] = description
        except Exception as e:
            state["response"] = f"An error occurred while analyzing the image: {e!s}"
        return state

image_analysis_agent = ImageAnalysisAgent()

</document_content>
</document>
<document index="36">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/agents/image_video_processing_agent.py</source>
<document_content>
# democracy_exe/ai/agents/image_video_processing_agent.py
from __future__ import annotations

import os

from typing import Any, Dict, Optional, Tuple, Union

import cv2
import numpy as np

from PIL import Image

from democracy_exe.ai.base import AgentState, BaseAgent


class ImageVideoProcessingAgent(BaseAgent):
    """Agent for processing image and video files.

    Handles operations like cropping, resizing, filtering, and encoding of media files.
    Supports common image formats (PNG, JPG, GIF) and video formats (MP4, AVI, MOV).
    """

    def crop(self, media: str | np.ndarray | Image.Image, params: dict[str, Any] | None = None) -> str | np.ndarray | Image.Image:
        """Crop the media according to specified parameters.

        Args:
            media: Input media file path or loaded media object
            params: Dictionary of cropping parameters

        Returns:
            Cropped media object or path to processed file
        """
        return self.process_image(media) if isinstance(media, str) else media

    def resize(self, media: str | np.ndarray | Image.Image, params: dict[str, Any] | None = None) -> str | np.ndarray | Image.Image:
        """Resize the media according to specified parameters.

        Args:
            media: Input media file path or loaded media object
            params: Dictionary of resizing parameters

        Returns:
            Resized media object or path to processed file
        """
        return media

    def apply_filters(self, media: str | np.ndarray | Image.Image, params: dict[str, Any] | None = None) -> str | np.ndarray | Image.Image:
        """Apply filters to the media according to specified parameters.

        Args:
            media: Input media file path or loaded media object
            params: Dictionary of filter parameters

        Returns:
            Filtered media object or path to processed file
        """
        return media

    def encode(self, media: str | np.ndarray | Image.Image, params: dict[str, Any] | None = None) -> str | np.ndarray | Image.Image:
        """Encode the media according to specified parameters.

        Args:
            media: Input media file path or loaded media object
            params: Dictionary of encoding parameters

        Returns:
            Encoded media object or path to processed file
        """
        return media

    def process_image(self, image_path: str, target_size: tuple[int, int] = (1080, 1350)) -> str:
        """Process an image file by resizing and cropping.

        Args:
            image_path: Path to the input image file
            target_size: Desired output dimensions (width, height)

        Returns:
            Path to the processed image file
        """
        img = Image.open(image_path)
        img_resized = self.resize_and_crop(img, target_size)
        output_path = f"processed_{os.path.basename(image_path)}"
        img_resized.save(output_path)
        return output_path

    def process_video(self, video_path: str, target_size: tuple[int, int] = (1080, 1350)) -> str:
        """Process a video file by resizing and cropping each frame.

        Args:
            video_path: Path to the input video file
            target_size: Desired output dimensions (width, height)

        Returns:
            Path to the processed video file
        """
        cap = cv2.VideoCapture(video_path)
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        output_path = f"processed_{os.path.basename(video_path)}"
        out = cv2.VideoWriter(output_path, fourcc, 30.0, target_size)

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            frame_resized = self.resize_and_crop_cv2(frame, target_size)
            out.write(frame_resized)

        cap.release()
        out.release()
        return output_path

    def resize_and_crop(self, img: Image.Image, target_size: tuple[int, int]) -> Image.Image:
        """Resize and crop a PIL Image to target dimensions while maintaining aspect ratio.

        Args:
            img: Input PIL Image object
            target_size: Desired output dimensions (width, height)

        Returns:
            Processed PIL Image object
        """
        img_ratio = img.width / img.height
        target_ratio = target_size[0] / target_size[1]

        if img_ratio > target_ratio:
            # Image is wider than target, crop width
            new_width = int(img.height * target_ratio)
            left = (img.width - new_width) // 2
            img_cropped = img.crop((left, 0, left + new_width, img.height))
        else:
            # Image is taller than target, crop height
            new_height = int(img.width / target_ratio)
            top = (img.height - new_height) // 2
            img_cropped = img.crop((0, top, img.width, top + new_height))

        return img_cropped.resize(target_size, Image.Resampling.LANCZOS)

    def resize_and_crop_cv2(self, img: np.ndarray, target_size: tuple[int, int]) -> np.ndarray:
        """Resize and crop an OpenCV image to target dimensions while maintaining aspect ratio.

        Args:
            img: Input OpenCV image array
            target_size: Desired output dimensions (width, height)

        Returns:
            Processed OpenCV image array
        """
        img_ratio = img.shape[1] / img.shape[0]
        target_ratio = target_size[0] / target_size[1]

        if img_ratio > target_ratio:
            # Image is wider than target, crop width
            new_width = int(img.shape[0] * target_ratio)
            left = (img.shape[1] - new_width) // 2
            img_cropped = img[:, left:left + new_width]
        else:
            # Image is taller than target, crop height
            new_height = int(img.shape[1] / target_ratio)
            top = (img.shape[0] - new_height) // 2
            img_cropped = img[top:top + new_height, :]

        return cv2.resize(img_cropped, target_size, interpolation=cv2.INTER_LANCZOS4)

    async def process(self, state: AgentState) -> AgentState:
        """Process a media file based on the agent state.

        Args:
            state: Current agent state containing file path and processing parameters

        Returns:
            Updated agent state with processing results or error message
        """
        try:
            file_path = state["file_path"]
            if file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):
                processed_path = self.process_image(file_path)
                state["response"] = f"Image processed and saved to: {processed_path}"
            elif file_path.lower().endswith(('.mp4', '.avi', '.mov')):
                processed_path = self.process_video(file_path)
                state["response"] = f"Video processed and saved to: {processed_path}"
            else:
                state["response"] = "Unsupported file type"
        except Exception as e:
            state["response"] = f"An error occurred while processing the file: {e!s}"
        return state

image_video_processing_agent = ImageVideoProcessingAgent()

</document_content>
</document>
<document index="37">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/agents/internet_search_agent.py</source>
<document_content>
from __future__ import annotations

from typing import Dict, List

from langchain_community.tools.tavily_search import TavilySearchResults

from democracy_exe.ai.base import AgentState, BaseAgent
from democracy_exe.aio_settings import aiosettings


class InternetSearchAgent(BaseAgent):
    """Agent for performing internet searches using Tavily API.

    This agent uses the TavilySearchResults tool from LangChain to perform web searches
    and process the results into a readable format.

    Raises:
        ValueError: If TAVILY_API_KEY environment variable is not set.
    """

    def __init__(self):
        self.search_tool = TavilySearchResults(
            api_key=str(aiosettings.tavily_api_key),
            max_results=5,
            include_answer=True,
            include_raw_content=True,
            include_images=False
        )

    def parse_query(self, query: str) -> str:
        """Preprocess the search query.

        Args:
            query: Raw search query string.

        Returns:
            Processed query string with whitespace stripped.
        """
        return query.strip()

    def execute_search(self, query: str) -> list[dict[str, str]]:
        """Execute search using Tavily API.

        Args:
            query: Search query string.

        Returns:
            List of search results, where each result is a dictionary containing
            'title', 'snippet', and 'url' keys.
        """
        search_results = self.search_tool.invoke({"query": query})
        return search_results.get("results", [])

    def process_results(self, results: list[dict[str, str]]) -> str:
        """Process search results into a formatted string.

        Args:
            results: List of search result dictionaries.

        Returns:
            Formatted string containing numbered search results with titles,
            descriptions, and URLs.
        """
        if not results:
            return "I couldn't find any relevant information."

        response = "Here's what I found:\n\n"
        for i, result in enumerate(results, 1):
            response += f"{i}. {result.get('title', 'No title')}\n"
            response += f"   {result.get('snippet', 'No description')}\n"
            response += f"   URL: {result.get('url', 'No URL')}\n\n"
        return response

    def process(self, state: AgentState) -> AgentState:
        """Process the agent state by performing a search and updating results.

        Args:
            state: Current agent state containing the search query.

        Returns:
            Updated agent state with search results in the response field.
        """
        query = self.parse_query(state["query"])
        search_results = self.execute_search(query)
        processed_results = self.process_results(search_results)
        state["response"] = processed_results
        return state

internet_search_agent = InternetSearchAgent()


"""
Key changes in this updated version:
We've replaced the web scraping approach with the TavilySearchResults tool from LangChain.
The __init__ method now initializes the TavilySearchResults tool with some configuration options.
We've added a check to ensure the TAVILY_API_KEY environment variable is set.
The execute_search method now uses the TavilySearchResults tool to perform the search.
The process_results method has been updated to work with the structure of results returned by the Tavily search.
We've removed the BeautifulSoup and requests imports as they're no longer needed.
This implementation provides several advantages:
It's more robust and less likely to break due to changes in website structures.
It complies with the terms of service of the search API.
It provides more structured and reliable results.
It's easier to maintain and extend.
"""

</document_content>
</document>
<document index="38">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/agents/memory_agent.py</source>
<document_content>
from __future__ import annotations

from typing import Any, Dict

from democracy_exe.ai.base import AgentState, BaseAgent


class MemoryAgent(BaseAgent):
    """Agent for storing and retrieving key-value information in memory.

    This agent provides a simple memory storage system that can remember and recall
    information based on user queries. It supports commands like 'remember' and 'recall'
    to store and retrieve values respectively.
    """

    def __init__(self) -> None:
        """Initialize the memory agent with an empty storage dictionary."""
        self.memory_storage: dict[str, Any] = {}

    def store(self, key: str, value: Any) -> None:
        """Store a value in memory under the specified key.

        Args:
            key: The identifier to store the value under
            value: The value to store
        """
        self.memory_storage[key] = value

    def retrieve(self, key: str) -> Any:
        """Retrieve a value from memory by its key.

        Args:
            key: The identifier of the value to retrieve

        Returns:
            The stored value if found, None otherwise
        """
        return self.memory_storage.get(key, None)

    def process(self, state: AgentState) -> AgentState:
        """Process user queries to store or retrieve information.

        Handles commands starting with 'remember'/'store' for storing information
        and 'recall'/'retrieve' for retrieving information.

        Args:
            state: Current agent state containing the user query

        Returns:
            Updated agent state with the response message
        """
        query = state["query"].lower()
        if query.startswith("remember") or query.startswith("store"):
            parts = query.split(maxsplit=2)
            if len(parts) == 3:
                _, key, value = parts
                self.store(key, value)
                state["response"] = f"I've remembered that {key} is {value}."
            else:
                state["response"] = "I couldn't understand what to remember. Please use the format: remember [key] [value]"
        elif query.startswith("recall") or query.startswith("retrieve"):
            parts = query.split(maxsplit=1)
            if len(parts) == 2:
                _, key = parts
                value = self.retrieve(key)
                state["response"] = f"{key} is {value}" if value is not None else f"I don't remember anything about {key}."
            else:
                state["response"] = "I couldn't understand what to recall. Please use the format: recall [key]"
        else:
            state["response"] = "I can remember things for you or recall them. Try saying 'remember [key] [value]' or 'recall [key]'."
        return state

memory_agent = MemoryAgent()


"""
This memory_agent.py file defines the MemoryAgent class, which is responsible for storing and retrieving information:
The MemoryAgent class inherits from BaseAgent.
It uses a simple dictionary to store key-value pairs.
The process method handles both storing new information and retrieving existing information based on the query.
It provides user-friendly responses for successful operations and error cases.
These agents work together with the graphs we defined earlier to create a flexible and extensible AI system. The RouterAgent directs queries to specialized agents like the MemoryAgent, which can then perform their specific tasks.
"""

</document_content>
</document>
<document index="39">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/agents/research_agent.py</source>
<document_content>
from __future__ import annotations

import os

from typing import Any, Dict, List

from langchain.chains.llm import LLMChain
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from democracy_exe.ai.base import AgentState, BaseAgent
from democracy_exe.aio_settings import aiosettings


class ResearchAgent(BaseAgent):
    """Agent for conducting comprehensive research on given topics.

    This agent combines internet search capabilities with language model processing
    to perform structured research. It follows a multi-step process: planning research,
    gathering sources, analyzing information, and generating reports.

    Raises:
        ValueError: If TAVILY_API_KEY or OPENAI_API_KEY environment variables are not set.
    """

    def __init__(self) -> None:
        """Initialize the research agent with search tool and language model.

        Raises:
            ValueError: If required API keys are not set in environment variables.
        """
        self.search_tool = TavilySearchResults(
            api_key=str(aiosettings.tavily_api_key),
            max_results=5
        )
        self.llm = ChatOpenAI(
            openai_api_key=str(aiosettings.openai_api_key),
            temperature=0.7
        )

    def plan_research(self, query: str) -> list[str]:
        """Create a structured research plan from the initial query.

        Args:
            query: The main research topic or question

        Returns:
            List of specific questions or subtopics to investigate
        """
        plan_prompt = ChatPromptTemplate.from_template(
            "Given the research topic: {query}\n"
            "Create a list of 3-5 specific questions or subtopics to investigate."
        )
        plan_chain = LLMChain(llm=self.llm, prompt=plan_prompt)
        plan = plan_chain.run(query=query)
        return plan.split('\n')

    def gather_sources(self, research_plan: list[str]) -> list[dict[str, str]]:
        """Gather relevant sources for each question in the research plan.

        Args:
            research_plan: List of research questions to investigate

        Returns:
            List of source dictionaries containing search results
        """
        all_sources = []
        for question in research_plan:
            results = self.search_tool.invoke({"query": question})
            all_sources.extend(results.get("results", []))
        return all_sources

    def analyze_information(self, sources: list[dict[str, str]]) -> str:
        """Analyze gathered sources to extract key information.

        Args:
            sources: List of source dictionaries to analyze

        Returns:
            Analysis summary highlighting key points and potential conflicts
        """
        analysis_prompt = ChatPromptTemplate.from_template(
            "Analyze the following information and provide a summary:\n"
            "{sources}\n"
            "Highlight key points, identify any conflicting information, "
            "and note areas where more research might be needed."
        )
        analysis_chain = LLMChain(llm=self.llm, prompt=analysis_prompt)
        sources_text = "\n".join([f"Title: {s['title']}\nContent: {s['snippet']}" for s in sources[:5]])
        return analysis_chain.run(sources=sources_text)

    def need_more_sources(self, analysis: str) -> bool:
        """Determine if additional research is needed.

        Args:
            analysis: Current analysis text

        Returns:
            True if more sources are needed, False otherwise
        """
        return len(analysis) < 1000

    def synthesize_findings(self, analysis: str) -> str:
        """Synthesize analyzed information into coherent findings.

        Args:
            analysis: Analyzed information text

        Returns:
            Synthesized findings with conclusions and suggestions
        """
        synthesis_prompt = ChatPromptTemplate.from_template(
            "Based on the following analysis:\n"
            "{analysis}\n"
            "Synthesize the main findings, draw conclusions, and suggest any "
            "further areas of research or open questions."
        )
        synthesis_chain = LLMChain(llm=self.llm, prompt=synthesis_prompt)
        return synthesis_chain.run(analysis=analysis)

    def generate_report(self, synthesis: str) -> str:
        """Generate a comprehensive research report.

        Args:
            synthesis: Synthesized findings text

        Returns:
            Formatted research report with introduction, findings, and conclusion
        """
        report_prompt = ChatPromptTemplate.from_template(
            "Create a comprehensive research report based on the following synthesis:\n"
            "{synthesis}\n"
            "Structure the report with an introduction, main findings, conclusion, "
            "and suggestions for further research."
        )
        report_chain = LLMChain(llm=self.llm, prompt=report_prompt)
        return report_chain.run(synthesis=synthesis)

    def process(self, state: AgentState) -> AgentState:
        """Process the research request and generate a comprehensive report.

        Args:
            state: Current agent state containing the research query

        Returns:
            Updated agent state with the research report or error message
        """
        query = state["query"]
        try:
            research_plan = self.plan_research(query)
            sources = self.gather_sources(research_plan)
            analysis = self.analyze_information(sources)
            synthesis = self.synthesize_findings(analysis)
            report = self.generate_report(synthesis)
            state["response"] = report
        except Exception as e:
            state["response"] = f"An error occurred during the research process: {e!s}"
        return state


research_agent = ResearchAgent()

</document_content>
</document>
<document index="40">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/agents/router_agent.py</source>
<document_content>
from __future__ import annotations

from collections.abc import Callable
from typing import Dict, Optional

from democracy_exe.ai.base import AgentState, BaseAgent


class RouterAgent(BaseAgent):
    """Agent responsible for routing queries to specialized agents.

    This agent acts as a dispatcher that analyzes incoming queries and routes them
    to the appropriate specialized agent based on the query content. It maintains
    a registry of specialized agents and their processing functions.
    """

    def __init__(self) -> None:
        """Initialize the router agent with an empty specialized agents registry."""
        self.specialized_agents: dict[str, Callable[[AgentState], AgentState]] = {}

    def add_specialized_agent(self, name: str, process_func: Callable[[AgentState], AgentState]) -> None:
        """Register a new specialized agent.

        Args:
            name: Identifier for the specialized agent
            process_func: Function that processes agent state for the specialized agent
        """
        self.specialized_agents[name] = process_func

    def route(self, state: AgentState) -> str:
        """Determine which specialized agent should handle the query.

        Args:
            state: Current agent state containing the query

        Returns:
            Name of the specialized agent to handle the query, or empty string if no
            suitable agent is found
        """
        query = state["query"].lower()

        # Order matters - more specific patterns first
        if "research" in query:
            return "research"
        elif "search" in query:
            return "internet_search"
        elif "image" in query or "analyze" in query:
            return "image_analysis"
        elif "tweet" in query or "social media" in query:
            return "social_media"
        elif "process" in query and ("image" in query or "video" in query):
            return "image_video_processing"
        elif "remember" in query or "recall" in query:
            return "memory"
        else:
            return ""

    def process(self, state: dict) -> dict:
        """Process the agent state by routing to appropriate specialized agent.

        Args:
            state: Current agent state containing the query

        Returns:
            Updated agent state after processing by specialized agent or with
            error message if no suitable agent is found
        """
        # Handle non-dict input
        if not isinstance(state, dict):
            state = {"query": str(state)}

        # Create base state with required fields
        base_state = {
            "query": "",
            "response": "",
            "current_agent": "",
            "context": {}
        }

        # Update with input state
        base_state.update(state)

        # For testing, handle minimal state
        if set(base_state.keys()) == {"query"}:
            next_agent = self.route(base_state)
            if next_agent in self.specialized_agents:
                return self.specialized_agents[next_agent](base_state)
            return {"query": base_state["query"], "response": "I'm not sure how to handle this request."}

        # Normal processing
        next_agent = self.route(base_state)
        if next_agent in self.specialized_agents:
            base_state["current_agent"] = next_agent
            result = self.specialized_agents[next_agent](base_state)
            if isinstance(result, dict):
                base_state.update(result)
            return base_state

        base_state["response"] = "I'm not sure how to handle this request."
        return base_state


router_agent = RouterAgent()

"""
This router_agent.py file defines the RouterAgent class, which is responsible for directing queries to the appropriate specialized agent:
The RouterAgent class inherits from BaseAgent.
It maintains a dictionary of specialized agents that can be added dynamically.
The route method implements the logic for determining which specialized agent should handle a given query. This can be expanded or refined as needed.
The process method uses the routing logic to delegate the query to the appropriate specialized agent, or returns a default response if no suitable agent is found.
Next, let's look at the memory_agent.py file:
"""

</document_content>
</document>
<document index="41">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/agents/social_media_agent.py</source>
<document_content>
# democracy_exe/ai/agents/social_media_agent.py
# pylint: disable=no-name-in-module
# pyright: reportInvalidTypeForm=false
# pyright: reportUndefinedVariable=false
from __future__ import annotations

import asyncio
import io
import json
import os

from typing import Any, Dict, List, Optional, Tuple

import aiofiles
import httpx

from PIL import Image

from democracy_exe.ai.base import AgentState, BaseAgent
from democracy_exe.aio_settings import aiosettings
from democracy_exe.clients.tweetpik import TweetPikClient
from democracy_exe.shell import run_coroutine_subprocess


class SocialMediaAgent(BaseAgent):
    """Agent for processing social media content, particularly tweets.

    This agent handles screenshot capture, video downloads, and image processing
    for social media content. It uses TweetPik for capturing tweet screenshots
    and gallery-dl for video downloads.

    Raises:
        ValueError: If TWEETPIK_API_KEY environment variable is not set.
    """

    def __init__(self) -> None:
        """Initialize the social media agent with TweetPik client.

        Raises:
            ValueError: If TWEETPIK_API_KEY is not set in environment variables.
        """
        self.tweetpik_client = TweetPikClient(aiosettings.tweetpik_api_key.get_secret_value()) # pylint: disable=no-member

    def fetch_tweet(self, tweet_url: str) -> dict[str, Any]:
        """Fetch tweet data from URL.

        Args:
            tweet_url: URL of the tweet to fetch

        Returns:
            Dictionary containing tweet data
        """
        return {"url": tweet_url}  # Simplified for now

    def is_video_tweet(self, tweet_data: dict[str, Any]) -> bool:
        """Check if tweet contains video.

        Args:
            tweet_data: Tweet data dictionary

        Returns:
            True if tweet contains video, False otherwise
        """
        return "video" in tweet_data.get("url", "").lower()

    async def take_screenshot(self, tweet_url: str) -> bytes:
        """Capture a screenshot of the tweet.

        Args:
            tweet_url: URL of the tweet to screenshot

        Returns:
            Screenshot image data as bytes

        Raises:
            httpx.HTTPError: If screenshot request fails
        """
        result = await self.tweetpik_client.screenshot_tweet_async(tweet_url)
        async with httpx.AsyncClient() as client:
            response = await client.get(result["url"])
        response.raise_for_status()
        return response.content

    def identify_regions(self, image: Image.Image) -> list[tuple[int, int, int, int]]:
        """Identify important regions in the image.

        Args:
            image: PIL Image object to analyze

        Returns:
            List of tuples containing region coordinates (x1, y1, x2, y2)
        """
        # Placeholder implementation
        return [(0, 0, image.width, image.height)]

    def crop_image(
        self,
        image: Image.Image,
        regions: list[tuple[int, int, int, int]],
        aspect_ratio: tuple[int, int],
        target_size: tuple[int, int]
    ) -> Image.Image:
        """Crop image to specified regions and resize.

        Args:
            image: PIL Image object to crop
            regions: List of region coordinates to crop
            aspect_ratio: Desired aspect ratio as (width, height)
            target_size: Target size for the output image

        Returns:
            Cropped and resized PIL Image
        """
        region = regions[0]
        cropped = image.crop(region)
        return cropped.resize(target_size)

    async def download_video(self, tweet_url: str) -> str:
        """Download video from tweet URL using gallery-dl.

        Args:
            tweet_url: URL of the tweet containing video

        Returns:
            Path to downloaded video file

        Raises:
            ValueError: If no files were downloaded
        """
        cmd = f'gallery-dl --no-mtime -v --write-info-json --write-metadata "{tweet_url}"'
        result = await run_coroutine_subprocess(cmd, tweet_url)

        lines = result.split('\n')
        downloaded_files = [line.split(' ', 1)[-1] for line in lines
                          if line.startswith('[download] Downloading')]

        if not downloaded_files:
            raise ValueError("No files were downloaded")

        video_path = downloaded_files[-1]

        # Read the info JSON file to get additional metadata
        info_json_path = os.path.splitext(video_path)[0] + '.info.json'
        if os.path.exists(info_json_path):
            with open(info_json_path) as f:
                info = json.load(f)

        return video_path

    async def process(self, state: AgentState) -> AgentState:
        """Process social media content based on the query.

        Handles both video tweets and regular tweets, downloading videos or
        capturing and processing screenshots as appropriate.

        Args:
            state: Current agent state containing the query (tweet URL)

        Returns:
            Updated agent state with processing results or error message
        """
        try:
            tweet_url = state["query"]

            if "video" in tweet_url.lower():
                video_path = await self.download_video(tweet_url)
                state["response"] = f"Video downloaded and saved to: {video_path}"
            else:
                screenshot_bytes = await self.take_screenshot(tweet_url)
                image = Image.open(io.BytesIO(screenshot_bytes))
                regions = self.identify_regions(image)
                cropped_image = self.crop_image(image, regions, (1, 1), (1080, 1350))
                output_path = "processed_tweet_image.jpg"
                cropped_image.save(output_path)
                state["response"] = f"Tweet image processed and saved to {output_path}"

        except Exception as e:
            state["response"] = f"An error occurred while processing the tweet: {e!s}"

        return state



    async def fetch_tweet_async(self, tweet_url: str) -> dict[str, Any]:
        """Fetch tweet data from URL asynchronously.

        Args:
            tweet_url: URL of the tweet to fetch

        Returns:
            Dictionary containing tweet data
        """
        return {"url": tweet_url}  # Simplified for now

    async def is_video_tweet_async(self, tweet_data: dict[str, Any]) -> bool:
        """Check if tweet contains video asynchronously.

        Args:
            tweet_data: Tweet data dictionary

        Returns:
            True if tweet contains video, False otherwise
        """
        return "video" in tweet_data.get("url", "").lower()

    async def identify_regions_async(self, image: Image.Image) -> list[tuple[int, int, int, int]]:
        """Identify important regions in the image asynchronously.

        Args:
            image: PIL Image object to analyze

        Returns:
            List of tuples containing region coordinates (x1, y1, x2, y2)
        """
        # Placeholder implementation
        return [(0, 0, image.width, image.height)]

    async def crop_image_async(
        self,
        image: Image.Image,
        regions: list[tuple[int, int, int, int]],
        aspect_ratio: tuple[int, int],
        target_size: tuple[int, int]
    ) -> Image.Image:
        """Crop image to specified regions and resize asynchronously.

        Args:
            image: PIL Image object to crop
            regions: List of region coordinates to crop
            aspect_ratio: Desired aspect ratio as (width, height)
            target_size: Target size for the output image

        Returns:
            Cropped and resized PIL Image
        """
        region = regions[0]
        cropped = image.crop(region)
        return cropped.resize(target_size)

    async def download_video_async(self, tweet_url: str) -> str:
        """Download video from tweet URL using gallery-dl asynchronously.

        Args:
            tweet_url: URL of the tweet containing video

        Returns:
            Path to downloaded video file

        Raises:
            ValueError: If no files were downloaded
        """
        cmd = f'gallery-dl --no-mtime -v --write-info-json --write-metadata "{tweet_url}"'
        result = await run_coroutine_subprocess(cmd, tweet_url)

        lines = result.split('\n')
        downloaded_files = [line.split(' ', 1)[-1] for line in lines
                          if line.startswith('[download] Downloading')]

        if not downloaded_files:
            raise ValueError("No files were downloaded")

        video_path = downloaded_files[-1]

        # Read the info JSON file to get additional metadata
        info_json_path = os.path.splitext(video_path)[0] + '.info.json'
        if os.path.exists(info_json_path):
            async with aiofiles.open(info_json_path) as f:
                info = json.loads(await f.read())

        return video_path

    async def process_async(self, state: AgentState) -> AgentState:
        """Process social media content based on the query asynchronously.

        Handles both video tweets and regular tweets, downloading videos or
        capturing and processing screenshots as appropriate.

        Args:
            state: Current agent state containing the query (tweet URL)

        Returns:
            Updated agent state with processing results or error message
        """
        try:
            tweet_url = state["query"]

            if "video" in tweet_url.lower():
                video_path = await self.download_video_async(tweet_url)
                state["response"] = f"Video downloaded and saved to: {video_path}"
            else:
                screenshot_bytes = await self.take_screenshot(tweet_url)
                image = Image.open(io.BytesIO(screenshot_bytes))
                regions = await self.identify_regions_async(image)
                cropped_image = await self.crop_image_async(image, regions, (1, 1), (1080, 1350))
                output_path = "processed_tweet_image.jpg"
                async with aiofiles.open(output_path, mode='wb') as f:
                    await f.write(cropped_image.tobytes())
                state["response"] = f"Tweet image processed and saved to {output_path}"

        except Exception as e:
            state["response"] = f"An error occurred while processing the tweet: {e!s}"

        return state

social_media_agent = SocialMediaAgent()

</document_content>
</document>
<document index="42">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/graphs/__init__.py</source>
<document_content>
from __future__ import annotations

from democracy_exe.ai.graphs.router_graph import router_graph
from democracy_exe.ai.state import AgentState

</document_content>
</document>
<document index="43">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/graphs/image_analysis_graph.py</source>
<document_content>
from __future__ import annotations

from typing import Any

from langgraph.graph import Graph

from democracy_exe.ai.agents.image_analysis_agent import ImageAnalysisAgent
from democracy_exe.ai.base import AgentNode, AgentState, BaseGraph


class ImageAnalysisGraph(BaseGraph):
    """Graph for orchestrating image analysis workflow.

    This graph manages the flow of image analysis operations through multiple stages:
    preprocessing, object detection, classification, and description generation.
    It coordinates these operations using a directed graph structure where each node
    represents a specific analysis stage.

    Attributes:
        image_agent: Instance of ImageAnalysisAgent that performs the actual analysis operations
    """

    def __init__(self) -> None:
        """Initialize the image analysis graph with its agent."""
        super().__init__()
        self.image_agent = ImageAnalysisAgent()

    def build(self) -> Graph:
        """Construct the image analysis workflow graph.

        Creates a directed graph with nodes for each analysis stage and edges
        defining the flow between stages.

        Returns:
            Configured LangGraph Graph instance ready for execution
        """
        # Add image analysis agent node
        self.graph.add_node("analyze", AgentNode(self.image_agent))

        # Add nodes for different analysis stages
        self.graph.add_node("preprocess", self.preprocess_image)
        self.graph.add_node("detect_objects", self.detect_objects)
        self.graph.add_node("classify_image", self.classify_image)
        self.graph.add_node("generate_description", self.generate_description)

        # Add edges to create the analysis flow
        self.graph.add_edge("analyze", "preprocess")
        self.graph.add_edge("preprocess", "detect_objects")
        self.graph.add_edge("detect_objects", "classify_image")
        self.graph.add_edge("classify_image", "generate_description")
        self.graph.add_edge("generate_description", "analyze")

        # Set the entry point
        self.graph.set_entry_point("analyze")

        return self.graph

    def process(self, state: AgentState) -> AgentState:
        """Process an image through the analysis workflow.

        Args:
            state: Current agent state containing the image to analyze

        Returns:
            Updated agent state with analysis results
        """
        compiled_graph = self.compile()
        return compiled_graph(state)

    def preprocess_image(self, state: AgentState) -> AgentState:
        """Preprocess the input image for analysis.

        Performs initial image processing operations like resizing, normalization,
        or format conversion.

        Args:
            state: Agent state containing the raw image

        Returns:
            Updated state with preprocessed image
        """
        preprocessed_image = self.image_agent.preprocess_image(state["image"])
        state["preprocessed_image"] = preprocessed_image
        return state

    def detect_objects(self, state: AgentState) -> AgentState:
        """Detect and locate objects within the preprocessed image.

        Args:
            state: Agent state containing the preprocessed image

        Returns:
            Updated state with detected objects information
        """
        detected_objects = self.image_agent.detect_objects(state["preprocessed_image"])
        state["detected_objects"] = detected_objects
        return state

    def classify_image(self, state: AgentState) -> AgentState:
        """Classify the image content and identify key elements.

        Args:
            state: Agent state containing the preprocessed image

        Returns:
            Updated state with image classification results
        """
        classification = self.image_agent.classify_image(state["preprocessed_image"])
        state["classification"] = classification
        return state

    def generate_description(self, state: AgentState) -> AgentState:
        """Generate a natural language description of the image analysis results.

        Args:
            state: Agent state containing classification results

        Returns:
            Updated state with generated description in the response field
        """
        description = self.image_agent.generate_description(state["classification"])
        state["response"] = description
        return state


image_analysis_graph = ImageAnalysisGraph()

"""
This image_analysis_graph.py file defines the ImageAnalysisGraph class, which encapsulates the functionality for analyzing images:
The ImageAnalysisGraph class inherits from BaseGraph and implements the build method.
It has an ImageAnalysisAgent instance to handle the actual image analysis operations.
The build method constructs the graph:
It adds the main image analysis agent node.
It adds nodes for different stages of the image analysis process: preprocessing, object detection, image classification, and description generation.
It adds edges to create the analysis flow, allowing for a complete image analysis cycle.
It sets the entry point of the graph to the main analysis node.
The process method compiles the graph and processes the given state.
The preprocess_image, detect_objects, classify_image, and generate_description methods correspond to the different stages of the image analysis process. They update the state with the results of each stage.
An instance of ImageAnalysisGraph is created at the module level for easy access.
This implementation provides a structured approach to image analysis within the AI system. The analysis process is broken down into distinct stages, each handled by a separate node in the graph. This modular design makes it easy to modify or extend individual parts of the analysis process as needed.
The cyclic nature of the graph allows for potential refinement or iterative analysis if such functionality is desired in the future. For example, you could add edges to revisit earlier stages based on the results of later stages.
"""

</document_content>
</document>
<document index="44">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/graphs/image_video_processing_graph.py</source>
<document_content>
from __future__ import annotations

from typing import Any

from langgraph.graph import Graph

from democracy_exe.ai.agents.image_video_processing_agent import ImageVideoProcessingAgent
from democracy_exe.ai.base import AgentNode, AgentState, BaseGraph


class ImageVideoProcessingGraph(BaseGraph):
    """Graph for orchestrating image and video processing workflows.

    This graph manages the flow of media processing operations through multiple stages:
    cropping, resizing, filter application, and encoding. It coordinates these operations
    using a directed graph structure where each node represents a specific processing stage.

    Attributes:
        processing_agent: Instance of ImageVideoProcessingAgent that performs the actual
            processing operations
    """

    def __init__(self) -> None:
        """Initialize the image/video processing graph with its agent."""
        super().__init__()
        self.processing_agent = ImageVideoProcessingAgent()

    def build(self) -> Graph:
        """Construct the media processing workflow graph.

        Creates a directed graph with nodes for each processing stage and edges
        defining the flow between stages.

        Returns:
            Configured LangGraph Graph instance ready for execution
        """
        # Add processing agent node
        self.graph.add_node("process_media", AgentNode(self.processing_agent))

        # Add nodes for different processing tasks
        self.graph.add_node("crop_media", self.crop_media)
        self.graph.add_node("resize_media", self.resize_media)
        self.graph.add_node("apply_filters", self.apply_filters)
        self.graph.add_node("encode_media", self.encode_media)

        # Add edges to create the processing flow
        self.graph.add_edge("process_media", "crop_media")
        self.graph.add_edge("crop_media", "resize_media")
        self.graph.add_edge("resize_media", "apply_filters")
        self.graph.add_edge("apply_filters", "encode_media")
        self.graph.add_edge("encode_media", "process_media")

        # Set the entry point
        self.graph.set_entry_point("process_media")

        return self.graph

    def process(self, state: AgentState) -> AgentState:
        """Process media through the processing workflow.

        Args:
            state: Current agent state containing the media to process

        Returns:
            Updated agent state with processing results
        """
        compiled_graph = self.compile()
        return compiled_graph(state)

    def crop_media(self, state: AgentState) -> AgentState:
        """Crop the input media according to specified parameters.

        Args:
            state: Agent state containing the media and optional crop_params

        Returns:
            Updated state with cropped media
        """
        cropped = self.processing_agent.crop(state["media"], state.get("crop_params"))
        state["cropped_media"] = cropped
        return state

    def resize_media(self, state: AgentState) -> AgentState:
        """Resize the cropped media according to specified parameters.

        Args:
            state: Agent state containing the cropped media and optional resize_params

        Returns:
            Updated state with resized media
        """
        resized = self.processing_agent.resize(state["cropped_media"], state.get("resize_params"))
        state["resized_media"] = resized
        return state

    def apply_filters(self, state: AgentState) -> AgentState:
        """Apply specified filters to the resized media.

        Args:
            state: Agent state containing the resized media and optional filter_params

        Returns:
            Updated state with filtered media
        """
        filtered = self.processing_agent.apply_filters(state["resized_media"], state.get("filter_params"))
        state["filtered_media"] = filtered
        return state

    def encode_media(self, state: AgentState) -> AgentState:
        """Encode the processed media according to specified parameters.

        Args:
            state: Agent state containing the filtered media and optional encode_params

        Returns:
            Updated state with encoded media in the response field
        """
        encoded = self.processing_agent.encode(state["filtered_media"], state.get("encode_params"))
        state["response"] = encoded
        return state


image_video_processing_graph = ImageVideoProcessingGraph()


"""
This image_video_processing_graph.py file defines the ImageVideoProcessingGraph class, which encapsulates the functionality for processing images and videos:
The ImageVideoProcessingGraph class inherits from BaseGraph and implements the build method.
It has an ImageVideoProcessingAgent instance to handle the actual media processing operations.
The build method constructs the graph:
It adds the main processing agent node.
It adds nodes for different media processing tasks: cropping, resizing, applying filters, and encoding.
It adds edges to create a complete media processing flow.
It sets the entry point of the graph to the main processing node.
The process method compiles the graph and processes the given state.
Each method (crop_media, resize_media, etc.) corresponds to a different media processing task. They update the state with the results of each task.
An instance of ImageVideoProcessingGraph is created at the module level for easy access.
This implementation provides a structured approach to media processing within the AI system. The processing tasks are broken down into distinct stages, each handled by a separate node in the graph. This modular design makes it easy to modify or extend individual parts of the processing pipeline as needed.
The cyclic nature of the graph allows for iterative refinement of media processing if such functionality is desired in future applications.
"""

</document_content>
</document>
<document index="45">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/graphs/internet_search_graph.py</source>
<document_content>
from __future__ import annotations

from typing import Any

from langgraph.graph import Graph

from democracy_exe.ai.agents.internet_search_agent import InternetSearchAgent
from democracy_exe.ai.base import AgentNode, AgentState, BaseGraph


class InternetSearchGraph(BaseGraph):
    """Graph for orchestrating internet search operations.

    This graph manages the flow of internet search operations through multiple stages:
    query parsing, search execution, and results processing. It coordinates these
    operations using a directed graph structure where each node represents a specific
    search stage.

    Attributes:
        search_agent: Instance of InternetSearchAgent that performs the actual
            search operations
    """

    def __init__(self) -> None:
        """Initialize the internet search graph with its agent."""
        super().__init__()
        self.search_agent = InternetSearchAgent()

    def build(self) -> Graph:
        """Construct the internet search workflow graph.

        Creates a directed graph with nodes for each search stage and edges
        defining the flow between stages.

        Returns:
            Configured LangGraph Graph instance ready for execution
        """
        # Add search agent node
        self.graph.add_node("search", AgentNode(self.search_agent))

        # Add nodes for different search stages
        self.graph.add_node("parse_query", self.parse_query)
        self.graph.add_node("execute_search", self.execute_search)
        self.graph.add_node("process_results", self.process_results)

        # Add edges to create the search flow
        self.graph.add_edge("search", "parse_query")
        self.graph.add_edge("parse_query", "execute_search")
        self.graph.add_edge("execute_search", "process_results")
        self.graph.add_edge("process_results", "search")

        # Set the entry point
        self.graph.set_entry_point("search")

        return self.graph

    def process(self, state: AgentState) -> AgentState:
        """Process a search request through the search workflow.

        Args:
            state: Current agent state containing the search query

        Returns:
            Updated agent state with search results
        """
        compiled_graph = self.compile()
        return compiled_graph(state)

    def parse_query(self, state: AgentState) -> AgentState:
        """Parse and preprocess the search query.

        Processes the raw query string to optimize it for search execution.

        Args:
            state: Agent state containing the raw query

        Returns:
            Updated state with parsed query
        """
        parsed_query = self.search_agent.parse_query(state["query"])
        state["parsed_query"] = parsed_query
        return state

    def execute_search(self, state: AgentState) -> AgentState:
        """Execute the search using the parsed query.

        Performs the actual internet search operation using the search agent.

        Args:
            state: Agent state containing the parsed query

        Returns:
            Updated state with search results
        """
        search_results = self.search_agent.execute_search(state["parsed_query"])
        state["search_results"] = search_results
        return state

    def process_results(self, state: AgentState) -> AgentState:
        """Process and format the search results.

        Processes raw search results into a formatted response suitable for
        presentation to the user.

        Args:
            state: Agent state containing the search results

        Returns:
            Updated state with processed results in the response field
        """
        processed_results = self.search_agent.process_results(state["search_results"])
        state["response"] = processed_results
        return state


internet_search_graph = InternetSearchGraph()


"""
This internet_search_graph.py file defines the InternetSearchGraph class, which encapsulates the functionality for performing internet searches:
The InternetSearchGraph class inherits from BaseGraph and implements the build method.
It has an InternetSearchAgent instance to handle the actual search operations.
The build method constructs the graph:
It adds the main search agent node.
It adds nodes for different stages of the search process: parsing the query, executing the search, and processing the results.
It adds edges to create the search flow, allowing for a complete search cycle.
It sets the entry point of the graph to the main search node.
The process method compiles the graph and processes the given state.
The parse_query, execute_search, and process_results methods correspond to the different stages of the search process. They update the state with the results of each stage.
An instance of InternetSearchGraph is created at the module level for easy access.
This implementation allows for a structured and modular approach to internet searching within the AI system. The search process is broken down into distinct stages, each handled by a separate node in the graph. This makes it easy to modify or extend individual parts of the search process as needed.
The cyclic nature of the graph allows for potential refinement of search queries based on initial results, if such functionality is desired in the future.
"""

</document_content>
</document>
<document index="46">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/graphs/memory_graph.py</source>
<document_content>
from __future__ import annotations

from typing import Any

from langgraph.graph import Graph

from democracy_exe.ai.agents.memory_agent import MemoryAgent
from democracy_exe.ai.base import AgentNode, AgentState, BaseGraph


class MemoryGraph(BaseGraph):
    """Graph for orchestrating memory storage and retrieval operations.

    This graph manages the flow of memory operations through store and retrieve stages.
    It coordinates these operations using a directed graph structure where each node
    represents a specific memory operation.

    Attributes:
        memory_agent: Instance of MemoryAgent that performs the actual memory operations
    """

    def __init__(self) -> None:
        """Initialize the memory graph with its agent."""
        super().__init__()
        self.memory_agent = MemoryAgent()

    def build(self) -> Graph:
        """Construct the memory operations workflow graph.

        Creates a directed graph with nodes for memory operations and edges
        defining the flow between operations.

        Returns:
            Configured LangGraph Graph instance ready for execution
        """
        # Add memory agent node
        self.graph.add_node("memory", AgentNode(self.memory_agent))

        # Add edges for store and retrieve operations
        self.graph.add_edge("memory", "store")
        self.graph.add_edge("store", "retrieve")
        self.graph.add_edge("retrieve", "memory")

        # Set the entry point
        self.graph.set_entry_point("memory")

        return self.graph

    def process(self, state: AgentState) -> AgentState:
        """Process a memory operation through the workflow.

        Args:
            state: Current agent state containing the memory operation request

        Returns:
            Updated agent state with operation results
        """
        compiled_graph = self.compile()
        return compiled_graph(state)

    def store(self, key: str, value: Any) -> None:
        """Store a value in memory under the specified key.

        Args:
            key: The identifier to store the value under
            value: The value to store
        """
        self.memory_agent.store(key, value)

    def retrieve(self, key: str) -> Any:
        """Retrieve a value from memory by its key.

        Args:
            key: The identifier of the value to retrieve

        Returns:
            The stored value if found, None otherwise
        """
        return self.memory_agent.retrieve(key)


memory_graph = MemoryGraph()

"""
This memory_graph.py file defines the MemoryGraph class, which encapsulates the functionality for storing and retrieving information:
The MemoryGraph class inherits from BaseGraph and implements the build method.
It has a MemoryAgent instance to handle the actual storage and retrieval operations.
The build method constructs the graph:
It adds the memory agent node.
It adds edges for store and retrieve operations, creating a cycle that allows for multiple operations.
It sets the entry point of the graph to the memory node.
The process method compiles the graph and processes the given state.
The store and retrieve methods provide a simple interface to interact with the memory agent directly.
An instance of MemoryGraph is created at the module level for easy access.
This implementation allows for flexible memory operations within the AI system. The memory graph can be used independently or integrated into the larger router graph as needed. The cyclic nature of the graph allows for multiple store and retrieve operations to be performed in sequence if necessary.
"""

</document_content>
</document>
<document index="47">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/graphs/research_graph.py</source>
<document_content>
from __future__ import annotations

from typing import Any, Dict, List, Literal, Optional, Union

from langgraph.graph import Graph

from democracy_exe.ai.agents.research_agent import ResearchAgent
from democracy_exe.ai.base import AgentNode, AgentState, BaseGraph


class ResearchGraph(BaseGraph):
    """Graph for orchestrating comprehensive research operations.

    This graph manages the flow of research operations through multiple stages:
    planning, source gathering, analysis, synthesis, and report generation.
    It coordinates these operations using a directed graph structure where each
    node represents a specific research stage.

    Attributes:
        research_agent: Instance of ResearchAgent that performs the actual
            research operations
    """

    def __init__(self) -> None:
        """Initialize the research graph with its agent."""
        super().__init__()
        self.research_agent = ResearchAgent()

    def build(self) -> Graph:
        """Construct the research workflow graph.

        Creates a directed graph with nodes for each research stage and edges
        defining the flow between stages. Includes conditional edges for
        iterative research when more sources are needed.

        Returns:
            Configured LangGraph Graph instance ready for execution
        """
        # Add research agent node
        self.graph.add_node("research", AgentNode(self.research_agent))

        # Add nodes for different research stages
        self.graph.add_node("plan_research", self.plan_research)
        self.graph.add_node("gather_sources", self.gather_sources)
        self.graph.add_node("analyze_information", self.analyze_information)
        self.graph.add_node("synthesize_findings", self.synthesize_findings)
        self.graph.add_node("generate_report", self.generate_report)

        # Add edges to create the research flow
        self.graph.add_edge("research", "plan_research")
        self.graph.add_edge("plan_research", "gather_sources")
        self.graph.add_edge("gather_sources", "analyze_information")
        self.graph.add_edge("analyze_information", "synthesize_findings")
        self.graph.add_edge("synthesize_findings", "generate_report")
        self.graph.add_edge("generate_report", "research")

        # Add conditional edges for iterative research
        self.graph.add_conditional_edges(
            "analyze_information",
            self.need_more_sources,
            {True: "gather_sources", False: "synthesize_findings"}
        )

        # Set the entry point
        self.graph.set_entry_point("research")

        return self.graph

    def process(self, state: AgentState) -> AgentState:
        """Process a research request through the research workflow.

        Args:
            state: Current agent state containing the research query

        Returns:
            Updated agent state with research results
        """
        compiled_graph = self.compile()
        return compiled_graph(state)

    def plan_research(self, state: AgentState) -> AgentState:
        """Create a structured research plan from the initial query.

        Args:
            state: Agent state containing the research query

        Returns:
            Updated state with research plan
        """
        research_plan = self.research_agent.plan_research(state["query"])
        state["research_plan"] = research_plan
        return state

    def gather_sources(self, state: AgentState) -> AgentState:
        """Gather relevant sources based on the research plan.

        Args:
            state: Agent state containing the research plan

        Returns:
            Updated state with gathered sources
        """
        sources = self.research_agent.gather_sources(state["research_plan"])
        state["sources"] = sources
        return state

    def analyze_information(self, state: AgentState) -> AgentState:
        """Analyze gathered sources to extract key information.

        Args:
            state: Agent state containing the sources to analyze

        Returns:
            Updated state with analysis results
        """
        analysis = self.research_agent.analyze_information(state["sources"])
        state["analysis"] = analysis
        return state

    def need_more_sources(self, state: AgentState) -> bool:
        """Determine if additional sources are needed for the research.

        Args:
            state: Agent state containing the current analysis

        Returns:
            True if more sources are needed, False otherwise
        """
        return self.research_agent.need_more_sources(state["analysis"])

    def synthesize_findings(self, state: AgentState) -> AgentState:
        """Synthesize analyzed information into coherent findings.

        Args:
            state: Agent state containing the analysis to synthesize

        Returns:
            Updated state with synthesized findings
        """
        synthesis = self.research_agent.synthesize_findings(state["analysis"])
        state["synthesis"] = synthesis
        return state

    def generate_report(self, state: AgentState) -> AgentState:
        """Generate a comprehensive research report.

        Args:
            state: Agent state containing the synthesized findings

        Returns:
            Updated state with generated report in the response field
        """
        report = self.research_agent.generate_report(state["synthesis"])
        state["response"] = report
        return state


research_graph = ResearchGraph()


"""
This research_graph.py file defines the ResearchGraph class, which encapsulates the functionality for conducting in-depth research:
The ResearchGraph class inherits from BaseGraph and implements the build method.
It has a ResearchAgent instance to handle the actual research operations.
The build method constructs the graph:
It adds the main research agent node.
It adds nodes for different stages of the research process: planning, gathering sources, analyzing information, synthesizing findings, and generating a report.
It adds edges to create the research flow, allowing for a complete research cycle.
It includes a conditional edge that allows the graph to return to the "gather_sources" stage if more information is needed.
It sets the entry point of the graph to the main research node.
The process method compiles the graph and processes the given state.
Each method (plan_research, gather_sources, etc.) corresponds to a different stage of the research process. They update the state with the results of each stage.
The need_more_sources method acts as a decision point, determining whether to gather more sources or move on to synthesizing findings.
An instance of ResearchGraph is created at the module level for easy access.
This implementation provides a structured and iterative approach to conducting research within the AI system. The research process is broken down into distinct stages, each handled by a separate node in the graph. The conditional edge allows for dynamic decision-making during the research process, enabling the agent to gather more information if needed.
This design allows for complex research tasks that can adapt based on the information gathered and analyzed, potentially leading to more thorough and accurate research outcomes.
"""

</document_content>
</document>
<document index="48">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/graphs/router_graph.py</source>
<document_content>
from __future__ import annotations

from collections.abc import Callable
from typing import Dict

from langgraph.graph import Graph

from democracy_exe.ai.agents.router_agent import RouterAgent
from democracy_exe.ai.base import AgentNode, AgentState, BaseGraph, conditional_edge


class RouterGraph(BaseGraph):
    def __init__(self):
        super().__init__()
        self.router_agent = RouterAgent()
        self.specialized_agents: dict[str, Callable[[AgentState], AgentState]] = {}

    def add_specialized_agent(self, name: str, agent: Callable[[AgentState], AgentState]):
        self.specialized_agents[name] = agent

    def build(self) -> Graph:
        # Create a new graph
        self.graph = Graph()

        # Always add router node, even for empty graph
        self.graph.add_node("router", AgentNode(self.router_agent))
        self.graph.set_entry_point("router")

        # Add specialized agent nodes and edges
        for agent_name, agent_func in self.specialized_agents.items():
            self.graph.add_node(agent_name, agent_func)
            # Add bidirectional edges
            self.graph.add_edge(agent_name, "router")
            self.graph.add_edge("router", agent_name)

        return self.graph

    def process(self, state: dict) -> dict:
        """Process state through the graph.

        Args:
            state: Initial agent state

        Returns:
            Final state after processing
        """
        # Create base state
        base_state = {
            "query": "",
            "response": "",
            "current_agent": "",
            "context": {}
        }

        # Update with input state
        if isinstance(state, dict):
            base_state.update(state)

        # Build and compile graph
        graph = self.build()
        compiled = graph.compile()

        # Process state through graph
        return compiled.invoke(base_state)

router_graph = RouterGraph()

"""
This router_graph.py file defines the RouterGraph class, which is responsible for managing the overall flow of the AI system:
The RouterGraph class inherits from BaseGraph and implements the build method.
It has a RouterAgent instance and a dictionary of specialized agents.
The add_specialized_agent method allows adding new specialized agents to the graph.
The build method constructs the graph:
It adds the router node and all specialized agent nodes.
It creates conditional edges from the router to specialized agents.
It adds edges from specialized agents back to the router.
It sets the entry point of the graph to the router.
The process method compiles the graph and processes the given state.
An instance of RouterGraph is created at the module level for easy access.
This implementation allows for a flexible and extensible routing system. The router can dynamically decide which specialized agent to call based on the current state, and the flow can return to the router after each specialized agent processes the state.
"""

</document_content>
</document>
<document index="49">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/ai/graphs/social_media_graph.py</source>
<document_content>
from __future__ import annotations

from typing import Any, Dict, Literal, Optional, Union

from langgraph.graph import Graph

from democracy_exe.ai.agents.social_media_agent import SocialMediaAgent
from democracy_exe.ai.base import AgentNode, AgentState, BaseGraph


class SocialMediaGraph(BaseGraph):
    """Graph for orchestrating social media content processing operations.

    This graph manages the flow of social media operations through multiple stages:
    content fetching, media processing (screenshots/videos), and formatting.
    It coordinates these operations using a directed graph structure where each
    node represents a specific processing stage.

    Attributes:
        social_media_agent: Instance of SocialMediaAgent that performs the actual
            social media operations
    """

    def __init__(self) -> None:
        """Initialize the social media graph with its agent."""
        super().__init__()
        self.social_media_agent = SocialMediaAgent()

    def build(self) -> Graph:
        """Construct the social media processing workflow graph.

        Creates a directed graph with nodes for each processing stage and edges
        defining the flow between stages. Includes conditional edges for
        different types of media content.

        Returns:
            Configured LangGraph Graph instance ready for execution
        """
        # Add social media agent node
        self.graph.add_node("process_media", AgentNode(self.social_media_agent))

        # Add nodes for different processing stages
        self.graph.add_node("fetch_content", self.fetch_content)
        self.graph.add_node("process_video", self.process_video)
        self.graph.add_node("process_image", self.process_image)
        self.graph.add_node("format_output", self.format_output)

        # Add conditional edges based on content type
        self.graph.add_conditional_edges(
            "fetch_content",
            self.determine_content_type,
            {
                "video": "process_video",
                "image": "process_image"
            }
        )

        # Add remaining edges
        self.graph.add_edge("process_media", "fetch_content")
        self.graph.add_edge("process_video", "format_output")
        self.graph.add_edge("process_image", "format_output")
        self.graph.add_edge("format_output", "process_media")

        # Set the entry point
        self.graph.set_entry_point("process_media")

        return self.graph

    def process(self, state: AgentState) -> AgentState:
        """Process a social media content request through the workflow.

        Args:
            state: Current agent state containing the content URL or data

        Returns:
            Updated agent state with processing results
        """
        compiled_graph = self.compile()
        return compiled_graph(state)

    def fetch_content(self, state: AgentState) -> AgentState:
        """Fetch content data from the provided URL.

        Args:
            state: Agent state containing the content URL

        Returns:
            Updated state with fetched content data
        """
        content_data = self.social_media_agent.fetch_tweet(state["url"])
        state["content_data"] = content_data
        return state

    def determine_content_type(self, state: AgentState) -> Literal["video", "image"]:
        """Determine the type of content to be processed.

        Args:
            state: Agent state containing the content data

        Returns:
            String indicating content type ("video" or "image")
        """
        is_video = self.social_media_agent.is_video_tweet(state["content_data"])
        return "video" if is_video else "image"

    async def process_video(self, state: AgentState) -> AgentState:
        """Process video content from the social media post.

        Args:
            state: Agent state containing the video content data

        Returns:
            Updated state with processed video data
        """
        video_path = await self.social_media_agent.download_video(state["url"])
        state["processed_content"] = video_path
        return state

    async def process_image(self, state: AgentState) -> AgentState:
        """Process image content from the social media post.

        Args:
            state: Agent state containing the image content data

        Returns:
            Updated state with processed image data
        """
        screenshot = await self.social_media_agent.take_screenshot(state["url"])
        state["processed_content"] = screenshot
        return state

    def format_output(self, state: AgentState) -> AgentState:
        """Format the processed content for final output.

        Args:
            state: Agent state containing the processed content

        Returns:
            Updated state with formatted content in the response field
        """
        content_type = self.determine_content_type(state)
        if content_type == "video":
            state["response"] = f"Video processed and saved to: {state['processed_content']}"
        else:
            state["response"] = f"Image processed and saved with size: {len(state['processed_content'])} bytes"
        return state


social_media_graph = SocialMediaGraph()


"""
This social_media_graph.py file defines the SocialMediaGraph class, which encapsulates the functionality for handling social media tasks, particularly focused on Twitter:
The SocialMediaGraph class inherits from BaseGraph and implements the build method.
It has a SocialMediaAgent instance to handle the actual social media operations.
The build method constructs the graph:
It adds the main social media agent node.
It adds nodes for different social media tasks: fetching tweets, taking screenshots, identifying important regions, cropping images, and downloading videos.
It adds edges to create the task flow, allowing for processing of both image and video tweets.
It includes a conditional edge that directs the flow to video download if the tweet contains a video.
It sets the entry point of the graph to the main social media node.
The process method compiles the graph and processes the given state.
Each method (fetch_tweet, take_screenshot, etc.) corresponds to a different task in processing social media content. They update the state with the results of each task.
The is_video_tweet method acts as a decision point, determining whether to process the tweet as an image or video.
An instance of SocialMediaGraph is created at the module level for easy access.
This implementation provides a structured approach to handling social media tasks within the AI system. It can process tweets, take screenshots, identify important regions in images, crop images to specific dimensions, and download videos. The conditional edge allows for different processing paths depending on whether the tweet contains an image or a video.
This design allows for flexible handling of different types of social media content, with the ability to easily extend or modify specific tasks as needed.
"""

</document_content>
</document>
<document index="50">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/__init__.py</source>
<document_content>
# pyright: reportImportCycles=false
from __future__ import annotations

from democracy_exe.agentic.graph import memgraph


__all__ = ["memgraph"]

</document_content>
</document>
<document index="51">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/_constants.py</source>
<document_content>
from __future__ import annotations


PAYLOAD_KEY = "content"
PATH_KEY = "path"
PATCH_PATH = "user/{user_id}/core"
INSERT_PATH = "user/{user_id}/recall/{event_id}"
TIMESTAMP_KEY = "timestamp"
TYPE_KEY = "type"

</document_content>
</document>
<document index="52">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/_schemas.py</source>
<document_content>
from __future__ import annotations

from typing import Annotated, List

from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages  # type: ignore
from typing_extensions import TypedDict


class GraphConfig(TypedDict):
    model: str | None
    """The model to use for the memory assistant."""
    thread_id: str
    """The thread ID of the conversation."""
    user_id: str
    """The ID of the user to remember in the conversation."""


# Define the schema for the state maintained throughout the conversation
class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
    """The messages in the conversation."""
    core_memories: list[str]
    """The core memories associated with the user."""
    recall_memories: list[str]
    """The recall memories retrieved for the current context."""


__all__ = [
    "State",
    "GraphConfig",
]

</document_content>
</document>
<document index="53">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/_utils.py</source>
<document_content>
# pyright: reportUninitializedInstanceVariable=false
# pyright: reportUndefinedVariable=false
# pyright: reportAttributeAccessIssue=false

from __future__ import annotations

import time
import uuid

from functools import lru_cache
from typing import TYPE_CHECKING, Union

import langsmith
import structlog

from langchain_anthropic import ChatAnthropic
from langchain_core.runnables import RunnableConfig
from langchain_fireworks import FireworksEmbeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings


logger = structlog.get_logger(__name__)
from pinecone import Pinecone, ServerlessSpec

import democracy_exe.agentic._schemas as schemas

from democracy_exe.aio_settings import aiosettings


if TYPE_CHECKING:
    from langchain_anthropic import ChatAnthropic
    from langchain_openai import ChatOpenAI

    # Type alias for chat models
    ChatModelLike = Union[ChatOpenAI, ChatAnthropic]

_DEFAULT_DELAY = 60  # seconds


# def get_fake_user_id_to_uuid(user_id: int = 1) -> str:
#     namespace = uuid.NAMESPACE_DNS  # You can choose a different namespace if appropriate
#     name = f"USER:{user_id}"
#     generated_uuid = uuid.uuid5(namespace, name)
#     logger.info(f"Generated fake user ID: {generated_uuid}")
#     return generated_uuid

def get_fake_thread_id(user_id: int = 1) -> str:
    """Generate a deterministic UUID for a thread based on user ID.

    Args:
        user_id (int): The user ID to generate a thread ID for. Defaults to 1.

    Returns:
        str: A UUID v5 string generated from the user ID.

    Note:
        Uses UUID v5 which generates a deterministic UUID based on a namespace and name,
        ensuring the same user_id always generates the same thread_id.
    """
    # Use DNS namespace as a stable namespace for UUID generation
    # UUID v5 requires a namespace UUID and a name to generate a deterministic UUID
    namespace: uuid.UUID = uuid.NAMESPACE_DNS  # You can choose a different namespace if appropriate

    # Create a unique name string by prefixing the user_id with "USER:"
    # This helps avoid potential collisions with other UUID generation in the system
    name: str = f"USER:{user_id}"

    # Generate a UUID v5 using the namespace and name
    # UUID v5 uses SHA-1 hashing to create a deterministic UUID
    generated_uuid: uuid.UUID = uuid.uuid5(namespace, name)

    # Log the components and result for debugging purposes
    logger.info(f"namespace: {namespace}")
    logger.info(f"name: {name}")
    logger.info(f"Generated fake thread ID: {generated_uuid}")

    # Convert the UUID to a string and return it
    return str(generated_uuid)

def get_index() -> Pinecone.Index:
    """Get a Pinecone index instance using settings from aiosettings.

    Returns:
        Pinecone.Index: A configured Pinecone index instance.
    """
    pc: Pinecone = get_or_create_index()
    # pc: Pinecone = Pinecone(api_key=aiosettings.pinecone_api_key.get_secret_value()) # pylint: disable=no-member
    # pc: Pinecone = Pinecone(api_key=aiosettings.pinecone_api_key.get_secret_value(), environment=aiosettings.pinecone_env) # pylint: disable=no-member
    return pc



def get_or_create_index() -> Pinecone.Index:
    """Get or create a Pinecone index instance using settings from aiosettings.

    This function checks if the index exists, creates it if it doesn't, and returns
    the index instance. It waits for the index to be ready before returning.

    Returns:
        Pinecone.Index: A configured Pinecone index instance.

    Note:
        If the index doesn't exist, it will be created with dimension=3072 and
        metric="cosine" in the us-east-1 region.
    """
    pc: Pinecone = Pinecone(api_key=aiosettings.pinecone_api_key.get_secret_value()) # pylint: disable=no-member
    index_name: str = aiosettings.pinecone_index_name

    existing_indexes: list[str] = [index_info["name"] for index_info in pc.list_indexes()]

    logger.info(f"Existing indexes: {existing_indexes}")

    if index_name not in existing_indexes:
        logger.info(f"Creating index: {index_name} with dimension=3072 and metric=cosine in us-east-1")
        pc.create_index(
            name=index_name,
            dimension=3072,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1"),
        )
        while not pc.describe_index(index_name).status["ready"]:
            time.sleep(1)

    return pc.Index(index_name)



@langsmith.traceable  # Decorator to enable tracing of this function in LangSmith
def ensure_configurable(config: RunnableConfig) -> schemas.GraphConfig:
    """Merge the user-provided config with default values.

    Args:
        config (RunnableConfig): The configuration object containing user settings.

    Returns:
        schemas.GraphConfig: A merged configuration containing both user-provided and default values.

    Note:
        If chatbot_type is "terminal", it will generate a fake thread_id and user_id.
        Otherwise, it will use the provided discord configuration.
    """
    # Check if we're running in terminal mode vs discord mode
    if aiosettings.chatbot_type == "terminal":
        # For terminal mode, use a default user ID of 1
        user_id: int = 1
        # Generate a deterministic thread ID based on the user ID
        thread_id: str = get_fake_thread_id(user_id=user_id)

        # Create a configurable dict with the fake thread_id and user_id, or use existing config if provided
        configurable: dict[str, str | int] = config.get("configurable", {"thread_id": thread_id, "user_id": user_id})
        # Log the terminal configuration for debugging
        logger.info(f"Using terminal config: {configurable}")
    else:
        # For discord mode, get the configurable dict from the config, or use empty dict if not provided
        configurable: dict = config.get("configurable", {})
        # Log the discord configuration for debugging
        logger.info(f"Using discord config: {configurable}")

    # Return a merged dictionary containing:
    # 1. All key/values from the configurable dict
    # 2. A new GraphConfig with:
    #    - delay: use value from configurable or default to _DEFAULT_DELAY
    #    - model: use value from configurable or default to "gpt-4o"
    #    - thread_id: required value from configurable
    #    - user_id: required value from configurable
    return {
        **configurable,  # Spread all existing configurable key/values
        **schemas.GraphConfig(  # Create and spread a new GraphConfig with defaults
            delay=configurable.get("delay", _DEFAULT_DELAY),  # Get delay or use default
            model=configurable.get("model", "gpt-4o"),  # Get model or use default
            thread_id=configurable["thread_id"],  # Required field
            user_id=configurable["user_id"],  # Required field
        ),
    }


@lru_cache(maxsize=32)  # Cache the results of this function to avoid recreating embedding models unnecessarily
def get_embeddings(model_name: str = "nomic-ai/nomic-embed-text-v1.5") -> FireworksEmbeddings:
    """Get an embedding model instance based on the model name.

    Args:
        model_name (str): Name of the embedding model to use. Defaults to "nomic-ai/nomic-embed-text-v1.5".

    Returns:
        FireworksEmbeddings | OpenAIEmbeddings: An instance of the specified embedding model.

    Note:
        The function is cached using @lru_cache to avoid recreating the same model multiple times.
        Currently supports:
        - nomic-ai/nomic-embed-text-v1.5 (default, uses FireworksEmbeddings)
        - text-embedding-3-large (uses OpenAIEmbeddings)
        - Any other model name will use FireworksEmbeddings
    """
    # If using the default Nomic AI model
    if model_name == "nomic-ai/nomic-embed-text-v1.5":
        # Return a FireworksEmbeddings instance configured for the Nomic AI model
        return FireworksEmbeddings(model="nomic-ai/nomic-embed-text-v1.5")
    # If using OpenAI's text-embedding-3-large model
    elif model_name == "text-embedding-3-large":
        # Return an OpenAIEmbeddings instance configured for the text-embedding-3-large model
        return OpenAIEmbeddings(model="text-embedding-3-large")
    # For any other model name
    return FireworksEmbeddings(model=model_name)  # Use FireworksEmbeddings with the specified model name


@lru_cache(maxsize=32)
def get_chat_model(
    model_name: str,
    model_provider: str = "openai",
    temperature: float = 0.0
) -> ChatModelLike:
    """Get a chat model instance based on the model name and provider.

    Args:
        model_name (str): Name of the model to use
        model_provider (str): Provider of the model ('openai' or 'anthropic'). Defaults to 'openai'.
        temperature (float): Temperature for model generation. Defaults to 0.0.

    Returns:
        ChatModelLike: An instance of either ChatOpenAI or ChatAnthropic.

    Note:
        The function is cached using @lru_cache to avoid recreating the same model multiple times.
        Currently supports:
        - OpenAI models (using ChatOpenAI)
        - Anthropic models (using ChatAnthropic)
    """
    if model_provider == "anthropic":
        return ChatAnthropic(model=model_name, temperature=temperature)
    return ChatOpenAI(model=model_name, temperature=temperature)


__all__ = ["ensure_configurable", "get_embeddings", "get_chat_model"]

</document_content>
</document>
<document index="54">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/event_server.py</source>
<document_content>
# # NOTE: Borrow from this soon. https://github.com/langchain-ai/lang-memgpt/blob/main/event_server/main.py

# """Discord bot that integrates with LangGraph for AI-assisted conversations.

# This module sets up a Discord bot that can interact with users in Discord channels
# and threads. It uses LangGraph to process messages and generate responses.
# """

# import asyncio
# import logging
# import os
# import uuid

# import discord
# from aiohttp import web
# from discord.ext import commands
# from discord.message import Message
# from dotenv import load_dotenv
# from langchain_core.messages import HumanMessage
# from langgraph_sdk import get_client
# from langgraph_sdk.schema import Thread

# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger("discord")

# # Load environment variables
# load_dotenv()


# TOKEN = os.getenv("DISCORD_TOKEN")
# if not TOKEN:
#     raise ValueError(
#         "No Discord token found. Make sure DISCORD_TOKEN is set in your environment."
#     )

# INTENTS = discord.Intents.default()
# INTENTS.message_content = True
# BOT = commands.Bot(command_prefix="!", intents=INTENTS)
# _LANGGRAPH_CLIENT = get_client(url=os.environ["ASSISTANT_URL"])
# _ASSISTANT_ID = os.environ.get("ASSISTANT_ID")
# _GRAPH_ID = os.environ.get("GRAPH_ID", "memory")
# _LOCK = asyncio.Lock()


# @BOT.event
# async def on_ready():
#     """Log a message when the bot has successfully connected to Discord."""
#     logger.info(f"{BOT.user} has connected to Discord!")


# async def _get_assistant_id() -> str:
#     """Retrieve or set the assistant ID for the bot.

#     This function checks if an assistant ID is already set. If not, it fetches
#     the first available assistant from the LangGraph client and sets it as the
#     current assistant ID.

#     Returns:
#         str: The assistant ID to be used for processing messages.

#     Raises:
#         ValueError: If no assistant is found in the graph.
#     """
#     global _ASSISTANT_ID
#     if _ASSISTANT_ID is None:
#         async with _LOCK:
#             if _ASSISTANT_ID is None:
#                 assistants = await _LANGGRAPH_CLIENT.assistants.search(
#                     graph_id=_GRAPH_ID
#                 )
#                 if not assistants:
#                     raise ValueError("No assistant found in the graph.")
#                 _ASSISTANT_ID = assistants[0]["assistant_id"]
#                 logger.warning(f"Using assistant ID: {_ASSISTANT_ID}")
#     return _ASSISTANT_ID


# async def _get_thread(message: discord.Message) -> discord.Thread:
#     """Get or create a Discord thread for the given message.

#     If the message is already in a thread, return that thread.
#     Otherwise, create a new thread in the channel where the message was sent.

#     Args:
#         message (Message): The Discord message to get or create a thread for.

#     Returns:
#         discord.Thread: The thread associated with the message.
#     """
#     channel = message.channel
#     if isinstance(channel, discord.Thread):
#         return channel
#     else:
#         return await channel.create_thread(name="Response", message=message)


# async def _create_or_fetch_lg_thread(thread_id: uuid.UUID) -> Thread:
#     """Create or fetch a LangGraph thread for the given thread ID.

#     This function attempts to fetch an existing LangGraph thread. If it doesn't
#     exist, a new thread is created.

#     Args:
#         thread_id (uuid.UUID): The unique identifier for the thread.

#     Returns:
#         Thread: The LangGraph thread object.
#     """
#     try:
#         return await _LANGGRAPH_CLIENT.threads.get(thread_id)
#     except Exception:
#         pass
#     return await _LANGGRAPH_CLIENT.threads.create(thread_id=thread_id)


# def _format_inbound_message(message: Message) -> HumanMessage:
#     """Format a Discord message into a HumanMessage for LangGraph processing.

#     This function takes a Discord message and formats it into a structured
#     HumanMessage object that includes context about the message's origin.

#     Args:
#         message (Message): The Discord message to format.

#     Returns:
#         HumanMessage: A formatted message ready for LangGraph processing.
#     """
#     guild_str = "" if message.guild is None else f"guild={message.guild}"
#     content = f"""<discord {guild_str} channel={message.channel} author={repr(message.author)}>
#     {message.content}
#     </discord>"""
#     return HumanMessage(
#         content=content, name=str(message.author.global_name), id=str(message.id)
#     )


# @BOT.event
# async def on_message(message: Message):
#     """Event handler for incoming Discord messages.

#     This function processes incoming messages, ignoring those sent by the bot itself.
#     When the bot is mentioned, it creates or fetches the appropriate threads,
#     processes the message through LangGraph, and sends the response.

#     Args:
#         message (Message): The incoming Discord message.
#     """
#     if message.author == BOT.user:
#         return
#     if BOT.user.mentioned_in(message):
#         aid = await _get_assistant_id()
#         thread = await _get_thread(message)
#         lg_thread = await _create_or_fetch_lg_thread(
#             uuid.uuid5(uuid.NAMESPACE_DNS, f"DISCORD:{thread.id}")
#         )
#         thread_id = lg_thread["thread_id"]
#         user_id = message.author.id  # TODO: is this unique?
#         run_result = await _LANGGRAPH_CLIENT.runs.wait(
#             thread_id,
#             assistant_id=aid,
#             input={"messages": [_format_inbound_message(message)]},
#             config={
#                 "configurable": {
#                     "user_id": user_id,
#                     # "model": "accounts/fireworks/models/firefunction-v2"
#                 }
#             },
#         )
#         bot_message = run_result["messages"][-1]
#         response = bot_message["content"]
#         if isinstance(response, list):
#             response = "".join([r["text"] for r in response])
#         await thread.send(response)


# async def health_check(request):
#     """Health check endpoint for the web server.

#     This function responds to GET requests on the /health endpoint with an "OK" message.

#     Args:
#         request: The incoming web request.

#     Returns:
#         web.Response: A response indicating the service is healthy.
#     """
#     return web.Response(text="OK")


# async def run_bot():
#     """Run the Discord bot.

#     This function starts the Discord bot and handles any exceptions that occur during its operation.
#     """
#     try:
#         await BOT.start(TOKEN)
#     except Exception as e:
#         print(f"Error starting BOT: {e}")


# async def run_web_server():
#     """Run the web server for health checks.

#     This function sets up and starts a simple web server that includes a health check endpoint.
#     """
#     app = web.Application()
#     app.router.add_get("/health", health_check)
#     runner = web.AppRunner(app)
#     await runner.setup()
#     site = web.TCPSite(runner, "0.0.0.0", 8080)
#     await site.start()


# async def main():
#     """Main function to run both the Discord bot and the web server concurrently.

#     This function uses asyncio.gather to run both the bot and the web server in parallel.
#     """
#     await asyncio.gather(run_bot(), run_web_server())


# if __name__ == "__main__":
#     asyncio.run(main())

</document_content>
</document>
<document index="55">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/graph.py</source>
<document_content>
"""Lang-MemGPT: A Long-Term Memory Agent.

This module implements an agent with long-term memory capabilities using LangGraph.
The agent can store, retrieve, and use memories to enhance its interactions with users.

Key Components:
1. Memory Types: Core (always available) and Recall (contextual/semantic)
2. Tools: For saving and retrieving memories + performing other tasks.
3. Vector Database: for recall memory. Uses Pinecone by default.

Configuration: Requires Pinecone and Fireworks API keys (see README for setup)
"""
# pyright: reportUninitializedInstanceVariable=false
# pyright: reportAttributeAccessIssue=false
# pyright: reportInvalidTypeForm=false

from __future__ import annotations

import json
import logging
import uuid

from datetime import UTC, datetime, timezone
from typing import Literal, Optional, Tuple, Union

import langsmith
import rich
import structlog
import tiktoken

from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages.utils import get_buffer_string
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.config import RunnableConfig, ensure_config, get_executor_for_config
from langchain_core.tools import tool
from langchain_core.tools.base import BaseTool
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph  # type: ignore
from langgraph.prebuilt import ToolNode


logger = structlog.get_logger(__name__)

from democracy_exe.agentic import _constants as constants
from democracy_exe.agentic import _schemas as schemas
from democracy_exe.agentic import _utils as agentic_utils
from democracy_exe.aio_settings import aiosettings


# Type alias for chat models
ChatModelLike = Union[ChatOpenAI, ChatAnthropic]

_EMPTY_VEC = [0.0] * 768

# Initialize the search tool
search_tool = TavilySearchResults(max_results=1)
tools = [search_tool]


@tool
async def save_recall_memory(memory: str) -> str:
    """Save a memory to the database for later semantic retrieval.

    Args:
        memory (str): The memory to be saved.

    Returns:
        str: The saved memory.
    """
    config: RunnableConfig = ensure_config()
    configurable: schemas.GraphConfig = agentic_utils.ensure_configurable(config)
    embeddings = agentic_utils.get_embeddings(model_name=aiosettings.openai_embeddings_model)
    vector = await embeddings.aembed_query(memory)
    current_time = datetime.now(tz=UTC)
    path = constants.INSERT_PATH.format(
        user_id=configurable["user_id"],  # pyright: ignore[reportUndefinedVariable]
        event_id=str(uuid.uuid4()),
    )
    documents = [
        {
            "id": path,
            "values": vector,
            "metadata": {
                constants.PAYLOAD_KEY: memory,
                constants.PATH_KEY: path,
                constants.TIMESTAMP_KEY: current_time,
                constants.TYPE_KEY: "recall",
                "user_id": configurable["user_id"], # pyright: ignore[reportUndefinedVariable]
            },
        }
    ]
    logger.error(f"configurable: {configurable}")
    logger.error(f"documents: {documents}")
    logger.error(f"path: {path}")
    logger.error(f"current_time: {current_time}")
    logger.error(f"vector: {vector}")
    logger.error(f"embeddings: {embeddings}")

    agentic_utils.get_index().upsert(
        vectors=documents,
        namespace=aiosettings.pinecone_namespace,
    )
    # await logger.complete()
    return memory


@tool
def search_memory(query: str, top_k: int = 5) -> list[str]:
    """Search for memories in the database based on semantic similarity.

    Args:
        query (str): The search query.
        top_k (int): The number of results to return.

    Returns:
        list[str]: A list of relevant memories.
    """
    config: RunnableConfig = ensure_config()
    configurable: schemas.GraphConfig = agentic_utils.ensure_configurable(config)
    embeddings = agentic_utils.get_embeddings(model_name=aiosettings.openai_embeddings_model)
    vector = embeddings.embed_query(query)
    with langsmith.trace("query", inputs={"query": query, "top_k": top_k}) as rt:
        response = agentic_utils.get_index().query(
            vector=vector,
            filter={
                "user_id": {"$eq": configurable["user_id"]}, # pyright: ignore[reportUndefinedVariable]
                constants.TYPE_KEY: {"$eq": "recall"},
            },
            namespace=aiosettings.pinecone_namespace,
            include_metadata=True,
            top_k=top_k,
        )
        rt.end(outputs={"response": response})
    memories = []
    if matches := response.get("matches"):
        memories = [m["metadata"][constants.PAYLOAD_KEY] for m in matches] # pyright: ignore[reportUndefinedVariable]
    return memories


@langsmith.traceable
def fetch_core_memories(user_id: str) -> tuple[str, list[str]]:
    """Fetch core memories for a specific user.

    Args:
        user_id (str): The ID of the user.

    Returns:
        Tuple[str, list[str]]: The path and list of core memories.
    """
    path: str = constants.PATCH_PATH.format(user_id=user_id)
    logger.error(f"path: {path}")
    response = agentic_utils.get_index().fetch(
        ids=[path], namespace=aiosettings.pinecone_namespace
    )
    memories = []
    if vectors := response.get("vectors"):
        document = vectors[path]
        payload = document["metadata"][constants.PAYLOAD_KEY]
        memories = json.loads(payload)["memories"]
    return path, memories


@tool
def store_core_memory(memory: str, index: int | None = None) -> str:
    """Store a core memory in the database.

    Args:
        memory (str): The memory to store.
        index (Optional[int]): The index at which to store the memory.

    Returns:
        str: A confirmation message.
    """
    config: RunnableConfig = ensure_config()
    configurable: schemas.GraphConfig = agentic_utils.ensure_configurable(config)
    path, memories = fetch_core_memories(configurable["user_id"]) # pyright: ignore[reportUndefinedVariable]
    if index is not None:
        if index < 0 or index >= len(memories):
            return "Error: Index out of bounds."
        memories[index] = memory
    else:
        memories.insert(0, memory)
    documents = [
        {
            "id": path,
            "values": _EMPTY_VEC,
            "metadata": {
                constants.PAYLOAD_KEY: json.dumps({"memories": memories}),
                constants.PATH_KEY: path,
                constants.TIMESTAMP_KEY: datetime.now(tz=UTC),
                constants.TYPE_KEY: "recall",
                "user_id": configurable["user_id"], # pyright: ignore[reportUndefinedVariable]
            },
        }
    ]
    agentic_utils.get_index().upsert(
        vectors=documents,
        namespace=aiosettings.pinecone_namespace,
    )
    return "Memory stored."


# Combine all tools
all_tools: list[BaseTool | TavilySearchResults] = tools + [save_recall_memory, search_memory, store_core_memory]

# Define the prompt template for the agent
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant with advanced long-term memory"
            " capabilities. Powered by a stateless LLM, you must rely on"
            " external memory to store information between conversations."
            " Utilize the available memory tools to store and retrieve"
            " important details that will help you better attend to the user's"
            " needs and understand their context.\n\n"
            "Memory Usage Guidelines:\n"
            "1. Actively use memory tools (save_core_memory, save_recall_memory)"
            " to build a comprehensive understanding of the user.\n"
            "2. Make informed suppositions and extrapolations based on stored"
            " memories.\n"
            "3. Regularly reflect on past interactions to identify patterns and"
            " preferences.\n"
            "4. Update your mental model of the user with each new piece of"
            " information.\n"
            "5. Cross-reference new information with existing memories for"
            " consistency.\n"
            "6. Prioritize storing emotional context and personal values"
            " alongside facts.\n"
            "7. Use memory to anticipate needs and tailor responses to the"
            " user's style.\n"
            "8. Recognize and acknowledge changes in the user's situation or"
            " perspectives over time.\n"
            "9. Leverage memories to provide personalized examples and"
            " analogies.\n"
            "10. Recall past challenges or successes to inform current"
            " problem-solving.\n\n"
            "## Core Memories\n"
            "Core memories are fundamental to understanding the user and are"
            " always available:\n{core_memories}\n\n"
            "## Recall Memories\n"
            "Recall memories are contextually retrieved based on the current"
            " conversation:\n{recall_memories}\n\n"
            "## Instructions\n"
            "Engage with the user naturally, as a trusted colleague or friend."
            " There's no need to explicitly mention your memory capabilities."
            " Instead, seamlessly incorporate your understanding of the user"
            " into your responses. Be attentive to subtle cues and underlying"
            " emotions. Adapt your communication style to match the user's"
            " preferences and current emotional state. Use tools to persist"
            " information you want to retain in the next conversation. If you"
            " do call tools, all text preceding the tool call is an internal"
            " message. Respond AFTER calling the tool, once you have"
            " confirmation that the tool completed successfully.\n\n"
            "Current system time: {current_time}\n\n",
        ),
        ("placeholder", "{messages}"),
    ]
)


@langsmith.traceable
async def agent(
    state: schemas.State,
    config: RunnableConfig,
) -> schemas.State:
    """Process the current state and generate a response using the LLM.

    This function is a core component of the memory-enabled agent that:
    1. Retrieves the appropriate LLM model based on configuration
    2. Binds available tools to the model
    3. Formats core and recall memories for context
    4. Generates a response using the LLM with full context
    5. Logs key information for debugging

    Args:
        state: The current conversation state containing messages and memories.
               Expected keys: "messages", "core_memories", "recall_memories"
        config: Runtime configuration for the agent execution.
                Must contain "model" and "user_id" in its configurable dict.

    Returns:
        Updated state dictionary containing the LLM's response in the "messages" key.

    Raises:
        KeyError: If required state keys are missing
        ValueError: If model configuration is invalid
        RuntimeError: If LLM invocation fails
    """
    configurable: schemas.GraphConfig = agentic_utils.ensure_configurable(config)
    llm: ChatModelLike = agentic_utils.get_chat_model(
        model_name=configurable["model"],  # type: ignore
        model_provider=aiosettings.llm_provider,
        temperature=0.0
    )
    bound = prompt | llm.bind_tools(all_tools)
    core_str = (
        "<core_memory>\n" + "\n".join(state["core_memories"]) + "\n</core_memory>"
    )
    recall_str = (
        "<recall_memory>\n" + "\n".join(state["recall_memories"]) + "\n</recall_memory>"
    )
    logger.error(f"core_str: {core_str}")
    logger.error(f"recall_str: {recall_str}")
    logger.error(f"agent state: {state}")
    logger.error(f"agent config: {config}")
    prediction = await bound.ainvoke(
        {
            "messages": state["messages"],
            "core_memories": core_str,
            "recall_memories": recall_str,
            "current_time": datetime.now(tz=UTC).isoformat(),
        }
    )
    # await logger.complete()
    return {
        "messages": prediction,
    }


def load_memories(state: schemas.State, config: RunnableConfig) -> schemas.State:
    """Load core and recall memories for the current conversation.

    Args:
        state (schemas.State): The current state of the conversation.
        config (RunnableConfig): The runtime configuration for the agent.

    Returns:
        schemas.State: The updated state with loaded memories.
    """
    configurable: schemas.GraphConfig = agentic_utils.ensure_configurable(config)
    user_id: str | int = configurable["user_id"]
    tokenizer = tiktoken.encoding_for_model("gpt-4o")
    convo_str = get_buffer_string(state["messages"])
    convo_str = tokenizer.decode(tokenizer.encode(convo_str)[:2048])

    with get_executor_for_config(config) as executor:
        futures = [
            executor.submit(fetch_core_memories, user_id),
            executor.submit(search_memory.invoke, convo_str),
        ]
        _, core_memories = futures[0].result()
        recall_memories = futures[1].result()
    return {
        "core_memories": core_memories,
        "recall_memories": recall_memories,
    }


def route_tools(state: schemas.State) -> Literal["tools", "__end__"]:
    """Determine whether to use tools or end the conversation based on the last message.

    Args:
        state (schemas.State): The current state of the conversation.

    Returns:
        Literal["tools", "__end__"]: The next step in the graph.
    """
    logger.error(f"state: {state}")
    msg = state["messages"][-1]
    rich.inspect(msg, all=True)
    if msg.tool_calls:
        return "tools"
    return END


# Create the graph and add nodes
builder = StateGraph(schemas.State, schemas.GraphConfig)
builder.add_node(load_memories)
builder.add_node(agent)
builder.add_node("tools", ToolNode(all_tools))

# Add edges to the graph
builder.add_edge(START, "load_memories")
builder.add_edge("load_memories", "agent")
builder.add_conditional_edges("agent", route_tools)
builder.add_edge("tools", "agent")

# Compile the graph
memgraph: CompiledStateGraph = builder.compile(interrupt_before=["agent"])

__all__ = ["memgraph"]

</document_content>
</document>
<document index="56">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/_constants.py</source>
<document_content>
from __future__ import annotations


PAYLOAD_KEY = "content"
PATH_KEY = "path"
PATCH_PATH = "user/{user_id}/core"
INSERT_PATH = "user/{user_id}/recall/{event_id}"
TIMESTAMP_KEY = "timestamp"
TYPE_KEY = "type"

</document_content>
</document>
<document index="57">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/_schemas.py</source>
<document_content>
from __future__ import annotations

from typing import Annotated, List

from langchain_core.messages import AnyMessage
from langgraph.graph import add_messages
from typing_extensions import TypedDict


class GraphConfig(TypedDict):
    model: str | None
    """The model to use for the memory assistant."""
    thread_id: str
    """The thread ID of the conversation."""
    user_id: str
    """The ID of the user to remember in the conversation."""


# Define the schema for the state maintained throughout the conversation
class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
    """The messages in the conversation."""
    core_memories: list[str]
    """The core memories associated with the user."""
    recall_memories: list[str]
    """The recall memories retrieved for the current context."""


__all__ = [
    "State",
    "GraphConfig",
]

</document_content>
</document>
<document index="58">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/_utils.py</source>
<document_content>
# pyright: reportUninitializedInstanceVariable=false
# pyright: reportUndefinedVariable=false
# pyright: reportAttributeAccessIssue=false

from __future__ import annotations

import time
import uuid

from functools import lru_cache

import _schemas as schemas
import langsmith
import structlog

from langchain_core.runnables import RunnableConfig
from langchain_fireworks import FireworksEmbeddings
from langchain_openai import OpenAIEmbeddings


logger = structlog.get_logger(__name__)
from pinecone import Pinecone, ServerlessSpec
from settings import aiosettings


_DEFAULT_DELAY = 60  # seconds


# def get_fake_user_id_to_uuid(user_id: int = 1) -> str:
#     namespace = uuid.NAMESPACE_DNS  # You can choose a different namespace if appropriate
#     name = f"USER:{user_id}"
#     generated_uuid = uuid.uuid5(namespace, name)
#     logger.info(f"Generated fake user ID: {generated_uuid}")
#     return generated_uuid

def get_fake_thread_id(user_id: int = 1) -> str:
    """Generate a deterministic UUID for a thread based on user ID.

    Args:
        user_id (int): The user ID to generate a thread ID for. Defaults to 1.

    Returns:
        str: A UUID v5 string generated from the user ID.
    """
    namespace: uuid.UUID = uuid.NAMESPACE_DNS  # You can choose a different namespace if appropriate
    name: str = f"USER:{user_id}"
    generated_uuid: uuid.UUID = uuid.uuid5(namespace, name)
    logger.info(f"namespace: {namespace}")
    logger.info(f"name: {name}")
    logger.info(f"Generated fake thread ID: {generated_uuid}")
    return str(generated_uuid)

def get_index() -> Pinecone.Index:
    """Get a Pinecone index instance using settings from aiosettings.

    Returns:
        Pinecone.Index: A configured Pinecone index instance.
    """
    pc: Pinecone = get_or_create_index()
    # pc: Pinecone = Pinecone(api_key=aiosettings.pinecone_api_key.get_secret_value()) # pylint: disable=no-member
    # pc: Pinecone = Pinecone(api_key=aiosettings.pinecone_api_key.get_secret_value(), environment=aiosettings.pinecone_env) # pylint: disable=no-member
    return pc



def get_or_create_index() -> Pinecone.Index:
    """Get or create a Pinecone index instance using settings from aiosettings.

    This function checks if the index exists, creates it if it doesn't, and returns
    the index instance. It waits for the index to be ready before returning.

    Returns:
        Pinecone.Index: A configured Pinecone index instance.

    Note:
        If the index doesn't exist, it will be created with dimension=3072 and
        metric="cosine" in the us-east-1 region.
    """
    pc: Pinecone = Pinecone(api_key=aiosettings.pinecone_api_key.get_secret_value()) # pylint: disable=no-member
    index_name: str = aiosettings.pinecone_index_name

    existing_indexes: list[str] = [index_info["name"] for index_info in pc.list_indexes()]

    logger.info(f"Existing indexes: {existing_indexes}")

    if index_name not in existing_indexes:
        logger.info(f"Creating index: {index_name} with dimension=3072 and metric=cosine in us-east-1")
        pc.create_index(
            name=index_name,
            dimension=3072,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1"),
        )
        while not pc.describe_index(index_name).status["ready"]:
            time.sleep(1)

    return pc.Index(index_name)



@langsmith.traceable
def ensure_configurable(config: RunnableConfig) -> schemas.GraphConfig:
    """Merge the user-provided config with default values.

    Args:
        config (RunnableConfig): The configuration object containing user settings.

    Returns:
        schemas.GraphConfig: A merged configuration containing both user-provided and default values.

    Note:
        If chatbot_type is "terminal", it will generate a fake thread_id and user_id.
        Otherwise, it will use the provided discord configuration.
    """
    if aiosettings.chatbot_type == "terminal":
        user_id: int = 1
        thread_id: str = get_fake_thread_id(user_id=user_id)

        configurable: dict[str, str | int] = config.get("configurable", {"thread_id": thread_id, "user_id": user_id})
        logger.info(f"Using terminal config: {configurable}")
    else:
        configurable: dict = config.get("configurable", {})
        logger.info(f"Using discord config: {configurable}")

    return {
        **configurable,
        **schemas.GraphConfig(
            delay=configurable.get("delay", _DEFAULT_DELAY),
            model=configurable.get("model", "gpt-4o"),
            thread_id=configurable["thread_id"],
            user_id=configurable["user_id"],
        ),
    }


@lru_cache
def get_embeddings(model_name: str = "nomic-ai/nomic-embed-text-v1.5") -> FireworksEmbeddings:
    if model_name == "nomic-ai/nomic-embed-text-v1.5":
        return FireworksEmbeddings(model="nomic-ai/nomic-embed-text-v1.5")
    elif model_name == "text-embedding-3-large":
        return OpenAIEmbeddings(model="text-embedding-3-large")
    return FireworksEmbeddings(model=model_name)


__all__ = ["ensure_configurable"]

</document_content>
</document>
<document index="59">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/graph.py</source>
<document_content>
"""Lang-MemGPT: A Long-Term Memory Agent.

This module implements an agent with long-term memory capabilities using LangGraph.
The agent can store, retrieve, and use memories to enhance its interactions with users.

Key Components:
1. Memory Types: Core (always available) and Recall (contextual/semantic)
2. Tools: For saving and retrieving memories + performing other tasks.
3. Vector Database: for recall memory. Uses Pinecone by default.

Configuration: Requires Pinecone and Fireworks API keys (see README for setup)
"""
# pyright: reportUninitializedInstanceVariable=false
# pyright: reportAttributeAccessIssue=false
# pyright: reportInvalidTypeForm=false

from __future__ import annotations

import json
import logging
import uuid

from datetime import UTC, datetime, timezone
from typing import Literal, Optional, Tuple

import _constants as constants
import _schemas as schemas
import _utils as agentic_utils
import langsmith
import rich
import structlog
import tiktoken

from langchain.chat_models import init_chat_model
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages.utils import get_buffer_string
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.config import RunnableConfig, ensure_config, get_executor_for_config
from langchain_core.tools import tool
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt import ToolNode


logger = structlog.get_logger(__name__)
from settings import aiosettings


_EMPTY_VEC = [0.0] * 768

# Initialize the search tool
search_tool = TavilySearchResults(max_results=1)
tools = [search_tool]


@tool
async def save_recall_memory(memory: str) -> str:
    """Save a memory to the database for later semantic retrieval.

    Args:
        memory (str): The memory to be saved.

    Returns:
        str: The saved memory.
    """
    config = ensure_config()
    configurable = agentic_utils.ensure_configurable(config)
    embeddings = agentic_utils.get_embeddings(model_name=aiosettings.openai_embeddings_model)
    vector = await embeddings.aembed_query(memory)
    current_time = datetime.now(tz=UTC)
    path = constants.INSERT_PATH.format(
        user_id=configurable["user_id"],  # pyright: ignore[reportUndefinedVariable]
        event_id=str(uuid.uuid4()),
    )
    documents = [
        {
            "id": path,
            "values": vector,
            "metadata": {
                constants.PAYLOAD_KEY: memory,
                constants.PATH_KEY: path,
                constants.TIMESTAMP_KEY: current_time,
                constants.TYPE_KEY: "recall",
                "user_id": configurable["user_id"], # pyright: ignore[reportUndefinedVariable]
            },
        }
    ]
    logger.error(f"configurable: {configurable}")
    logger.error(f"documents: {documents}")
    logger.error(f"path: {path}")
    logger.error(f"current_time: {current_time}")
    logger.error(f"vector: {vector}")
    logger.error(f"embeddings: {embeddings}")

    agentic_utils.get_index().upsert(
        vectors=documents,
        namespace=aiosettings.pinecone_namespace,
    )
    await logger.complete()
    return memory


@tool
def search_memory(query: str, top_k: int = 5) -> list[str]:
    """Search for memories in the database based on semantic similarity.

    Args:
        query (str): The search query.
        top_k (int): The number of results to return.

    Returns:
        list[str]: A list of relevant memories.
    """
    config = ensure_config()
    configurable = agentic_utils.ensure_configurable(config)
    embeddings = agentic_utils.get_embeddings(model_name=aiosettings.openai_embeddings_model)
    vector = embeddings.embed_query(query)
    with langsmith.trace("query", inputs={"query": query, "top_k": top_k}) as rt:
        response = agentic_utils.get_index().query(
            vector=vector,
            filter={
                "user_id": {"$eq": configurable["user_id"]}, # pyright: ignore[reportUndefinedVariable]
                constants.TYPE_KEY: {"$eq": "recall"},
            },
            namespace=aiosettings.pinecone_namespace,
            include_metadata=True,
            top_k=top_k,
        )
        rt.end(outputs={"response": response})
    memories = []
    if matches := response.get("matches"):
        memories = [m["metadata"][constants.PAYLOAD_KEY] for m in matches] # pyright: ignore[reportUndefinedVariable]
    return memories


@langsmith.traceable
def fetch_core_memories(user_id: str) -> tuple[str, list[str]]:
    """Fetch core memories for a specific user.

    Args:
        user_id (str): The ID of the user.

    Returns:
        Tuple[str, list[str]]: The path and list of core memories.
    """
    path = constants.PATCH_PATH.format(user_id=user_id)
    logger.error(f"path: {path}")
    response = agentic_utils.get_index().fetch(
        ids=[path], namespace=aiosettings.pinecone_namespace
    )
    memories = []
    if vectors := response.get("vectors"):
        document = vectors[path]
        payload = document["metadata"][constants.PAYLOAD_KEY]
        memories = json.loads(payload)["memories"]
    return path, memories


@tool
def store_core_memory(memory: str, index: int | None = None) -> str:
    """Store a core memory in the database.

    Args:
        memory (str): The memory to store.
        index (Optional[int]): The index at which to store the memory.

    Returns:
        str: A confirmation message.
    """
    config = ensure_config()
    configurable = agentic_utils.ensure_configurable(config)
    path, memories = fetch_core_memories(configurable["user_id"]) # pyright: ignore[reportUndefinedVariable]
    if index is not None:
        if index < 0 or index >= len(memories):
            return "Error: Index out of bounds."
        memories[index] = memory
    else:
        memories.insert(0, memory)
    documents = [
        {
            "id": path,
            "values": _EMPTY_VEC,
            "metadata": {
                constants.PAYLOAD_KEY: json.dumps({"memories": memories}),
                constants.PATH_KEY: path,
                constants.TIMESTAMP_KEY: datetime.now(tz=UTC),
                constants.TYPE_KEY: "recall",
                "user_id": configurable["user_id"], # pyright: ignore[reportUndefinedVariable]
            },
        }
    ]
    agentic_utils.get_index().upsert(
        vectors=documents,
        namespace=aiosettings.pinecone_namespace,
    )
    return "Memory stored."


# Combine all tools
all_tools = tools + [save_recall_memory, search_memory, store_core_memory]

# Define the prompt template for the agent
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant with advanced long-term memory"
            " capabilities. Powered by a stateless LLM, you must rely on"
            " external memory to store information between conversations."
            " Utilize the available memory tools to store and retrieve"
            " important details that will help you better attend to the user's"
            " needs and understand their context.\n\n"
            "Memory Usage Guidelines:\n"
            "1. Actively use memory tools (save_core_memory, save_recall_memory)"
            " to build a comprehensive understanding of the user.\n"
            "2. Make informed suppositions and extrapolations based on stored"
            " memories.\n"
            "3. Regularly reflect on past interactions to identify patterns and"
            " preferences.\n"
            "4. Update your mental model of the user with each new piece of"
            " information.\n"
            "5. Cross-reference new information with existing memories for"
            " consistency.\n"
            "6. Prioritize storing emotional context and personal values"
            " alongside facts.\n"
            "7. Use memory to anticipate needs and tailor responses to the"
            " user's style.\n"
            "8. Recognize and acknowledge changes in the user's situation or"
            " perspectives over time.\n"
            "9. Leverage memories to provide personalized examples and"
            " analogies.\n"
            "10. Recall past challenges or successes to inform current"
            " problem-solving.\n\n"
            "## Core Memories\n"
            "Core memories are fundamental to understanding the user and are"
            " always available:\n{core_memories}\n\n"
            "## Recall Memories\n"
            "Recall memories are contextually retrieved based on the current"
            " conversation:\n{recall_memories}\n\n"
            "## Instructions\n"
            "Engage with the user naturally, as a trusted colleague or friend."
            " There's no need to explicitly mention your memory capabilities."
            " Instead, seamlessly incorporate your understanding of the user"
            " into your responses. Be attentive to subtle cues and underlying"
            " emotions. Adapt your communication style to match the user's"
            " preferences and current emotional state. Use tools to persist"
            " information you want to retain in the next conversation. If you"
            " do call tools, all text preceding the tool call is an internal"
            " message. Respond AFTER calling the tool, once you have"
            " confirmation that the tool completed successfully.\n\n"
            "Current system time: {current_time}\n\n",
        ),
        ("placeholder", "{messages}"),
    ]
)


async def agent(state: schemas.State, config: RunnableConfig) -> schemas.State:
    """Process the current state and generate a response using the LLM.

    Args:
        state (schemas.State): The current state of the conversation.
        config (RunnableConfig): The runtime configuration for the agent.

    Returns:
        schemas.State: The updated state with the agent's response.
    """
    configurable = agentic_utils.ensure_configurable(config)
    llm = init_chat_model(configurable["model"], model_provider=aiosettings.llm_provider, temperature=0.0) # pyright: ignore[reportUndefinedVariable]
    bound = prompt | llm.bind_tools(all_tools)
    core_str = (
        "<core_memory>\n" + "\n".join(state["core_memories"]) + "\n</core_memory>"
    )
    recall_str = (
        "<recall_memory>\n" + "\n".join(state["recall_memories"]) + "\n</recall_memory>"
    )
    logger.error(f"core_str: {core_str}")
    logger.error(f"recall_str: {recall_str}")
    logger.error(f"agent state: {state}")
    logger.error(f"agent config: {config}")
    prediction = await bound.ainvoke(
        {
            "messages": state["messages"],
            "core_memories": core_str,
            "recall_memories": recall_str,
            "current_time": datetime.now(tz=UTC).isoformat(),
        }
    )
    await logger.complete()
    return {
        "messages": prediction,
    }


def load_memories(state: schemas.State, config: RunnableConfig) -> schemas.State:
    """Load core and recall memories for the current conversation.

    Args:
        state (schemas.State): The current state of the conversation.
        config (RunnableConfig): The runtime configuration for the agent.

    Returns:
        schemas.State: The updated state with loaded memories.
    """
    configurable = agentic_utils.ensure_configurable(config)
    user_id = configurable["user_id"]
    tokenizer = tiktoken.encoding_for_model("gpt-4o")
    convo_str = get_buffer_string(state["messages"])
    convo_str = tokenizer.decode(tokenizer.encode(convo_str)[:2048])

    with get_executor_for_config(config) as executor:
        futures = [
            executor.submit(fetch_core_memories, user_id),
            executor.submit(search_memory.invoke, convo_str),
        ]
        _, core_memories = futures[0].result()
        recall_memories = futures[1].result()
    return {
        "core_memories": core_memories,
        "recall_memories": recall_memories,
    }


def route_tools(state: schemas.State) -> Literal["tools", "__end__"]:
    """Determine whether to use tools or end the conversation based on the last message.

    Args:
        state (schemas.State): The current state of the conversation.

    Returns:
        Literal["tools", "__end__"]: The next step in the graph.
    """
    logger.error(f"state: {state}")
    msg = state["messages"][-1]
    rich.inspect(msg, all=True)
    if msg.tool_calls:
        return "tools"
    return END


# Create the graph and add nodes
builder = StateGraph(schemas.State, schemas.GraphConfig)
builder.add_node(load_memories)
builder.add_node(agent)
builder.add_node("tools", ToolNode(all_tools))

# Add edges to the graph
builder.add_edge(START, "load_memories")
builder.add_edge("load_memories", "agent")
builder.add_conditional_edges("agent", route_tools)
builder.add_edge("tools", "agent")

# Compile the graph
memgraph: CompiledStateGraph = builder.compile(interrupt_before=["agent"])

__all__ = ["memgraph"]

</document_content>
</document>
<document index="60">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/langgraph.json</source>
<document_content>
{
    "dockerfile_lines": [],
    "graphs": {
      "agent": "./graph.py:memgraph"
    },
    "env": "./.env",
    "python_version": "3.11",
    "dependencies": [
      "."
    ]
  }

</document_content>
</document>
<document index="61">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/requirements.txt</source>
<document_content>
langgraph
langchain-core
langchain-community
langchain-openai
langchain-fireworks
pinecone-client
tiktoken
loguru
typing-extensions
python-dotenv
uuid
aiohttp
discord.py
rich
pydantic_settings
yarl
pinecone-text

</document_content>
</document>
<document index="62">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/settings.py</source>
<document_content>
"""copy of settings for use in Langgraph Studio"""

# pylint: disable=no-name-in-module
# pyright: reportInvalidTypeForm=false
# pyright: reportUndefinedVariable=false
from __future__ import annotations

import enum
import os
import pathlib

from collections.abc import Callable
from pathlib import Path
from tempfile import gettempdir
from typing import Annotated, Any, Dict, List, Literal, Optional, Set, Union, cast

import structlog


logger = structlog.get_logger(__name__)
from pydantic import (
    AliasChoices,
    AmqpDsn,
    BaseModel,
    Field,
    ImportString,
    Json,
    PostgresDsn,
    RedisDsn,
    SecretBytes,
    SecretStr,
    field_serializer,
    model_validator,
)
from pydantic_settings import BaseSettings, SettingsConfigDict
from rich.console import Console
from rich.table import Table
from typing_extensions import Self, TypedDict
from yarl import URL


TEMP_DIR = Path(gettempdir())
# SOURCE: https://github.com/taikinman/langrila/blob/main/src/langrila/openai/model_config.py
# SOURCE: https://github.com/taikinman/langrila/blob/main/src/langrila/openai/model_config.py
# TODO: Look at this https://github.com/h2oai/h2ogpt/blob/542543dc23aa9eb7d4ce7fe6b9af1204a047b50f/src/enums.py#L386 and see if we can add some more models
_TOKENS_PER_TILE = 170
_TILE_SIZE = 512

_OLDER_MODEL_CONFIG = {
    "gpt-4-0613": {
        "max_tokens": 8192,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00003,
        "completion_cost_per_token": 0.00006,
    },
    "gpt-4-32k-0314": {
        "max_tokens": 32768,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00006,
        "completion_cost_per_token": 0.00012,
    },
    "gpt-4-32k-0613": {
        "max_tokens": 32768,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00006,
        "completion_cost_per_token": 0.00012,
    },
    "gpt-3.5-turbo-0301": {
        "max_tokens": 4096,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.0000015,
        "completion_cost_per_token": 0.000002,
    },
    "gpt-3.5-turbo-0613": {
        "max_tokens": 4096,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.0000015,
        "completion_cost_per_token": 0.000002,
    },
    "gpt-3.5-turbo-16k-0613": {
        "max_tokens": 16384,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.000003,
        "completion_cost_per_token": 0.000004,
    },
    "gpt-3.5-turbo-instruct": {
        "max_tokens": 4096,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.0000015,
        "completion_cost_per_token": 0.000002,
    },
}


_NEWER_MODEL_CONFIG = {
    "claude-3-5-sonnet-20240620": {
        "max_tokens": 2048,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.0000025,
        "completion_cost_per_token": 0.00001,
    },
    "claude-3-opus-20240229": {
        "max_tokens": 2048,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.0000025,
        "completion_cost_per_token": 0.00001,
    },
    "claude-3-sonnet-20240229": {
        "max_tokens": 2048,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.0000025,
        "completion_cost_per_token": 0.00001,
    },
    "claude-3-haiku-20240307": {
        "max_tokens": 2048,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.0000025,
        "completion_cost_per_token": 0.00001,
    },
    "gpt-4o-2024-08-06": {
        "max_tokens": 128000,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.0000025,
        "completion_cost_per_token": 0.00001,
    },
    "gpt-4o-mini-2024-07-18": {
        # "max_tokens": 128000,
        "max_tokens": 900,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.000000150,
        "completion_cost_per_token": 0.00000060,
    },
    "gpt-4o-2024-05-13": {
        "max_tokens": 128000,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.000005,
        "completion_cost_per_token": 0.000015,
    },
    "gpt-4-turbo-2024-04-09": {
        "max_tokens": 128000,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00001,
        "completion_cost_per_token": 0.00003,
    },
    "gpt-4-0125-preview": {
        "max_tokens": 128000,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00001,
        "completion_cost_per_token": 0.00003,
    },
    "gpt-4-1106-preview": {
        "max_tokens": 128000,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00001,
        "completion_cost_per_token": 0.00003,
    },
    "gpt-4-vision-preview": {
        "max_tokens": 128000,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00001,
        "completion_cost_per_token": 0.00003,
    },
    "gpt-3.5-turbo-0125": {
        "max_tokens": 16384,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.0000005,
        "completion_cost_per_token": 0.0000015,
    },
    "gpt-3.5-turbo-1106": {
        "max_tokens": 16384,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.000001,
        "completion_cost_per_token": 0.000002,
    },
}

_NEWER_EMBEDDING_CONFIG = {
    "text-embedding-3-small": {
        "max_tokens": 8192,
        "prompt_cost_per_token": 0.00000002,
    },
    "text-embedding-3-large": {
        "max_tokens": 8192,
        "prompt_cost_per_token": 0.00000013,
    },
}

_OLDER_EMBEDDING_CONFIG = {
    "text-embedding-ada-002": {
        "max_tokens": 8192,
        "prompt_cost_per_token": 0.0000001,
    },
}


EMBEDDING_CONFIG = {}
EMBEDDING_CONFIG.update(_OLDER_EMBEDDING_CONFIG)
EMBEDDING_CONFIG.update(_NEWER_EMBEDDING_CONFIG)

MODEL_CONFIG = {}
MODEL_CONFIG.update(_OLDER_MODEL_CONFIG)
MODEL_CONFIG.update(_NEWER_MODEL_CONFIG)

MODEL_POINT = {
    "gpt-4o-mini": "gpt-4o-mini-2024-07-18",
    "gpt-4o": "gpt-4o-2024-08-06",
    "gpt-4-turbo": "gpt-4-turbo-2024-04-09",
    "gpt-4": "gpt-4-0613",
    "gpt-4-32k": "gpt-4-32k-0613",
    "gpt-4-vision": "gpt-4-vision-preview",
    "gpt-3.5-turbo": "gpt-3.5-turbo-0125",
    "gpt-3.5-turbo-16k": "gpt-3.5-turbo-16k-0613",
    "claude-3-opus": "claude-3-opus-20240229",
    "claude-3-sonnet": "claude-3-sonnet-20240229",
    "claude-3-haiku": "claude-3-haiku-20240307",
    "claude-3-5-sonnet": "claude-3-5-sonnet-20240620",
}

_MODEL_POINT_CONFIG = {
    "gpt-4o-mini": MODEL_CONFIG[MODEL_POINT["gpt-4o-mini"]],
    "gpt-4o": MODEL_CONFIG[MODEL_POINT["gpt-4o"]],
    "gpt-4-turbo": MODEL_CONFIG[MODEL_POINT["gpt-4-turbo"]],
    "gpt-4": MODEL_CONFIG[MODEL_POINT["gpt-4"]],
    "gpt-4-32k": MODEL_CONFIG[MODEL_POINT["gpt-4-32k"]],
    "gpt-4-vision": MODEL_CONFIG[MODEL_POINT["gpt-4-vision"]],
    "gpt-3.5-turbo": MODEL_CONFIG[MODEL_POINT["gpt-3.5-turbo"]],
    "gpt-3.5-turbo-16k": MODEL_CONFIG[MODEL_POINT["gpt-3.5-turbo-16k"]],
    "claude-3-opus": MODEL_CONFIG[MODEL_POINT["claude-3-opus"]],
    "claude-3-5-sonnet": MODEL_CONFIG[MODEL_POINT["claude-3-5-sonnet"]],
    "claude-3-sonnet": MODEL_CONFIG[MODEL_POINT["claude-3-sonnet"]],
    "claude-3-haiku": MODEL_CONFIG[MODEL_POINT["claude-3-haiku"]],
}

# contains all the models and embeddings info
MODEL_CONFIG.update(_MODEL_POINT_CONFIG)

# produces a list of all models and embeddings available
MODEL_ZOO = set(MODEL_CONFIG.keys()) | set(EMBEDDING_CONFIG.keys())

# SOURCE: https://github.com/JuliusHenke/autopentest/blob/ca822f723a356ec974d2dff332c2d92389a4c5e3/src/text_embeddings.py#L19
# https://platform.openai.com/docs/guides/embeddings/embedding-models
EMBEDDING_MODEL_DIMENSIONS_DATA = {
    "text-embedding-ada-002": 1536,
    "text-embedding-3-small": 512,
    "text-embedding-3-large": 1024,
}


# NOTE: DIRTY HACK TO GET AROUND CIRCULAR IMPORTS
# NOTE: There is a bug in pydantic that prevents us from using the `tilda` package and dealing with circular imports
def tilda(obj):
    """
    Wrapper for linux ~/ shell notation

    Args:
    ----
        obj (_type_): _description_

    Returns:
    -------
        _type_: _description_

    """
    if isinstance(obj, list):
        return [str(pathlib.Path(o).expanduser()) if isinstance(o, str) else o for o in obj]
    elif isinstance(obj, str):
        return str(pathlib.Path(obj).expanduser())
    else:
        return obj


def normalize_settings_path(file_path: str) -> str:
    """
    field_validator used to detect shell tilda notation and expand field automatically

    Args:
    ----
        file_path (str): _description_

    Returns:
    -------
        pathlib.PosixPath | str: _description_

    """
    # prevent circular import
    # from democracy_exe.utils import file_functions

    return tilda(file_path) if file_path.startswith("~") else file_path


def get_rich_console() -> Console:
    """
    _summary_

    Returns
    -------
        Console: _description_

    """
    return Console()


class LogLevel(str, enum.Enum):
    """Possible log levels."""

    NOTSET = "NOTSET"
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    FATAL = "FATAL"


class AioSettings(BaseSettings):
    """
    Application settings.

    These parameters can be configured
    with environment variables.
    """

    # By default, the environment variable name is the same as the field name.

    # You can change the prefix for all environment variables by setting the env_prefix config setting, or via the _env_prefix keyword argument on instantiation:

    # add a comment to each line in model_config explaining what it does
    model_config = SettingsConfigDict(
        env_prefix="DEMOCRACY_EXE_CONFIG_",
        env_file=(".env", ".envrc"),
        env_file_encoding="utf-8",
        extra="allow",
        arbitrary_types_allowed=True,
        json_schema_extra={
            "properties": {
                "llm_retriever_type": {
                    "type": "string",
                    "default": "vector_store",
                    "description": "Type of retriever to use",
                }
            }
        },
    )

    monitor_host: str = "localhost"
    monitor_port: int = 50102

    debug_langchain: bool | None = False

    # tweetpik_background_image = "510"  # a image that you want to use as background. you need to use this as a valid url like https://mysite.com/image.png and it should not be protected by cors
    audit_log_send_channel: str = ""

    # ***************************************************
    # NOTE: these are grouped together
    # ***************************************************
    # token: str = ""
    prefix: str = "?"
    discord_command_prefix: str = "?"

    discord_admin_user_id: int | None = None

    discord_general_channel: int = 908894727779258390

    discord_server_id: int = 0
    discord_client_id: int | str = 0

    discord_token: SecretStr = ""

    vector_store_type: Literal["pgvector", "chroma", "pinecone", "sklearn"] = "pgvector"

    # openai_token: str = ""
    openai_api_key: SecretStr = ""

    discord_admin_user_invited: bool = False

    debug: bool = True
    log_pii: bool = True

    personalization_file: str = Field(
        env="PERSONALIZATION_FILE",
        description="Path to the personalization JSON file",
        default="./personalization.json",
    )
    scratch_pad_dir: str = Field(
        env="SCRATCH_PAD_DIR",
        description="Directory for scratch pad files",
        default="./scratchpad",
    )
    active_memory_file: str = Field(
        env="ACTIVE_MEMORY_FILE",
        description="Path to the active memory JSON file",
        default="./active_memory.json",
    )

    changelogs_github_api_token: SecretStr = Field(
        env="CHANGELOGS_GITHUB_API_TOKEN", description="GitHub API token for Changelogs", default=""
    )
    firecrawl_api_key: SecretStr = Field(env="FIRECRAWL_API_KEY", description="Firecrawl API key", default="")

    # pylint: disable=redundant-keyword-arg
    better_exceptions: bool = Field(env="BETTER_EXCEPTIONS", description="Enable better exceptions", default=1)
    pythonasynciodebug: bool = Field(
        env="PYTHONASYNCIODEBUG", description="enable or disable asyncio debugging", default=0
    )
    pythondevmode: bool = Field(
        env="PYTHONDEVMODE",
        description="The Python Development Mode introduces additional runtime checks that are too expensive to be enabled by default. It should not be more verbose than the default if the code is correct; new warnings are only emitted when an issue is detected.",
        default=0,
    )
    langchain_debug_logs: bool = Field(
        env="LANGCHAIN_DEBUG_LOGS", description="enable or disable langchain debug logs", default=0
    )

    enable_ai: bool = False
    http_client_debug_enabled: bool = False

    localfilestore_root_path: str = Field(
        env="LOCALFILESTORE_ROOT_PATH", description="root path for local file store", default="./local_file_store"
    )

    # Try loading patchmatch
    globals_try_patchmatch: bool = True

    # Use CPU even if GPU is available (main use case is for debugging MPS issues)
    globals_always_use_cpu: bool = False

    # Whether the internet is reachable for dynamic downloads
    # The CLI will test connectivity at startup time.
    globals_internet_available: bool = True

    # whether we are forcing full precision
    globals_full_precision: bool = False

    # whether we should convert ckpt files into diffusers models on the fly
    globals_ckpt_convert: bool = False

    # logging tokenization everywhere
    globals_log_tokenization: bool = False

    bot_name: str = "SandboxAgentAI"

    # Variables for Redis
    redis_host: str = "localhost"
    redis_port: int = 8600
    redis_user: str | None = None
    redis_pass: SecretStr | None = None
    redis_base: int | None = None
    enable_redis: bool = False
    redis_url: URL | str | None = None

    sentry_dsn: SecretStr = ""
    enable_sentry: bool = False

    # Variables for ChromaDB


    chroma_host: str = "localhost"
    chroma_port: str = "9010"
    enable_chroma: bool = True

    dev_mode: bool = Field(env="DEV_MODE", description="enable dev mode", default=False)

    llm_temperature: float = 0.0

    vision_model: str = "gpt-4o"
    chat_model: str = "gpt-4o-mini"

    chat_history_buffer: int = 10

    retry_stop_after_attempt: int = 3
    retry_wait_exponential_multiplier: int | float = 2
    retry_wait_exponential_max: int | float = 5
    retry_wait_exponential_min: int | float = 1
    retry_wait_fixed: int | float = 15

    pinecone_api_key: SecretStr = Field(env="PINECONE_API_KEY", description="pinecone api key", default="")
    pinecone_env: str = Field(env="PINECONE_ENV", description="pinecone env", default="local")
    pinecone_index: str = Field(env="PINECONE_INDEX", description="pinecone index", default="")
    pinecone_namespace: str = Field(env="PINECONE_NAMESPACE", description="pinecone namespace", default="ns1")
    pinecone_index_name: str = Field(env="PINECONE_INDEX_NAME", description="pinecone index name", default="democracy-exe")
    pinecone_url: str = Field(env="PINECONE_URL", description="pinecone url", default="https://democracy-exe-dxt6ijd.svc.aped-4627-b74a.pinecone.io")

    chatbot_type: Literal["terminal", "discord"] = Field(env="CHATBOT_TYPE", description="chatbot type", default="terminal")

    unstructured_api_key: SecretStr = Field(env="UNSTRUCTURED_API_KEY", description="unstructured api key", default="")
    unstructured_api_url: str = Field(
        env="UNSTRUCTURED_API_URL",
        description="unstructured api url",
        default="https://api.unstructured.io/general/v0/general",
    )
    brave_search_api_key: SecretStr = Field(env="BRAVE_SEARCH_API_KEY", description="Brave Search API key", default="")

    anthropic_api_key: SecretStr = Field(env="ANTHROPIC_API_KEY", description="claude api key", default="")
    groq_api_key: SecretStr = Field(env="GROQ_API_KEY", description="groq api key", default="")
    cohere_api_key: SecretStr = Field(env="COHERE_API_KEY", description="cohere api key", default="")
    tavily_api_key: SecretStr = Field(env="TAVILY_API_KEY", description="Tavily API key", default="")
    brave_search_api_key: SecretStr = Field(env="BRAVE_SEARCH_API_KEY", description="Brave Search API key", default="")

    langchain_endpoint: str = Field(
        env="LANGCHAIN_ENDPOINT", description="langchain endpoint", default="https://api.smith.langchain.com"
    )
    langchain_tracing_v2: bool = Field(
        env="LANGCHAIN_TRACING_V2", description="langchain tracing version", default=False
    )
    langchain_api_key: SecretStr = Field(
        env="LANGCHAIN_API_KEY", description="langchain api key for langsmith", default=""
    )
    langchain_hub_api_url: str = Field(
        env="LANGCHAIN_HUB_API_URL",
        description="langchain hub api url for langsmith",
        default="https://api.hub.langchain.com",
    )
    langchain_hub_api_key: SecretStr = Field(
        env="LANGCHAIN_HUB_API_KEY", description="langchain hub api key for langsmith", default=""
    )
    langchain_project: str = Field(
        env="LANGCHAIN_PROJECT", description="langsmith project name", default="democracy_exe"
    )
    debug_aider: bool = Field(env="DEBUG_AIDER", description="debug tests stuff written by aider", default=False)

    local_test_debug: bool = Field(env="LOCAL_TEST_DEBUG", description="enable local debug testing", default=False)
    local_test_enable_evals: bool = Field(
        env="LOCAL_TEST_ENABLE_EVALS", description="enable local debug testing with evals", default=False
    )
    python_debug: bool = Field(env="PYTHON_DEBUG", description="enable bpdb on cli", default=False)
    experimental_redis_memory: bool = Field(
        env="EXPERIMENTAL_REDIS_MEMORY", description="enable experimental redis memory", default=False
    )

    oco_openai_api_key: SecretStr = Field(env="OCO_OPENAI_API_KEY", description="opencommit api key", default="")
    oco_tokens_max_input: int = Field(env="OCO_TOKENS_MAX_INPUT", description="OCO_TOKENS_MAX_INPUT", default=4096)
    oco_tokens_max_output: int = Field(env="OCO_TOKENS_MAX_OUTPUT", description="OCO_TOKENS_MAX_OUTPUT", default=500)
    oco_model: str = Field(env="OCO_MODEL", description="OCO_MODEL", default="gpt-4o")
    oco_language: str = Field(env="OCO_LANGUAGE", description="OCO_LANGUAGE", default="en")
    oco_prompt_module: str = Field(
        env="OCO_PROMPT_MODULE", description="OCO_PROMPT_MODULE", default="conventional-commit"
    )
    oco_ai_provider: str = Field(env="OCO_AI_PROVIDER", description="OCO_AI_PROVIDER", default="openai")

    openai_embeddings_model: str = Field(
        env="OPENAI_EMBEDDINGS_MODEL", description="openai embeddings model", default="text-embedding-3-large"
    )

    editor: str = Field(env="EDITOR", description="EDITOR", default="vim")
    visual: str = Field(env="VISUAL", description="VISUAL", default="vim")
    git_editor: str = Field(env="GIT_EDITOR", description="GIT_EDITOR", default="vim")

    llm_streaming: bool = Field(env="LLM_STREAMING", description="Enable streaming for LLM", default=False)
    llm_provider: str = Field(
        env="LLM_PROVIDER", description="LLM provider (e.g., openai, anthropic)", default="openai"
    )
    llm_max_retries: int = Field(
        env="LLM_MAX_RETRIES", description="Maximum number of retries for LLM API calls", default=3
    )
    llm_recursion_limit: int = Field(env="LLM_RECURSION_LIMIT", description="Recursion limit for LLM", default=50)
    llm_document_loader_type: Literal["pymupdf", "web", "directory"] = Field(
        env="LLM_DOCUMENT_LOADER_TYPE", description="Document loader type", default="pymupdf"
    )
    llm_text_splitter_type: Literal["recursive_text", "recursive_character"] = Field(
        env="LLM_TEXT_SPLITTER_TYPE", description="Text splitter type", default="recursive_text"
    )
    llm_vectorstore_type: Literal["pgvector", "chroma", "pinecone", "sklearn"] = Field(
        env="LLM_VECTORSTORE_TYPE", description="Vector store type", default="pgvector"
    )
    llm_embedding_model_type: Literal["text-embedding-3-large", "text-embedding-ada-002"] = Field(
        env="LLM_EMBEDDING_MODEL_TYPE", description="Embedding model type", default="text-embedding-3-large"
    )
    llm_key_value_stores_type: Literal["redis", "dynamodb"] = Field(
        env="LLM_KEY_VALUE_STORES_TYPE", description="Key-value stores type", default="redis"
    )
    # Variables for Postgres/pgvector
    pgvector_driver: str = Field(
        env="PGVECTOR_DRIVER",
        description="The database driver to use for pgvector (e.g., psycopg)",
        default="psycopg",
    )
    pgvector_host: str = Field(
        env="PGVECTOR_HOST",
        description="The hostname or IP address of the pgvector database server",
        default="localhost",
    )
    pgvector_port: int = Field(
        env="PGVECTOR_PORT",
        description="The port number of the pgvector database server",
        default=6432,
    )
    pgvector_database: str = Field(
        env="PGVECTOR_DATABASE",
        description="The name of the pgvector database",
        default="langchain",
    )
    pgvector_user: str = Field(
        env="PGVECTOR_USER",
        description="The username to connect to the pgvector database",
        default="langchain",
    )
    pgvector_password: SecretStr = Field(
        env="PGVECTOR_PASSWORD",
        description="The password to connect to the pgvector database",
        default="langchain",
    )
    pgvector_pool_size: int = Field(
        env="PGVECTOR_POOL_SIZE",
        description="The size of the connection pool for the pgvector database",
        default=10,
    )
    pgvector_dsn_uri: str = Field(
        env="PGVECTOR_DSN_URI",
        description="optional DSN URI, if set other pgvector_* settings are ignored",
        default="",
    )

    # Index - text splitter settings
    text_chunk_size: int = 2000
    text_chunk_overlap: int = 200
    text_splitter: Json[dict[str, Any]] = "{}"  # custom splitter settings

    # LLM settings
    qa_completion_llm: Json[dict[str, Any]] = """{
        "_type": "openai-chat",
        "model_name": "gpt-4o-mini",
        "temperature": 0,
        "max_tokens": 1000,
        "verbose": true
    }"""
    qa_followup_llm: Json[dict[str, Any]] = """{
        "_type": "openai-chat",
        "model_name": "gpt-4o-mini",
        "temperature": 0,
        "max_tokens": 200,
        "verbose": true
    }"""
    summarize_llm: Json[dict[str, Any]] = """{
        "_type": "openai-chat",
        "model_name": "gpt-4o",
        "temperature": 0,
        "max_tokens": 2000
    }"""


    postgres_host: str = "localhost"
    postgres_port: int = 8432
    postgres_password: str | None = "langchain"
    postgres_driver: str | None = "psycopg"
    postgres_database: str | None = "langchain"
    postgres_collection_name: str | None = "langchain"
    postgres_user: str | None = "langchain"
    enable_postgres: bool = True

    # QA
    qa_no_chat_history: bool = False  # don't load chat history
    qa_followup_sim_threshold: float = 0.735  # similitude threshold in followup
    qa_retriever: Json[dict[str, Any]] = "{}"  # custom retriever settings

    # Summarization
    summ_default_chain: str = "stuff"
    summ_token_splitter: int = 4000
    summ_token_overlap: int = 500

    sklearn_persist_path: str = Field(
        env="SKLEARN_PERSIST_PATH",
        description="Path to persist the SKLearn vector store",
        default="./db.db",
    )
    sklearn_serializer: Literal["json", "bson", "parquet"] = Field(
        env="SKLEARN_SERIALIZER",
        description="Serializer for the SKLearn vector store",
        default="json",
    )
    sklearn_metric: str = Field(
        env="SKLEARN_METRIC",
        description="Metric for the SKLearn vector store",
        default="cosine",
    )

    # Evaluation settings
    eval_max_concurrency: int = Field(
        env="EVAL_MAX_CONCURRENCY", description="Maximum number of concurrent evaluations", default=4
    )
    llm_model_name: str = Field(
        env="LLM_MODEL_NAME", description="Name of the LLM model to use", default="gpt-4o-mini", init=True
    )
    llm_json_model_name: str = Field(
        env="LLM_JSON_MODEL_NAME", description="Name of the LLM model to use", default="gpt-4o-mini", init=True
    )
    provider: str = Field(env="PROVIDER", description="AI provider (openai or anthropic)", default="openai")
    chunk_size: int = Field(env="CHUNK_SIZE", description="Size of each text chunk", default=1000)
    chunk_overlap: int = Field(env="CHUNK_OVERLAP", description="Overlap between text chunks", default=200)
    add_start_index: bool = Field(
        env="ADD_START_INDEX", description="Whether to add start index to text chunks", default=False
    )
    llm_embedding_model_name: str = Field(
        env="LLM_EMBEDDING_MODEL_NAME",
        description="Name of the embedding model to use",
        default="text-embedding-3-large",
    )
    llm_retriever_type: str = Field(
        env="LLM_RETRIEVER_TYPE",
        description="Type of retriever to use",
        default="vector_store",
    )
    default_search_kwargs: dict[str, int] = Field(
        env="DEFAULT_SEARCH_KWARGS",
        description="Default arguments for similarity search",
        default_factory=lambda: {"k": 2},
    )
    question_to_ask: str = Field(
        env="QUESTION_TO_ASK",
        description="Question to ask for evaluation",
        default="What is the main cause of climate change?",
    )
    dataset_name: str = Field(
        env="DATASET_NAME", description="Name of the dataset to use for evaluation", default="Climate Change Q&A"
    )

    # Model-specific settings
    max_tokens: int = Field(env="MAX_TOKENS", description="Maximum number of tokens for the model", default=900)
    max_retries: int = Field(env="MAX_RETRIES", description="Maximum number of retries for API calls", default=9)

    # # Evaluation feature flags
    compare_models_feature_flag: bool = Field(
        env="COMPARE_MODELS_FEATURE_FLAG", description="Enable comparing different models", default=False
    )
    rag_answer_v_reference_feature_flag: bool = Field(
        env="RAG_ANSWER_V_REFERENCE_FEATURE_FLAG", description="Enable comparing RAG answer to reference", default=False
    )
    helpfulness_feature_flag: bool = Field(
        env="HELPFULNESS_FEATURE_FLAG", description="Enable helpfulness evaluation", default=False
    )
    rag_answer_hallucination_feature_flag: bool = Field(
        env="RAG_ANSWER_HALLUCINATION_FEATURE_FLAG",
        description="Enable evaluating RAG answer hallucination",
        default=False,
    )
    rag_doc_relevance_feature_flag: bool = Field(
        env="RAG_DOC_RELEVANCE_FEATURE_FLAG", description="Enable evaluating RAG document relevance", default=False
    )
    rag_doc_relevance_and_hallucination_feature_flag: bool = Field(
        env="RAG_DOC_RELEVANCE_AND_HALLUCINATION_FEATURE_FLAG",
        description="Enable evaluating RAG document relevance and hallucination",
        default=False,
    )
    rag_answer_accuracy_feature_flag: bool = Field(
        env="RAG_ANSWER_ACCURACY_FEATURE_FLAG", description="Enable evaluating RAG answer accuracy", default=True
    )
    helpfulness_testing_feature_flag: bool = Field(
        env="HELPFULNESS_TESTING_FEATURE_FLAG", description="Enable helpfulness testing", default=False
    )
    rag_string_embedding_distance_metrics_feature_flag: bool = Field(
        env="RAG_STRING_EMBEDDING_DISTANCE_METRICS_FEATURE_FLAG",
        description="Enable evaluating RAG string embedding distance metrics",
        default=False,
    )

    llm_memory_type: str = Field(env="LLM_MEMORY_TYPE", description="Type of memory to use", default="memorysaver")
    llm_memory_enabled: bool = Field(env="LLM_MEMORY_ENABLED", description="Enable memory", default=True)
    llm_human_loop_enabled: bool = Field(env="LLM_HUMAN_LOOP_ENABLED", description="Enable human loop", default=False)
    # Tool allowlist
    tool_allowlist: list[str] = ["tavily_search", "magic_function"]

    # Tool-specific configuration
    tavily_search_max_results: int = 3

    agent_type: Literal["plan_and_execute", "basic", "advanced", "adaptive_rag"] = Field(
        env="AGENT_TYPE", description="Type of agent to use", default="adaptive_rag"
    )

    tweetpik_api_key: SecretStr = Field(env="TWEETPIK_API_KEY", description="TweetPik API key", default="")

    tweetpik_authorization: SecretStr = Field(env="TWEETPIK_AUTHORIZATION", description="TweetPik authorization", default="")
    tweetpik_bucket_id: str = Field(env="TWEETPIK_BUCKET_ID", description="TweetPik bucket ID", default="323251495115948625")
    # change the background color of the tweet screenshot
    tweetpik_background_color: str = "#ffffff"

    # Theme and dimension settings
    tweetpik_theme: str = Field(env="TWEETPIK_THEME", description="Theme for tweet screenshots", default="dim")
    tweetpik_dimension: str = Field(env="TWEETPIK_DIMENSION", description="Dimension for tweet screenshots", default="instagramFeed")

    # Color settings
    tweetpik_background_color: str = Field(env="TWEETPIK_BACKGROUND_COLOR", description="Background color for tweet screenshots", default="#15212b")
    tweetpik_text_primary_color: str = Field(env="TWEETPIK_TEXT_PRIMARY_COLOR", description="Primary text color", default="#FFFFFF")
    tweetpik_text_secondary_color: str = Field(env="TWEETPIK_TEXT_SECONDARY_COLOR", description="Secondary text color", default="#8899a6")
    tweetpik_link_color: str = Field(env="TWEETPIK_LINK_COLOR", description="Color for links and mentions", default="#1b95e0")
    tweetpik_verified_icon_color: str = Field(env="TWEETPIK_VERIFIED_ICON_COLOR", description="Color for verified badge", default="#1b95e0")

    # Display settings
    tweetpik_display_verified: str = Field(env="TWEETPIK_DISPLAY_VERIFIED", description="Show verified badge", default="default")
    tweetpik_display_metrics: bool = Field(env="TWEETPIK_DISPLAY_METRICS", description="Show metrics (likes, retweets)", default=False)
    tweetpik_display_embeds: bool = Field(env="TWEETPIK_DISPLAY_EMBEDS", description="Show embedded content", default=True)

    # Content settings
    tweetpik_content_scale: float = Field(env="TWEETPIK_CONTENT_SCALE", description="Scale factor for content", default=0.77)
    tweetpik_content_width: int = Field(env="TWEETPIK_CONTENT_WIDTH", description="Width of content in percentage", default=100)

    # any number higher than zero. this value is used in pixels(px) units
    tweetpik_canvas_width: str = "510"
    tweetpik_dimension_ig_feed: str = "1:1"
    tweetpik_dimension_ig_story: str = "9:16"
    tweetpik_display_likes: bool = False
    tweetpik_display_link_preview: bool = True
    tweetpik_display_media_images: bool = True
    tweetpik_display_replies: bool = False
    tweetpik_display_retweets: bool = False
    tweetpik_display_source: bool = True
    tweetpik_display_time: bool = True
    tweetpik_display_verified: bool = True

    # change the link colors used for the links, hashtags and mentions
    tweetpik_link_color: str = "#1b95e0"

    tweetpik_text_primary_color: str = (
        "#000000"  # change the text primary color used for the main text of the tweet and user's name
    )
    tweetpik_text_secondary_color: str = (
        "#5b7083"  # change the text secondary used for the secondary info of the tweet like the username
    )

    # any number higher than zero. this value is representing a percentage
    tweetpik_text_width: str = "100"

    tweetpik_timezone: str = "america/new_york"

    # change the verified icon color
    tweetpik_verified_icon: str = "#1b95e0"

    @model_validator(mode="before")
    @classmethod
    def pre_update(cls, values: dict[str, Any]) -> dict[str, Any]:
        llm_model_name = values.get("llm_model_name")
        llm_embedding_model_name = values.get("llm_embedding_model_name")
        logger.info(f"llm_model_name: {llm_model_name}")
        logger.info(f"llm_embedding_model_name: {llm_embedding_model_name}")
        if llm_model_name:
            values["max_tokens"] = MODEL_CONFIG[llm_model_name]["max_tokens"]
            values["max_output_tokens"] = MODEL_CONFIG[llm_model_name]["max_output_tokens"]
            values["prompt_cost_per_token"] = MODEL_CONFIG[llm_model_name]["prompt_cost_per_token"]
            values["completion_cost_per_token"] = MODEL_CONFIG[llm_model_name]["completion_cost_per_token"]
            if llm_embedding_model_name:
                values["embedding_max_tokens"] = EMBEDDING_MODEL_DIMENSIONS_DATA[llm_embedding_model_name]
                values["embedding_model_dimensions"] = EMBEDDING_MODEL_DIMENSIONS_DATA[llm_embedding_model_name]
        else:
            llm_model_name = "gpt-4o-mini"
            llm_embedding_model_name = "text-embedding-3-large"
            logger.info(f"setting default llm_model_name: {llm_model_name}")
            logger.info(f"setting default llm_embedding_model_name: {llm_embedding_model_name}")
            values["max_tokens"] = MODEL_CONFIG[llm_model_name]["max_tokens"]
            values["max_output_tokens"] = MODEL_CONFIG[llm_model_name]["max_output_tokens"]
            values["prompt_cost_per_token"] = MODEL_CONFIG[llm_model_name]["prompt_cost_per_token"]
            values["completion_cost_per_token"] = MODEL_CONFIG[llm_model_name]["completion_cost_per_token"]
            values["embedding_max_tokens"] = EMBEDDING_MODEL_DIMENSIONS_DATA[llm_embedding_model_name]
            values["embedding_model_dimensions"] = EMBEDDING_MODEL_DIMENSIONS_DATA[llm_embedding_model_name]

        return values

    @model_validator(mode="after")
    def post_root(self) -> Self:
        redis_path = f"/{self.redis_base}" if self.redis_base is not None else ""
        redis_pass = self.redis_pass if self.redis_pass is not None else None
        redis_user = self.redis_user if self.redis_user is not None else None
        logger.info(f"before redis_path: {redis_path}")
        logger.info(f"before redis_pass: {redis_pass}")
        logger.info(f"before redis_user: {redis_user}")
        if redis_pass is None and redis_user is None:
            self.redis_url = URL.build(
                scheme="redis",
                host=self.redis_host,
                port=self.redis_port,
                path=redis_path,
            )
        else:
            self.redis_url = URL.build(
                scheme="redis",
                host=self.redis_host,
                port=self.redis_port,
                path=redis_path,
                user=redis_user,
                password=redis_pass.get_secret_value(),
            )

        return self

    @property
    def postgres_url(self) -> URL:
        """
        Assemble postgres URL from settings.

        :return: postgres URL.
        """
        return f"postgresql+{self.postgres_driver}://{self.postgres_user}:{self.postgres_password}@{self.postgres_host}:{self.postgres_port}/{self.postgres_database}"

    # @property
    # def redis_url(self) -> URL:
    #     """
    #     Assemble REDIS URL from settings.

    #     :return: redis URL.
    #     """
    #     path = f"/{self.redis_base}" if self.redis_base is not None else ""
    #     return URL.build(
    #         scheme="redis",
    #         host=self.redis_host,
    #         port=self.redis_port,
    #         user=self.redis_user,
    #         password=self.redis_pass.get_secret_value(),
    #         path=path,
    #     )

    @field_serializer(
        "discord_token",
        "openai_api_key",
        "redis_pass",
        "pinecone_api_key",
        "langchain_api_key",
        "langchain_hub_api_key",
        when_used="json",
    )
    def dump_secret(self, v):
        return v.get_secret_value()


aiosettings = AioSettings()  # sourcery skip: docstrings-for-classes, avoid-global-variables

</document_content>
</document>
<document index="63">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/basic/_constants.py</source>
<document_content>
from __future__ import annotations


PAYLOAD_KEY = "content"
PATH_KEY = "path"
PATCH_PATH = "user/{user_id}/core"
INSERT_PATH = "user/{user_id}/recall/{event_id}"
TIMESTAMP_KEY = "timestamp"
TYPE_KEY = "type"

</document_content>
</document>
<document index="64">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/basic/_schemas.py</source>
<document_content>
from __future__ import annotations

from typing import Annotated, List

from langchain_core.messages import AnyMessage
from langgraph.graph import add_messages
from typing_extensions import TypedDict


class GraphConfig(TypedDict):
    model: str | None
    """The model to use for the memory assistant."""
    thread_id: str
    """The thread ID of the conversation."""
    user_id: str
    """The ID of the user to remember in the conversation."""


# Define the schema for the state maintained throughout the conversation
class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
    """The messages in the conversation."""
    core_memories: list[str]
    """The core memories associated with the user."""
    recall_memories: list[str]
    """The recall memories retrieved for the current context."""


__all__ = [
    "State",
    "GraphConfig",
]

</document_content>
</document>
<document index="65">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/basic/_utils.py</source>
<document_content>
# pyright: reportUninitializedInstanceVariable=false
# pyright: reportUndefinedVariable=false
# pyright: reportAttributeAccessIssue=false

from __future__ import annotations

import time
import uuid

from functools import lru_cache

import _schemas as schemas
import langsmith
import structlog

from langchain_core.runnables import RunnableConfig
from langchain_fireworks import FireworksEmbeddings
from langchain_openai import OpenAIEmbeddings


logger = structlog.get_logger(__name__)
from pinecone import Pinecone, ServerlessSpec
from settings import aiosettings


_DEFAULT_DELAY = 60  # seconds


# def get_fake_user_id_to_uuid(user_id: int = 1) -> str:
#     namespace = uuid.NAMESPACE_DNS  # You can choose a different namespace if appropriate
#     name = f"USER:{user_id}"
#     generated_uuid = uuid.uuid5(namespace, name)
#     logger.info(f"Generated fake user ID: {generated_uuid}")
#     return generated_uuid

def get_fake_thread_id(user_id: int = 1) -> str:
    """Generate a deterministic UUID for a thread based on user ID.

    Args:
        user_id (int): The user ID to generate a thread ID for. Defaults to 1.

    Returns:
        str: A UUID v5 string generated from the user ID.
    """
    namespace: uuid.UUID = uuid.NAMESPACE_DNS  # You can choose a different namespace if appropriate
    name: str = f"USER:{user_id}"
    generated_uuid: uuid.UUID = uuid.uuid5(namespace, name)
    logger.info(f"namespace: {namespace}")
    logger.info(f"name: {name}")
    logger.info(f"Generated fake thread ID: {generated_uuid}")
    return str(generated_uuid)

def get_index() -> Pinecone.Index:
    """Get a Pinecone index instance using settings from aiosettings.

    Returns:
        Pinecone.Index: A configured Pinecone index instance.
    """
    pc: Pinecone = get_or_create_index()
    # pc: Pinecone = Pinecone(api_key=aiosettings.pinecone_api_key.get_secret_value()) # pylint: disable=no-member
    # pc: Pinecone = Pinecone(api_key=aiosettings.pinecone_api_key.get_secret_value(), environment=aiosettings.pinecone_env) # pylint: disable=no-member
    return pc



def get_or_create_index() -> Pinecone.Index:
    """Get or create a Pinecone index instance using settings from aiosettings.

    This function checks if the index exists, creates it if it doesn't, and returns
    the index instance. It waits for the index to be ready before returning.

    Returns:
        Pinecone.Index: A configured Pinecone index instance.

    Note:
        If the index doesn't exist, it will be created with dimension=3072 and
        metric="cosine" in the us-east-1 region.
    """
    pc: Pinecone = Pinecone(api_key=aiosettings.pinecone_api_key.get_secret_value()) # pylint: disable=no-member
    index_name: str = aiosettings.pinecone_index_name

    existing_indexes: list[str] = [index_info["name"] for index_info in pc.list_indexes()]

    logger.info(f"Existing indexes: {existing_indexes}")

    if index_name not in existing_indexes:
        logger.info(f"Creating index: {index_name} with dimension=3072 and metric=cosine in us-east-1")
        pc.create_index(
            name=index_name,
            dimension=3072,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1"),
        )
        while not pc.describe_index(index_name).status["ready"]:
            time.sleep(1)

    return pc.Index(index_name)



@langsmith.traceable
def ensure_configurable(config: RunnableConfig) -> schemas.GraphConfig:
    """Merge the user-provided config with default values.

    Args:
        config (RunnableConfig): The configuration object containing user settings.

    Returns:
        schemas.GraphConfig: A merged configuration containing both user-provided and default values.

    Note:
        If chatbot_type is "terminal", it will generate a fake thread_id and user_id.
        Otherwise, it will use the provided discord configuration.
    """
    if aiosettings.chatbot_type == "terminal":
        user_id: int = 1
        thread_id: str = get_fake_thread_id(user_id=user_id)

        configurable: dict[str, str | int] = config.get("configurable", {"thread_id": thread_id, "user_id": user_id})
        logger.info(f"Using terminal config: {configurable}")
    else:
        configurable: dict = config.get("configurable", {})
        logger.info(f"Using discord config: {configurable}")

    return {
        **configurable,
        **schemas.GraphConfig(
            delay=configurable.get("delay", _DEFAULT_DELAY),
            model=configurable.get("model", "gpt-4o"),
            thread_id=configurable["thread_id"],
            user_id=configurable["user_id"],
        ),
    }


@lru_cache
def get_embeddings(model_name: str = "nomic-ai/nomic-embed-text-v1.5") -> FireworksEmbeddings:
    if model_name == "nomic-ai/nomic-embed-text-v1.5":
        return FireworksEmbeddings(model="nomic-ai/nomic-embed-text-v1.5")
    elif model_name == "text-embedding-3-large":
        return OpenAIEmbeddings(model="text-embedding-3-large")
    return FireworksEmbeddings(model=model_name)


__all__ = ["ensure_configurable"]

</document_content>
</document>
<document index="66">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/basic/graph.py</source>
<document_content>
"""Lang-MemGPT: A Long-Term Memory Agent.

This module implements an agent with long-term memory capabilities using LangGraph.
The agent can store, retrieve, and use memories to enhance its interactions with users.

Key Components:
1. Memory Types: Core (always available) and Recall (contextual/semantic)
2. Tools: For saving and retrieving memories + performing other tasks.
3. Vector Database: for recall memory. Uses Pinecone by default.

Configuration: Requires Pinecone and Fireworks API keys (see README for setup)
"""
# pyright: reportUninitializedInstanceVariable=false
# pyright: reportAttributeAccessIssue=false
# pyright: reportInvalidTypeForm=false

from __future__ import annotations

import json
import logging
import uuid

from datetime import UTC, datetime, timezone
from typing import Literal, Optional, Tuple

import _constants as constants
import _schemas as schemas
import _utils as agentic_utils
import langsmith
import rich
import structlog
import tiktoken

from langchain.chat_models import init_chat_model
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages.utils import get_buffer_string
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.config import RunnableConfig, ensure_config, get_executor_for_config
from langchain_core.tools import tool
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt import ToolNode


logger = structlog.get_logger(__name__)
from settings import aiosettings


_EMPTY_VEC = [0.0] * 768

# Initialize the search tool
search_tool = TavilySearchResults(max_results=1)
tools = [search_tool]


@tool
async def save_recall_memory(memory: str) -> str:
    """Save a memory to the database for later semantic retrieval.

    Args:
        memory (str): The memory to be saved.

    Returns:
        str: The saved memory.
    """
    config = ensure_config()
    configurable = agentic_utils.ensure_configurable(config)
    embeddings = agentic_utils.get_embeddings(model_name=aiosettings.openai_embeddings_model)
    vector = await embeddings.aembed_query(memory)
    current_time = datetime.now(tz=UTC)
    path = constants.INSERT_PATH.format(
        user_id=configurable["user_id"],  # pyright: ignore[reportUndefinedVariable]
        event_id=str(uuid.uuid4()),
    )
    documents = [
        {
            "id": path,
            "values": vector,
            "metadata": {
                constants.PAYLOAD_KEY: memory,
                constants.PATH_KEY: path,
                constants.TIMESTAMP_KEY: current_time,
                constants.TYPE_KEY: "recall",
                "user_id": configurable["user_id"], # pyright: ignore[reportUndefinedVariable]
            },
        }
    ]
    logger.error(f"configurable: {configurable}")
    logger.error(f"documents: {documents}")
    logger.error(f"path: {path}")
    logger.error(f"current_time: {current_time}")
    logger.error(f"vector: {vector}")
    logger.error(f"embeddings: {embeddings}")

    agentic_utils.get_index().upsert(
        vectors=documents,
        namespace=aiosettings.pinecone_namespace,
    )
    await logger.complete()
    return memory


@tool
def search_memory(query: str, top_k: int = 5) -> list[str]:
    """Search for memories in the database based on semantic similarity.

    Args:
        query (str): The search query.
        top_k (int): The number of results to return.

    Returns:
        list[str]: A list of relevant memories.
    """
    config = ensure_config()
    configurable = agentic_utils.ensure_configurable(config)
    embeddings = agentic_utils.get_embeddings(model_name=aiosettings.openai_embeddings_model)
    vector = embeddings.embed_query(query)
    with langsmith.trace("query", inputs={"query": query, "top_k": top_k}) as rt:
        response = agentic_utils.get_index().query(
            vector=vector,
            filter={
                "user_id": {"$eq": configurable["user_id"]}, # pyright: ignore[reportUndefinedVariable]
                constants.TYPE_KEY: {"$eq": "recall"},
            },
            namespace=aiosettings.pinecone_namespace,
            include_metadata=True,
            top_k=top_k,
        )
        rt.end(outputs={"response": response})
    memories = []
    if matches := response.get("matches"):
        memories = [m["metadata"][constants.PAYLOAD_KEY] for m in matches] # pyright: ignore[reportUndefinedVariable]
    return memories


@langsmith.traceable
def fetch_core_memories(user_id: str) -> tuple[str, list[str]]:
    """Fetch core memories for a specific user.

    Args:
        user_id (str): The ID of the user.

    Returns:
        Tuple[str, list[str]]: The path and list of core memories.
    """
    path = constants.PATCH_PATH.format(user_id=user_id)
    logger.error(f"path: {path}")
    response = agentic_utils.get_index().fetch(
        ids=[path], namespace=aiosettings.pinecone_namespace
    )
    memories = []
    if vectors := response.get("vectors"):
        document = vectors[path]
        payload = document["metadata"][constants.PAYLOAD_KEY]
        memories = json.loads(payload)["memories"]
    return path, memories


@tool
def store_core_memory(memory: str, index: int | None = None) -> str:
    """Store a core memory in the database.

    Args:
        memory (str): The memory to store.
        index (Optional[int]): The index at which to store the memory.

    Returns:
        str: A confirmation message.
    """
    config = ensure_config()
    configurable = agentic_utils.ensure_configurable(config)
    path, memories = fetch_core_memories(configurable["user_id"]) # pyright: ignore[reportUndefinedVariable]
    if index is not None:
        if index < 0 or index >= len(memories):
            return "Error: Index out of bounds."
        memories[index] = memory
    else:
        memories.insert(0, memory)
    documents = [
        {
            "id": path,
            "values": _EMPTY_VEC,
            "metadata": {
                constants.PAYLOAD_KEY: json.dumps({"memories": memories}),
                constants.PATH_KEY: path,
                constants.TIMESTAMP_KEY: datetime.now(tz=UTC),
                constants.TYPE_KEY: "recall",
                "user_id": configurable["user_id"], # pyright: ignore[reportUndefinedVariable]
            },
        }
    ]
    agentic_utils.get_index().upsert(
        vectors=documents,
        namespace=aiosettings.pinecone_namespace,
    )
    return "Memory stored."


# Combine all tools
all_tools = tools + [save_recall_memory, search_memory, store_core_memory]

# Define the prompt template for the agent
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant with advanced long-term memory"
            " capabilities. Powered by a stateless LLM, you must rely on"
            " external memory to store information between conversations."
            " Utilize the available memory tools to store and retrieve"
            " important details that will help you better attend to the user's"
            " needs and understand their context.\n\n"
            "Memory Usage Guidelines:\n"
            "1. Actively use memory tools (save_core_memory, save_recall_memory)"
            " to build a comprehensive understanding of the user.\n"
            "2. Make informed suppositions and extrapolations based on stored"
            " memories.\n"
            "3. Regularly reflect on past interactions to identify patterns and"
            " preferences.\n"
            "4. Update your mental model of the user with each new piece of"
            " information.\n"
            "5. Cross-reference new information with existing memories for"
            " consistency.\n"
            "6. Prioritize storing emotional context and personal values"
            " alongside facts.\n"
            "7. Use memory to anticipate needs and tailor responses to the"
            " user's style.\n"
            "8. Recognize and acknowledge changes in the user's situation or"
            " perspectives over time.\n"
            "9. Leverage memories to provide personalized examples and"
            " analogies.\n"
            "10. Recall past challenges or successes to inform current"
            " problem-solving.\n\n"
            "## Core Memories\n"
            "Core memories are fundamental to understanding the user and are"
            " always available:\n{core_memories}\n\n"
            "## Recall Memories\n"
            "Recall memories are contextually retrieved based on the current"
            " conversation:\n{recall_memories}\n\n"
            "## Instructions\n"
            "Engage with the user naturally, as a trusted colleague or friend."
            " There's no need to explicitly mention your memory capabilities."
            " Instead, seamlessly incorporate your understanding of the user"
            " into your responses. Be attentive to subtle cues and underlying"
            " emotions. Adapt your communication style to match the user's"
            " preferences and current emotional state. Use tools to persist"
            " information you want to retain in the next conversation. If you"
            " do call tools, all text preceding the tool call is an internal"
            " message. Respond AFTER calling the tool, once you have"
            " confirmation that the tool completed successfully.\n\n"
            "Current system time: {current_time}\n\n",
        ),
        ("placeholder", "{messages}"),
    ]
)


async def agent(state: schemas.State, config: RunnableConfig) -> schemas.State:
    """Process the current state and generate a response using the LLM.

    Args:
        state (schemas.State): The current state of the conversation.
        config (RunnableConfig): The runtime configuration for the agent.

    Returns:
        schemas.State: The updated state with the agent's response.
    """
    configurable = agentic_utils.ensure_configurable(config)
    llm = init_chat_model(configurable["model"], model_provider=aiosettings.llm_provider, temperature=0.0) # pyright: ignore[reportUndefinedVariable]
    bound = prompt | llm.bind_tools(all_tools)
    core_str = (
        "<core_memory>\n" + "\n".join(state["core_memories"]) + "\n</core_memory>"
    )
    recall_str = (
        "<recall_memory>\n" + "\n".join(state["recall_memories"]) + "\n</recall_memory>"
    )
    logger.error(f"core_str: {core_str}")
    logger.error(f"recall_str: {recall_str}")
    logger.error(f"agent state: {state}")
    logger.error(f"agent config: {config}")
    prediction = await bound.ainvoke(
        {
            "messages": state["messages"],
            "core_memories": core_str,
            "recall_memories": recall_str,
            "current_time": datetime.now(tz=UTC).isoformat(),
        }
    )
    await logger.complete()
    return {
        "messages": prediction,
    }


def load_memories(state: schemas.State, config: RunnableConfig) -> schemas.State:
    """Load core and recall memories for the current conversation.

    Args:
        state (schemas.State): The current state of the conversation.
        config (RunnableConfig): The runtime configuration for the agent.

    Returns:
        schemas.State: The updated state with loaded memories.
    """
    configurable = agentic_utils.ensure_configurable(config)
    user_id = configurable["user_id"]
    tokenizer = tiktoken.encoding_for_model("gpt-4o")
    convo_str = get_buffer_string(state["messages"])
    convo_str = tokenizer.decode(tokenizer.encode(convo_str)[:2048])

    with get_executor_for_config(config) as executor:
        futures = [
            executor.submit(fetch_core_memories, user_id),
            executor.submit(search_memory.invoke, convo_str),
        ]
        _, core_memories = futures[0].result()
        recall_memories = futures[1].result()
    return {
        "core_memories": core_memories,
        "recall_memories": recall_memories,
    }


def route_tools(state: schemas.State) -> Literal["tools", "__end__"]:
    """Determine whether to use tools or end the conversation based on the last message.

    Args:
        state (schemas.State): The current state of the conversation.

    Returns:
        Literal["tools", "__end__"]: The next step in the graph.
    """
    logger.error(f"state: {state}")
    msg = state["messages"][-1]
    rich.inspect(msg, all=True)
    if msg.tool_calls:
        return "tools"
    return END


# Create the graph and add nodes
builder = StateGraph(schemas.State, schemas.GraphConfig)
builder.add_node(load_memories)
builder.add_node(agent)
builder.add_node("tools", ToolNode(all_tools))

# Add edges to the graph
builder.add_edge(START, "load_memories")
builder.add_edge("load_memories", "agent")
builder.add_conditional_edges("agent", route_tools)
builder.add_edge("tools", "agent")

# Compile the graph
memgraph: CompiledStateGraph = builder.compile(interrupt_before=["agent"])

__all__ = ["memgraph"]

</document_content>
</document>
<document index="67">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/basic/langgraph.json</source>
<document_content>
{
    "dockerfile_lines": [],
    "graphs": {
      "agent": "./graph.py:memgraph"
    },
    "env": "./.env",
    "python_version": "3.11",
    "dependencies": [
      "."
    ]
  }

</document_content>
</document>
<document index="68">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/basic/requirements.txt</source>
<document_content>
langgraph
langchain-core
langchain-community
langchain-openai
langchain-fireworks
pinecone-client
tiktoken
loguru
typing-extensions
python-dotenv
uuid
aiohttp
discord.py
rich
pydantic_settings
yarl
pinecone-text

</document_content>
</document>
<document index="69">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/basic/settings.py</source>
<document_content>
"""copy of settings for use in Langgraph Studio"""

# pylint: disable=no-name-in-module
# pyright: reportInvalidTypeForm=false
# pyright: reportUndefinedVariable=false
from __future__ import annotations

import enum
import os
import pathlib

from collections.abc import Callable
from pathlib import Path
from tempfile import gettempdir
from typing import Annotated, Any, Dict, List, Literal, Optional, Set, Union, cast

import structlog


logger = structlog.get_logger(__name__)
from pydantic import (
    AliasChoices,
    AmqpDsn,
    BaseModel,
    Field,
    ImportString,
    Json,
    PostgresDsn,
    RedisDsn,
    SecretBytes,
    SecretStr,
    field_serializer,
    model_validator,
)
from pydantic_settings import BaseSettings, SettingsConfigDict
from rich.console import Console
from rich.table import Table
from typing_extensions import Self, TypedDict
from yarl import URL


TEMP_DIR = Path(gettempdir())
# SOURCE: https://github.com/taikinman/langrila/blob/main/src/langrila/openai/model_config.py
# SOURCE: https://github.com/taikinman/langrila/blob/main/src/langrila/openai/model_config.py
# TODO: Look at this https://github.com/h2oai/h2ogpt/blob/542543dc23aa9eb7d4ce7fe6b9af1204a047b50f/src/enums.py#L386 and see if we can add some more models
_TOKENS_PER_TILE = 170
_TILE_SIZE = 512

_OLDER_MODEL_CONFIG = {
    "gpt-4-0613": {
        "max_tokens": 8192,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00003,
        "completion_cost_per_token": 0.00006,
    },
    "gpt-4-32k-0314": {
        "max_tokens": 32768,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00006,
        "completion_cost_per_token": 0.00012,
    },
    "gpt-4-32k-0613": {
        "max_tokens": 32768,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00006,
        "completion_cost_per_token": 0.00012,
    },
    "gpt-3.5-turbo-0301": {
        "max_tokens": 4096,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.0000015,
        "completion_cost_per_token": 0.000002,
    },
    "gpt-3.5-turbo-0613": {
        "max_tokens": 4096,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.0000015,
        "completion_cost_per_token": 0.000002,
    },
    "gpt-3.5-turbo-16k-0613": {
        "max_tokens": 16384,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.000003,
        "completion_cost_per_token": 0.000004,
    },
    "gpt-3.5-turbo-instruct": {
        "max_tokens": 4096,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.0000015,
        "completion_cost_per_token": 0.000002,
    },
}


_NEWER_MODEL_CONFIG = {
    "claude-3-5-sonnet-20240620": {
        "max_tokens": 2048,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.0000025,
        "completion_cost_per_token": 0.00001,
    },
    "claude-3-opus-20240229": {
        "max_tokens": 2048,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.0000025,
        "completion_cost_per_token": 0.00001,
    },
    "claude-3-sonnet-20240229": {
        "max_tokens": 2048,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.0000025,
        "completion_cost_per_token": 0.00001,
    },
    "claude-3-haiku-20240307": {
        "max_tokens": 2048,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.0000025,
        "completion_cost_per_token": 0.00001,
    },
    "gpt-4o-2024-08-06": {
        "max_tokens": 128000,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.0000025,
        "completion_cost_per_token": 0.00001,
    },
    "gpt-4o-mini-2024-07-18": {
        # "max_tokens": 128000,
        "max_tokens": 900,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.000000150,
        "completion_cost_per_token": 0.00000060,
    },
    "gpt-4o-2024-05-13": {
        "max_tokens": 128000,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.000005,
        "completion_cost_per_token": 0.000015,
    },
    "gpt-4-turbo-2024-04-09": {
        "max_tokens": 128000,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00001,
        "completion_cost_per_token": 0.00003,
    },
    "gpt-4-0125-preview": {
        "max_tokens": 128000,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00001,
        "completion_cost_per_token": 0.00003,
    },
    "gpt-4-1106-preview": {
        "max_tokens": 128000,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00001,
        "completion_cost_per_token": 0.00003,
    },
    "gpt-4-vision-preview": {
        "max_tokens": 128000,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00001,
        "completion_cost_per_token": 0.00003,
    },
    "gpt-3.5-turbo-0125": {
        "max_tokens": 16384,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.0000005,
        "completion_cost_per_token": 0.0000015,
    },
    "gpt-3.5-turbo-1106": {
        "max_tokens": 16384,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.000001,
        "completion_cost_per_token": 0.000002,
    },
}

_NEWER_EMBEDDING_CONFIG = {
    "text-embedding-3-small": {
        "max_tokens": 8192,
        "prompt_cost_per_token": 0.00000002,
    },
    "text-embedding-3-large": {
        "max_tokens": 8192,
        "prompt_cost_per_token": 0.00000013,
    },
}

_OLDER_EMBEDDING_CONFIG = {
    "text-embedding-ada-002": {
        "max_tokens": 8192,
        "prompt_cost_per_token": 0.0000001,
    },
}


EMBEDDING_CONFIG = {}
EMBEDDING_CONFIG.update(_OLDER_EMBEDDING_CONFIG)
EMBEDDING_CONFIG.update(_NEWER_EMBEDDING_CONFIG)

MODEL_CONFIG = {}
MODEL_CONFIG.update(_OLDER_MODEL_CONFIG)
MODEL_CONFIG.update(_NEWER_MODEL_CONFIG)

MODEL_POINT = {
    "gpt-4o-mini": "gpt-4o-mini-2024-07-18",
    "gpt-4o": "gpt-4o-2024-08-06",
    "gpt-4-turbo": "gpt-4-turbo-2024-04-09",
    "gpt-4": "gpt-4-0613",
    "gpt-4-32k": "gpt-4-32k-0613",
    "gpt-4-vision": "gpt-4-vision-preview",
    "gpt-3.5-turbo": "gpt-3.5-turbo-0125",
    "gpt-3.5-turbo-16k": "gpt-3.5-turbo-16k-0613",
    "claude-3-opus": "claude-3-opus-20240229",
    "claude-3-sonnet": "claude-3-sonnet-20240229",
    "claude-3-haiku": "claude-3-haiku-20240307",
    "claude-3-5-sonnet": "claude-3-5-sonnet-20240620",
}

_MODEL_POINT_CONFIG = {
    "gpt-4o-mini": MODEL_CONFIG[MODEL_POINT["gpt-4o-mini"]],
    "gpt-4o": MODEL_CONFIG[MODEL_POINT["gpt-4o"]],
    "gpt-4-turbo": MODEL_CONFIG[MODEL_POINT["gpt-4-turbo"]],
    "gpt-4": MODEL_CONFIG[MODEL_POINT["gpt-4"]],
    "gpt-4-32k": MODEL_CONFIG[MODEL_POINT["gpt-4-32k"]],
    "gpt-4-vision": MODEL_CONFIG[MODEL_POINT["gpt-4-vision"]],
    "gpt-3.5-turbo": MODEL_CONFIG[MODEL_POINT["gpt-3.5-turbo"]],
    "gpt-3.5-turbo-16k": MODEL_CONFIG[MODEL_POINT["gpt-3.5-turbo-16k"]],
    "claude-3-opus": MODEL_CONFIG[MODEL_POINT["claude-3-opus"]],
    "claude-3-5-sonnet": MODEL_CONFIG[MODEL_POINT["claude-3-5-sonnet"]],
    "claude-3-sonnet": MODEL_CONFIG[MODEL_POINT["claude-3-sonnet"]],
    "claude-3-haiku": MODEL_CONFIG[MODEL_POINT["claude-3-haiku"]],
}

# contains all the models and embeddings info
MODEL_CONFIG.update(_MODEL_POINT_CONFIG)

# produces a list of all models and embeddings available
MODEL_ZOO = set(MODEL_CONFIG.keys()) | set(EMBEDDING_CONFIG.keys())

# SOURCE: https://github.com/JuliusHenke/autopentest/blob/ca822f723a356ec974d2dff332c2d92389a4c5e3/src/text_embeddings.py#L19
# https://platform.openai.com/docs/guides/embeddings/embedding-models
EMBEDDING_MODEL_DIMENSIONS_DATA = {
    "text-embedding-ada-002": 1536,
    "text-embedding-3-small": 512,
    "text-embedding-3-large": 1024,
}


# NOTE: DIRTY HACK TO GET AROUND CIRCULAR IMPORTS
# NOTE: There is a bug in pydantic that prevents us from using the `tilda` package and dealing with circular imports
def tilda(obj):
    """
    Wrapper for linux ~/ shell notation

    Args:
    ----
        obj (_type_): _description_

    Returns:
    -------
        _type_: _description_

    """
    if isinstance(obj, list):
        return [str(pathlib.Path(o).expanduser()) if isinstance(o, str) else o for o in obj]
    elif isinstance(obj, str):
        return str(pathlib.Path(obj).expanduser())
    else:
        return obj


def normalize_settings_path(file_path: str) -> str:
    """
    field_validator used to detect shell tilda notation and expand field automatically

    Args:
    ----
        file_path (str): _description_

    Returns:
    -------
        pathlib.PosixPath | str: _description_

    """
    # prevent circular import
    # from democracy_exe.utils import file_functions

    return tilda(file_path) if file_path.startswith("~") else file_path


def get_rich_console() -> Console:
    """
    _summary_

    Returns
    -------
        Console: _description_

    """
    return Console()


class LogLevel(str, enum.Enum):
    """Possible log levels."""

    NOTSET = "NOTSET"
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    FATAL = "FATAL"


class AioSettings(BaseSettings):
    """
    Application settings.

    These parameters can be configured
    with environment variables.
    """

    # By default, the environment variable name is the same as the field name.

    # You can change the prefix for all environment variables by setting the env_prefix config setting, or via the _env_prefix keyword argument on instantiation:

    # add a comment to each line in model_config explaining what it does
    model_config = SettingsConfigDict(
        env_prefix="DEMOCRACY_EXE_CONFIG_",
        env_file=(".env", ".envrc"),
        env_file_encoding="utf-8",
        extra="allow",
        arbitrary_types_allowed=True,
        json_schema_extra={
            "properties": {
                "llm_retriever_type": {
                    "type": "string",
                    "default": "vector_store",
                    "description": "Type of retriever to use",
                }
            }
        },
    )

    monitor_host: str = "localhost"
    monitor_port: int = 50102

    debug_langchain: bool | None = False

    # tweetpik_background_image = "510"  # a image that you want to use as background. you need to use this as a valid url like https://mysite.com/image.png and it should not be protected by cors
    audit_log_send_channel: str = ""

    # ***************************************************
    # NOTE: these are grouped together
    # ***************************************************
    # token: str = ""
    prefix: str = "?"
    discord_command_prefix: str = "?"

    discord_admin_user_id: int | None = None

    discord_general_channel: int = 908894727779258390

    discord_server_id: int = 0
    discord_client_id: int | str = 0

    discord_token: SecretStr = ""

    vector_store_type: Literal["pgvector", "chroma", "pinecone", "sklearn"] = "pgvector"

    # openai_token: str = ""
    openai_api_key: SecretStr = ""

    discord_admin_user_invited: bool = False

    debug: bool = True
    log_pii: bool = True

    personalization_file: str = Field(
        env="PERSONALIZATION_FILE",
        description="Path to the personalization JSON file",
        default="./personalization.json",
    )
    scratch_pad_dir: str = Field(
        env="SCRATCH_PAD_DIR",
        description="Directory for scratch pad files",
        default="./scratchpad",
    )
    active_memory_file: str = Field(
        env="ACTIVE_MEMORY_FILE",
        description="Path to the active memory JSON file",
        default="./active_memory.json",
    )

    changelogs_github_api_token: SecretStr = Field(
        env="CHANGELOGS_GITHUB_API_TOKEN", description="GitHub API token for Changelogs", default=""
    )
    firecrawl_api_key: SecretStr = Field(env="FIRECRAWL_API_KEY", description="Firecrawl API key", default="")

    # pylint: disable=redundant-keyword-arg
    better_exceptions: bool = Field(env="BETTER_EXCEPTIONS", description="Enable better exceptions", default=1)
    pythonasynciodebug: bool = Field(
        env="PYTHONASYNCIODEBUG", description="enable or disable asyncio debugging", default=0
    )
    pythondevmode: bool = Field(
        env="PYTHONDEVMODE",
        description="The Python Development Mode introduces additional runtime checks that are too expensive to be enabled by default. It should not be more verbose than the default if the code is correct; new warnings are only emitted when an issue is detected.",
        default=0,
    )
    langchain_debug_logs: bool = Field(
        env="LANGCHAIN_DEBUG_LOGS", description="enable or disable langchain debug logs", default=0
    )

    enable_ai: bool = False
    http_client_debug_enabled: bool = False

    localfilestore_root_path: str = Field(
        env="LOCALFILESTORE_ROOT_PATH", description="root path for local file store", default="./local_file_store"
    )

    # Try loading patchmatch
    globals_try_patchmatch: bool = True

    # Use CPU even if GPU is available (main use case is for debugging MPS issues)
    globals_always_use_cpu: bool = False

    # Whether the internet is reachable for dynamic downloads
    # The CLI will test connectivity at startup time.
    globals_internet_available: bool = True

    # whether we are forcing full precision
    globals_full_precision: bool = False

    # whether we should convert ckpt files into diffusers models on the fly
    globals_ckpt_convert: bool = False

    # logging tokenization everywhere
    globals_log_tokenization: bool = False

    bot_name: str = "SandboxAgentAI"

    # Variables for Redis
    redis_host: str = "localhost"
    redis_port: int = 8600
    redis_user: str | None = None
    redis_pass: SecretStr | None = None
    redis_base: int | None = None
    enable_redis: bool = False
    redis_url: URL | str | None = None

    sentry_dsn: SecretStr = ""
    enable_sentry: bool = False

    # Variables for ChromaDB


    chroma_host: str = "localhost"
    chroma_port: str = "9010"
    enable_chroma: bool = True

    dev_mode: bool = Field(env="DEV_MODE", description="enable dev mode", default=False)

    llm_temperature: float = 0.0

    vision_model: str = "gpt-4o"
    chat_model: str = "gpt-4o-mini"

    chat_history_buffer: int = 10

    retry_stop_after_attempt: int = 3
    retry_wait_exponential_multiplier: int | float = 2
    retry_wait_exponential_max: int | float = 5
    retry_wait_exponential_min: int | float = 1
    retry_wait_fixed: int | float = 15

    pinecone_api_key: SecretStr = Field(env="PINECONE_API_KEY", description="pinecone api key", default="")
    pinecone_env: str = Field(env="PINECONE_ENV", description="pinecone env", default="local")
    pinecone_index: str = Field(env="PINECONE_INDEX", description="pinecone index", default="")
    pinecone_namespace: str = Field(env="PINECONE_NAMESPACE", description="pinecone namespace", default="ns1")
    pinecone_index_name: str = Field(env="PINECONE_INDEX_NAME", description="pinecone index name", default="democracy-exe")
    pinecone_url: str = Field(env="PINECONE_URL", description="pinecone url", default="https://democracy-exe-dxt6ijd.svc.aped-4627-b74a.pinecone.io")

    chatbot_type: Literal["terminal", "discord"] = Field(env="CHATBOT_TYPE", description="chatbot type", default="terminal")

    unstructured_api_key: SecretStr = Field(env="UNSTRUCTURED_API_KEY", description="unstructured api key", default="")
    unstructured_api_url: str = Field(
        env="UNSTRUCTURED_API_URL",
        description="unstructured api url",
        default="https://api.unstructured.io/general/v0/general",
    )
    brave_search_api_key: SecretStr = Field(env="BRAVE_SEARCH_API_KEY", description="Brave Search API key", default="")

    anthropic_api_key: SecretStr = Field(env="ANTHROPIC_API_KEY", description="claude api key", default="")
    groq_api_key: SecretStr = Field(env="GROQ_API_KEY", description="groq api key", default="")
    cohere_api_key: SecretStr = Field(env="COHERE_API_KEY", description="cohere api key", default="")
    tavily_api_key: SecretStr = Field(env="TAVILY_API_KEY", description="Tavily API key", default="")
    brave_search_api_key: SecretStr = Field(env="BRAVE_SEARCH_API_KEY", description="Brave Search API key", default="")

    langchain_endpoint: str = Field(
        env="LANGCHAIN_ENDPOINT", description="langchain endpoint", default="https://api.smith.langchain.com"
    )
    langchain_tracing_v2: bool = Field(
        env="LANGCHAIN_TRACING_V2", description="langchain tracing version", default=False
    )
    langchain_api_key: SecretStr = Field(
        env="LANGCHAIN_API_KEY", description="langchain api key for langsmith", default=""
    )
    langchain_hub_api_url: str = Field(
        env="LANGCHAIN_HUB_API_URL",
        description="langchain hub api url for langsmith",
        default="https://api.hub.langchain.com",
    )
    langchain_hub_api_key: SecretStr = Field(
        env="LANGCHAIN_HUB_API_KEY", description="langchain hub api key for langsmith", default=""
    )
    langchain_project: str = Field(
        env="LANGCHAIN_PROJECT", description="langsmith project name", default="democracy_exe"
    )
    debug_aider: bool = Field(env="DEBUG_AIDER", description="debug tests stuff written by aider", default=False)

    local_test_debug: bool = Field(env="LOCAL_TEST_DEBUG", description="enable local debug testing", default=False)
    local_test_enable_evals: bool = Field(
        env="LOCAL_TEST_ENABLE_EVALS", description="enable local debug testing with evals", default=False
    )
    python_debug: bool = Field(env="PYTHON_DEBUG", description="enable bpdb on cli", default=False)
    experimental_redis_memory: bool = Field(
        env="EXPERIMENTAL_REDIS_MEMORY", description="enable experimental redis memory", default=False
    )

    oco_openai_api_key: SecretStr = Field(env="OCO_OPENAI_API_KEY", description="opencommit api key", default="")
    oco_tokens_max_input: int = Field(env="OCO_TOKENS_MAX_INPUT", description="OCO_TOKENS_MAX_INPUT", default=4096)
    oco_tokens_max_output: int = Field(env="OCO_TOKENS_MAX_OUTPUT", description="OCO_TOKENS_MAX_OUTPUT", default=500)
    oco_model: str = Field(env="OCO_MODEL", description="OCO_MODEL", default="gpt-4o")
    oco_language: str = Field(env="OCO_LANGUAGE", description="OCO_LANGUAGE", default="en")
    oco_prompt_module: str = Field(
        env="OCO_PROMPT_MODULE", description="OCO_PROMPT_MODULE", default="conventional-commit"
    )
    oco_ai_provider: str = Field(env="OCO_AI_PROVIDER", description="OCO_AI_PROVIDER", default="openai")

    openai_embeddings_model: str = Field(
        env="OPENAI_EMBEDDINGS_MODEL", description="openai embeddings model", default="text-embedding-3-large"
    )

    editor: str = Field(env="EDITOR", description="EDITOR", default="vim")
    visual: str = Field(env="VISUAL", description="VISUAL", default="vim")
    git_editor: str = Field(env="GIT_EDITOR", description="GIT_EDITOR", default="vim")

    llm_streaming: bool = Field(env="LLM_STREAMING", description="Enable streaming for LLM", default=False)
    llm_provider: str = Field(
        env="LLM_PROVIDER", description="LLM provider (e.g., openai, anthropic)", default="openai"
    )
    llm_max_retries: int = Field(
        env="LLM_MAX_RETRIES", description="Maximum number of retries for LLM API calls", default=3
    )
    llm_recursion_limit: int = Field(env="LLM_RECURSION_LIMIT", description="Recursion limit for LLM", default=50)
    llm_document_loader_type: Literal["pymupdf", "web", "directory"] = Field(
        env="LLM_DOCUMENT_LOADER_TYPE", description="Document loader type", default="pymupdf"
    )
    llm_text_splitter_type: Literal["recursive_text", "recursive_character"] = Field(
        env="LLM_TEXT_SPLITTER_TYPE", description="Text splitter type", default="recursive_text"
    )
    llm_vectorstore_type: Literal["pgvector", "chroma", "pinecone", "sklearn"] = Field(
        env="LLM_VECTORSTORE_TYPE", description="Vector store type", default="pgvector"
    )
    llm_embedding_model_type: Literal["text-embedding-3-large", "text-embedding-ada-002"] = Field(
        env="LLM_EMBEDDING_MODEL_TYPE", description="Embedding model type", default="text-embedding-3-large"
    )
    llm_key_value_stores_type: Literal["redis", "dynamodb"] = Field(
        env="LLM_KEY_VALUE_STORES_TYPE", description="Key-value stores type", default="redis"
    )
    # Variables for Postgres/pgvector
    pgvector_driver: str = Field(
        env="PGVECTOR_DRIVER",
        description="The database driver to use for pgvector (e.g., psycopg)",
        default="psycopg",
    )
    pgvector_host: str = Field(
        env="PGVECTOR_HOST",
        description="The hostname or IP address of the pgvector database server",
        default="localhost",
    )
    pgvector_port: int = Field(
        env="PGVECTOR_PORT",
        description="The port number of the pgvector database server",
        default=6432,
    )
    pgvector_database: str = Field(
        env="PGVECTOR_DATABASE",
        description="The name of the pgvector database",
        default="langchain",
    )
    pgvector_user: str = Field(
        env="PGVECTOR_USER",
        description="The username to connect to the pgvector database",
        default="langchain",
    )
    pgvector_password: SecretStr = Field(
        env="PGVECTOR_PASSWORD",
        description="The password to connect to the pgvector database",
        default="langchain",
    )
    pgvector_pool_size: int = Field(
        env="PGVECTOR_POOL_SIZE",
        description="The size of the connection pool for the pgvector database",
        default=10,
    )
    pgvector_dsn_uri: str = Field(
        env="PGVECTOR_DSN_URI",
        description="optional DSN URI, if set other pgvector_* settings are ignored",
        default="",
    )

    # Index - text splitter settings
    text_chunk_size: int = 2000
    text_chunk_overlap: int = 200
    text_splitter: Json[dict[str, Any]] = "{}"  # custom splitter settings

    # LLM settings
    qa_completion_llm: Json[dict[str, Any]] = """{
        "_type": "openai-chat",
        "model_name": "gpt-4o-mini",
        "temperature": 0,
        "max_tokens": 1000,
        "verbose": true
    }"""
    qa_followup_llm: Json[dict[str, Any]] = """{
        "_type": "openai-chat",
        "model_name": "gpt-4o-mini",
        "temperature": 0,
        "max_tokens": 200,
        "verbose": true
    }"""
    summarize_llm: Json[dict[str, Any]] = """{
        "_type": "openai-chat",
        "model_name": "gpt-4o",
        "temperature": 0,
        "max_tokens": 2000
    }"""


    postgres_host: str = "localhost"
    postgres_port: int = 8432
    postgres_password: str | None = "langchain"
    postgres_driver: str | None = "psycopg"
    postgres_database: str | None = "langchain"
    postgres_collection_name: str | None = "langchain"
    postgres_user: str | None = "langchain"
    enable_postgres: bool = True

    # QA
    qa_no_chat_history: bool = False  # don't load chat history
    qa_followup_sim_threshold: float = 0.735  # similitude threshold in followup
    qa_retriever: Json[dict[str, Any]] = "{}"  # custom retriever settings

    # Summarization
    summ_default_chain: str = "stuff"
    summ_token_splitter: int = 4000
    summ_token_overlap: int = 500

    sklearn_persist_path: str = Field(
        env="SKLEARN_PERSIST_PATH",
        description="Path to persist the SKLearn vector store",
        default="./db.db",
    )
    sklearn_serializer: Literal["json", "bson", "parquet"] = Field(
        env="SKLEARN_SERIALIZER",
        description="Serializer for the SKLearn vector store",
        default="json",
    )
    sklearn_metric: str = Field(
        env="SKLEARN_METRIC",
        description="Metric for the SKLearn vector store",
        default="cosine",
    )

    # Evaluation settings
    eval_max_concurrency: int = Field(
        env="EVAL_MAX_CONCURRENCY", description="Maximum number of concurrent evaluations", default=4
    )
    llm_model_name: str = Field(
        env="LLM_MODEL_NAME", description="Name of the LLM model to use", default="gpt-4o-mini", init=True
    )
    llm_json_model_name: str = Field(
        env="LLM_JSON_MODEL_NAME", description="Name of the LLM model to use", default="gpt-4o-mini", init=True
    )
    provider: str = Field(env="PROVIDER", description="AI provider (openai or anthropic)", default="openai")
    chunk_size: int = Field(env="CHUNK_SIZE", description="Size of each text chunk", default=1000)
    chunk_overlap: int = Field(env="CHUNK_OVERLAP", description="Overlap between text chunks", default=200)
    add_start_index: bool = Field(
        env="ADD_START_INDEX", description="Whether to add start index to text chunks", default=False
    )
    llm_embedding_model_name: str = Field(
        env="LLM_EMBEDDING_MODEL_NAME",
        description="Name of the embedding model to use",
        default="text-embedding-3-large",
    )
    llm_retriever_type: str = Field(
        env="LLM_RETRIEVER_TYPE",
        description="Type of retriever to use",
        default="vector_store",
    )
    default_search_kwargs: dict[str, int] = Field(
        env="DEFAULT_SEARCH_KWARGS",
        description="Default arguments for similarity search",
        default_factory=lambda: {"k": 2},
    )
    question_to_ask: str = Field(
        env="QUESTION_TO_ASK",
        description="Question to ask for evaluation",
        default="What is the main cause of climate change?",
    )
    dataset_name: str = Field(
        env="DATASET_NAME", description="Name of the dataset to use for evaluation", default="Climate Change Q&A"
    )

    # Model-specific settings
    max_tokens: int = Field(env="MAX_TOKENS", description="Maximum number of tokens for the model", default=900)
    max_retries: int = Field(env="MAX_RETRIES", description="Maximum number of retries for API calls", default=9)

    # # Evaluation feature flags
    compare_models_feature_flag: bool = Field(
        env="COMPARE_MODELS_FEATURE_FLAG", description="Enable comparing different models", default=False
    )
    rag_answer_v_reference_feature_flag: bool = Field(
        env="RAG_ANSWER_V_REFERENCE_FEATURE_FLAG", description="Enable comparing RAG answer to reference", default=False
    )
    helpfulness_feature_flag: bool = Field(
        env="HELPFULNESS_FEATURE_FLAG", description="Enable helpfulness evaluation", default=False
    )
    rag_answer_hallucination_feature_flag: bool = Field(
        env="RAG_ANSWER_HALLUCINATION_FEATURE_FLAG",
        description="Enable evaluating RAG answer hallucination",
        default=False,
    )
    rag_doc_relevance_feature_flag: bool = Field(
        env="RAG_DOC_RELEVANCE_FEATURE_FLAG", description="Enable evaluating RAG document relevance", default=False
    )
    rag_doc_relevance_and_hallucination_feature_flag: bool = Field(
        env="RAG_DOC_RELEVANCE_AND_HALLUCINATION_FEATURE_FLAG",
        description="Enable evaluating RAG document relevance and hallucination",
        default=False,
    )
    rag_answer_accuracy_feature_flag: bool = Field(
        env="RAG_ANSWER_ACCURACY_FEATURE_FLAG", description="Enable evaluating RAG answer accuracy", default=True
    )
    helpfulness_testing_feature_flag: bool = Field(
        env="HELPFULNESS_TESTING_FEATURE_FLAG", description="Enable helpfulness testing", default=False
    )
    rag_string_embedding_distance_metrics_feature_flag: bool = Field(
        env="RAG_STRING_EMBEDDING_DISTANCE_METRICS_FEATURE_FLAG",
        description="Enable evaluating RAG string embedding distance metrics",
        default=False,
    )

    llm_memory_type: str = Field(env="LLM_MEMORY_TYPE", description="Type of memory to use", default="memorysaver")
    llm_memory_enabled: bool = Field(env="LLM_MEMORY_ENABLED", description="Enable memory", default=True)
    llm_human_loop_enabled: bool = Field(env="LLM_HUMAN_LOOP_ENABLED", description="Enable human loop", default=False)
    # Tool allowlist
    tool_allowlist: list[str] = ["tavily_search", "magic_function"]

    # Tool-specific configuration
    tavily_search_max_results: int = 3

    agent_type: Literal["plan_and_execute", "basic", "advanced", "adaptive_rag"] = Field(
        env="AGENT_TYPE", description="Type of agent to use", default="adaptive_rag"
    )

    tweetpik_api_key: SecretStr = Field(env="TWEETPIK_API_KEY", description="TweetPik API key", default="")

    tweetpik_authorization: SecretStr = Field(env="TWEETPIK_AUTHORIZATION", description="TweetPik authorization", default="")
    tweetpik_bucket_id: str = Field(env="TWEETPIK_BUCKET_ID", description="TweetPik bucket ID", default="323251495115948625")
    # change the background color of the tweet screenshot
    tweetpik_background_color: str = "#ffffff"

    # Theme and dimension settings
    tweetpik_theme: str = Field(env="TWEETPIK_THEME", description="Theme for tweet screenshots", default="dim")
    tweetpik_dimension: str = Field(env="TWEETPIK_DIMENSION", description="Dimension for tweet screenshots", default="instagramFeed")

    # Color settings
    tweetpik_background_color: str = Field(env="TWEETPIK_BACKGROUND_COLOR", description="Background color for tweet screenshots", default="#15212b")
    tweetpik_text_primary_color: str = Field(env="TWEETPIK_TEXT_PRIMARY_COLOR", description="Primary text color", default="#FFFFFF")
    tweetpik_text_secondary_color: str = Field(env="TWEETPIK_TEXT_SECONDARY_COLOR", description="Secondary text color", default="#8899a6")
    tweetpik_link_color: str = Field(env="TWEETPIK_LINK_COLOR", description="Color for links and mentions", default="#1b95e0")
    tweetpik_verified_icon_color: str = Field(env="TWEETPIK_VERIFIED_ICON_COLOR", description="Color for verified badge", default="#1b95e0")

    # Display settings
    tweetpik_display_verified: str = Field(env="TWEETPIK_DISPLAY_VERIFIED", description="Show verified badge", default="default")
    tweetpik_display_metrics: bool = Field(env="TWEETPIK_DISPLAY_METRICS", description="Show metrics (likes, retweets)", default=False)
    tweetpik_display_embeds: bool = Field(env="TWEETPIK_DISPLAY_EMBEDS", description="Show embedded content", default=True)

    # Content settings
    tweetpik_content_scale: float = Field(env="TWEETPIK_CONTENT_SCALE", description="Scale factor for content", default=0.77)
    tweetpik_content_width: int = Field(env="TWEETPIK_CONTENT_WIDTH", description="Width of content in percentage", default=100)

    # any number higher than zero. this value is used in pixels(px) units
    tweetpik_canvas_width: str = "510"
    tweetpik_dimension_ig_feed: str = "1:1"
    tweetpik_dimension_ig_story: str = "9:16"
    tweetpik_display_likes: bool = False
    tweetpik_display_link_preview: bool = True
    tweetpik_display_media_images: bool = True
    tweetpik_display_replies: bool = False
    tweetpik_display_retweets: bool = False
    tweetpik_display_source: bool = True
    tweetpik_display_time: bool = True
    tweetpik_display_verified: bool = True

    # change the link colors used for the links, hashtags and mentions
    tweetpik_link_color: str = "#1b95e0"

    tweetpik_text_primary_color: str = (
        "#000000"  # change the text primary color used for the main text of the tweet and user's name
    )
    tweetpik_text_secondary_color: str = (
        "#5b7083"  # change the text secondary used for the secondary info of the tweet like the username
    )

    # any number higher than zero. this value is representing a percentage
    tweetpik_text_width: str = "100"

    tweetpik_timezone: str = "america/new_york"

    # change the verified icon color
    tweetpik_verified_icon: str = "#1b95e0"

    @model_validator(mode="before")
    @classmethod
    def pre_update(cls, values: dict[str, Any]) -> dict[str, Any]:
        llm_model_name = values.get("llm_model_name")
        llm_embedding_model_name = values.get("llm_embedding_model_name")
        logger.info(f"llm_model_name: {llm_model_name}")
        logger.info(f"llm_embedding_model_name: {llm_embedding_model_name}")
        if llm_model_name:
            values["max_tokens"] = MODEL_CONFIG[llm_model_name]["max_tokens"]
            values["max_output_tokens"] = MODEL_CONFIG[llm_model_name]["max_output_tokens"]
            values["prompt_cost_per_token"] = MODEL_CONFIG[llm_model_name]["prompt_cost_per_token"]
            values["completion_cost_per_token"] = MODEL_CONFIG[llm_model_name]["completion_cost_per_token"]
            if llm_embedding_model_name:
                values["embedding_max_tokens"] = EMBEDDING_MODEL_DIMENSIONS_DATA[llm_embedding_model_name]
                values["embedding_model_dimensions"] = EMBEDDING_MODEL_DIMENSIONS_DATA[llm_embedding_model_name]
        else:
            llm_model_name = "gpt-4o-mini"
            llm_embedding_model_name = "text-embedding-3-large"
            logger.info(f"setting default llm_model_name: {llm_model_name}")
            logger.info(f"setting default llm_embedding_model_name: {llm_embedding_model_name}")
            values["max_tokens"] = MODEL_CONFIG[llm_model_name]["max_tokens"]
            values["max_output_tokens"] = MODEL_CONFIG[llm_model_name]["max_output_tokens"]
            values["prompt_cost_per_token"] = MODEL_CONFIG[llm_model_name]["prompt_cost_per_token"]
            values["completion_cost_per_token"] = MODEL_CONFIG[llm_model_name]["completion_cost_per_token"]
            values["embedding_max_tokens"] = EMBEDDING_MODEL_DIMENSIONS_DATA[llm_embedding_model_name]
            values["embedding_model_dimensions"] = EMBEDDING_MODEL_DIMENSIONS_DATA[llm_embedding_model_name]

        return values

    @model_validator(mode="after")
    def post_root(self) -> Self:
        redis_path = f"/{self.redis_base}" if self.redis_base is not None else ""
        redis_pass = self.redis_pass if self.redis_pass is not None else None
        redis_user = self.redis_user if self.redis_user is not None else None
        logger.info(f"before redis_path: {redis_path}")
        logger.info(f"before redis_pass: {redis_pass}")
        logger.info(f"before redis_user: {redis_user}")
        if redis_pass is None and redis_user is None:
            self.redis_url = URL.build(
                scheme="redis",
                host=self.redis_host,
                port=self.redis_port,
                path=redis_path,
            )
        else:
            self.redis_url = URL.build(
                scheme="redis",
                host=self.redis_host,
                port=self.redis_port,
                path=redis_path,
                user=redis_user,
                password=redis_pass.get_secret_value(),
            )

        return self

    @property
    def postgres_url(self) -> URL:
        """
        Assemble postgres URL from settings.

        :return: postgres URL.
        """
        return f"postgresql+{self.postgres_driver}://{self.postgres_user}:{self.postgres_password}@{self.postgres_host}:{self.postgres_port}/{self.postgres_database}"

    # @property
    # def redis_url(self) -> URL:
    #     """
    #     Assemble REDIS URL from settings.

    #     :return: redis URL.
    #     """
    #     path = f"/{self.redis_base}" if self.redis_base is not None else ""
    #     return URL.build(
    #         scheme="redis",
    #         host=self.redis_host,
    #         port=self.redis_port,
    #         user=self.redis_user,
    #         password=self.redis_pass.get_secret_value(),
    #         path=path,
    #     )

    @field_serializer(
        "discord_token",
        "openai_api_key",
        "redis_pass",
        "pinecone_api_key",
        "langchain_api_key",
        "langchain_hub_api_key",
        when_used="json",
    )
    def dump_secret(self, v):
        return v.get_secret_value()


aiosettings = AioSettings()  # sourcery skip: docstrings-for-classes, avoid-global-variables

</document_content>
</document>
<document index="70">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/react/_constants.py</source>
<document_content>
from __future__ import annotations


PAYLOAD_KEY = "content"
PATH_KEY = "path"
PATCH_PATH = "user/{user_id}/core"
INSERT_PATH = "user/{user_id}/recall/{event_id}"
TIMESTAMP_KEY = "timestamp"
TYPE_KEY = "type"

</document_content>
</document>
<document index="71">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/react/_schemas.py</source>
<document_content>
from __future__ import annotations

from typing import Annotated, List

from langchain_core.messages import AnyMessage
from langgraph.graph import add_messages
from typing_extensions import TypedDict


# SOURCE: lang-memgpt
class GraphConfig(TypedDict):
    model: str | None
    """The model to use for the memory assistant."""
    thread_id: str
    """The thread ID of the conversation."""
    user_id: str
    """The ID of the user to remember in the conversation."""

# SOURCE: lang-memgpt
# Define the schema for the state maintained throughout the conversation
class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
    """The messages in the conversation."""
    core_memories: list[str]
    """The core memories associated with the user."""
    recall_memories: list[str]
    """The recall memories retrieved for the current context."""


__all__ = [
    "State",
    "GraphConfig",
]

</document_content>
</document>
<document index="72">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/react/_utils.py</source>
<document_content>
# pyright: reportUninitializedInstanceVariable=false
# pyright: reportUndefinedVariable=false
# pyright: reportAttributeAccessIssue=false

from __future__ import annotations

import time
import uuid

from functools import lru_cache

import _schemas as schemas
import langsmith
import structlog

from langchain.chat_models import init_chat_model
from langchain_core.embeddings import Embeddings
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage
from langchain_core.runnables import RunnableConfig


logger = structlog.get_logger(__name__)
from pinecone import Pinecone, ServerlessSpec
from settings import aiosettings


def get_message_text(msg: BaseMessage) -> str:
    """Get the text content of a message."""
    content = msg.content
    if isinstance(content, str):
        return content
    elif isinstance(content, dict):
        return content.get("text", "")
    else:
        txts = [c if isinstance(c, str) else (c.get("text") or "") for c in content]
        return "".join(txts).strip()


def load_chat_model(fully_specified_name: str) -> BaseChatModel:
    """Load a chat model from a fully specified name.

    Args:
        fully_specified_name (str): String in the format 'provider/model'.
    """
    provider, model = fully_specified_name.split("/", maxsplit=1)
    return init_chat_model(model, model_provider=provider)


_DEFAULT_DELAY = 60  # seconds


def get_fake_thread_id(user_id: int = 1) -> str:
    """Generate a deterministic UUID for a thread based on user ID.

    Args:
        user_id (int): The user ID to generate a thread ID for. Defaults to 1.

    Returns:
        str: A UUID v5 string generated from the user ID.
    """
    namespace: uuid.UUID = uuid.NAMESPACE_DNS  # You can choose a different namespace if appropriate
    name: str = f"USER:{user_id}"
    generated_uuid: uuid.UUID = uuid.uuid5(namespace, name)
    logger.info(f"namespace: {namespace}")
    logger.info(f"name: {name}")
    logger.info(f"Generated fake thread ID: {generated_uuid}")
    return str(generated_uuid)

def get_index() -> Pinecone.Index:
    """Get a Pinecone index instance using settings from aiosettings.

    Returns:
        Pinecone.Index: A configured Pinecone index instance.
    """
    pc: Pinecone = get_or_create_index()
    # pc: Pinecone = Pinecone(api_key=aiosettings.pinecone_api_key.get_secret_value()) # pylint: disable=no-member
    # pc: Pinecone = Pinecone(api_key=aiosettings.pinecone_api_key.get_secret_value(), environment=aiosettings.pinecone_env) # pylint: disable=no-member
    return pc



def get_or_create_index() -> Pinecone.Index:
    """Get or create a Pinecone index instance using settings from aiosettings.

    This function checks if the index exists, creates it if it doesn't, and returns
    the index instance. It waits for the index to be ready before returning.

    Returns:
        Pinecone.Index: A configured Pinecone index instance.

    Note:
        If the index doesn't exist, it will be created with dimension=3072 and
        metric="cosine" in the us-east-1 region.
    """
    pc: Pinecone = Pinecone(api_key=aiosettings.pinecone_api_key.get_secret_value()) # pylint: disable=no-member
    index_name: str = aiosettings.pinecone_index_name

    existing_indexes: list[str] = [index_info["name"] for index_info in pc.list_indexes()]

    logger.info(f"Existing indexes: {existing_indexes}")

    if index_name not in existing_indexes:
        logger.info(f"Creating index: {index_name} with dimension=3072 and metric=cosine in us-east-1")
        pc.create_index(
            name=index_name,
            dimension=3072,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1"),
        )
        while not pc.describe_index(index_name).status["ready"]:
            time.sleep(1)

    return pc.Index(index_name)



@langsmith.traceable
def ensure_configurable(config: RunnableConfig) -> schemas.GraphConfig:
    """Merge the user-provided config with default values.

    Args:
        config (RunnableConfig): The configuration object containing user settings.

    Returns:
        schemas.GraphConfig: A merged configuration containing both user-provided and default values.

    Note:
        If chatbot_type is "terminal", it will generate a fake thread_id and user_id.
        Otherwise, it will use the provided discord configuration.
    """
    if aiosettings.chatbot_type == "terminal":
        user_id: int = 1
        thread_id: str = get_fake_thread_id(user_id=user_id)

        configurable: dict[str, str | int] = config.get("configurable", {"thread_id": thread_id, "user_id": user_id})
        logger.info(f"Using terminal config: {configurable}")
    else:
        configurable: dict = config.get("configurable", {})
        logger.info(f"Using discord config: {configurable}")

    return {
        **configurable,
        **schemas.GraphConfig(
            delay=configurable.get("delay", _DEFAULT_DELAY),
            model=configurable.get("model", "gpt-4o"),
            thread_id=configurable["thread_id"],
            user_id=configurable["user_id"],
        ),
    }


@lru_cache
def get_embeddings(model_name: str = "nomic-ai/nomic-embed-text-v1.5") -> Embeddings|OpenAIEmbeddings:
    if model_name == "nomic-ai/nomic-embed-text-v1.5":
        from langchain_fireworks import FireworksEmbeddings
        return FireworksEmbeddings(model="nomic-ai/nomic-embed-text-v1.5")
    elif model_name == "text-embedding-3-large":
        from langchain_openai import OpenAIEmbeddings
        return OpenAIEmbeddings(model="text-embedding-3-large")
    return FireworksEmbeddings(model=model_name)


# NOTE: via memory-agent
def split_model_and_provider(fully_specified_name: str) -> dict:
    """Initialize the configured chat model."""
    if "/" in fully_specified_name:
        provider, model = fully_specified_name.split("/", maxsplit=1)
    else:
        provider = None
        model = fully_specified_name
    return {"model": model, "provider": provider}


def make_text_encoder(model: str) -> Embeddings:
    """Connect to the configured text encoder."""
    provider, model = model.split("/", maxsplit=1)
    match provider:
        case "openai":
            from langchain_openai import OpenAIEmbeddings

            return OpenAIEmbeddings(model=model)
        # case "cohere":
        #     from langchain_cohere import CohereEmbeddings

        #     return CohereEmbeddings(model=model)  # type: ignore
        case _:
            raise ValueError(f"Unsupported embedding provider: {provider}")

def make_chat_model(name: str) -> BaseChatModel:
    """Connect to the configured chat model."""
    provider, model = name.split("/", maxsplit=1)
    return init_chat_model(model, model_provider=provider, temperature=0.0) # pyright: ignore[reportUndefinedVariable]

__all__ = ["ensure_configurable"]

</document_content>
</document>
<document index="73">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/react/configuration.py</source>
<document_content>
from __future__ import annotations

import os

from dataclasses import dataclass, field, fields
from typing import Annotated, Any, Optional, Type, TypeVar

import prompts
import rich
import structlog

from langchain_core.runnables import RunnableConfig, ensure_config


logger = structlog.get_logger(__name__)
from rich.pretty import pprint


def _update_configurable_for_backwards_compatibility(
    configurable: dict[str, Any],
) -> dict[str, Any]:
    update = {}
    # if "k" in configurable:
    #     update["search_kwargs"] = {"k": configurable["k"]}

    if "model" in configurable:
        logger.error(f"Pre Configurable - model: {configurable['model']}")
        update["model"] = "openai/gpt-4o"
        logger.error(f"Post Configurable - model: {update['model']}")

    if "delay" in configurable:
        logger.error(f"Pre Configurable - delay: {configurable['delay']}")
        update["delay"] = 60
        logger.error(f"Post Configurable - delay: {update['delay']}")

    if "system_prompt" in configurable:
        logger.error(f"Pre Configurable - system_prompt: {configurable['system_prompt']}")
        update["system_prompt"] = prompts.MODEL_SYSTEM_MESSAGE
        logger.error(f"Post Configurable - system_prompt: {update['system_prompt']}")
    if update:
        return {**configurable, **update}

    return configurable

@dataclass(kw_only=True)
class Configuration:
    """The configurable fields for the chatbot."""
    user_id: str = "default-user"

    """The ID of the user to remember in the conversation."""
    model: Annotated[str, {"__template_metadata__": {"kind": "llm"}}] = field(
        # default="anthropic/claude-3-5-sonnet-20240620",
        default="openai/gpt-4o",
        metadata={
            "description": "The name of the language model to use for the agent. "
            "Should be in the form: provider/model-name."
        },
    )
    # for debounding memory creation
    delay: int = 60

    system_prompt: str = prompts.MODEL_SYSTEM_MESSAGE
    # trustcall_instruction: str = prompts.TRUSTCALL_INSTRUCTION

    @classmethod
    def from_runnable_config(
        cls: type[T], config: RunnableConfig | None | None = None
    ) -> Configuration:
        """Create a Configuration instance from a RunnableConfig."""
        config = ensure_config(config)
        configurable = config.get("configurable") or {}
        # logger.error(f"Config - config: {config}")
        # configurable = (
        #     config["configurable"] if config and "configurable" in config else {}
        # )
        configurable = _update_configurable_for_backwards_compatibility(configurable)
        rich.print("configurable:")
        pprint(configurable)
        # logger.error(f"Configurable - configurable after _update_configurable_for_backwards_compatibility: {configurable}")
        values: dict[str, Any] = {
            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))
            for f in fields(cls)
            if f.init
        }
        # logger.error(f"Values - values: {values}")
        return cls(**{k: v for k, v in values.items() if v})

T = TypeVar("T", bound=Configuration)

if __name__ == "__main__":  # pragma: no cover
    import rich
    config = {}
    configurable = Configuration.from_runnable_config(config)
    rich.print(f"configurable: {configurable}")

</document_content>
</document>
<document index="74">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/react/langgraph.json</source>
<document_content>
{
    "dockerfile_lines": [],
    "graphs": {
      "agent": "./memory_agent.py:graph"
    },
    "env": "./.env",
    "python_version": "3.11",
    "dependencies": [
      "."
    ]
  }

</document_content>
</document>
<document index="75">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/react/memory_agent.py</source>
<document_content>
"""React Agent with Long-Term Memory.

This module implements a React agent with long-term memory capabilities using LangChain and LangGraph.
It manages user profiles, todo lists, and custom instructions through a state graph architecture.
"""
# pyright: reportUninitializedInstanceVariable=false
# pyright: reportUndefinedVariable=false
# pyright: reportAttributeAccessIssue=false
# pyright: reportInvalidTypeForm=false

from __future__ import annotations

import asyncio
import json
import uuid

from datetime import UTC, datetime, timezone
from typing import Any, Dict, List, Literal, Optional, Tuple, TypedDict, Union

import _utils
import configuration
import langsmith
import rich
import structlog
import tiktoken

from langchain.chat_models import init_chat_model
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, merge_message_runs
from langchain_core.messages.utils import get_buffer_string
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable, RunnableLambda
from langchain_core.runnables.config import RunnableConfig, ensure_config, get_executor_for_config
from langchain_core.tools import tool
from langchain_core.tracers.schemas import Run
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, START, MessagesState, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt import ToolNode
from langgraph.store.base import BaseStore
from langgraph.store.memory import InMemoryStore


logger = structlog.get_logger(__name__)
from prompts import CREATE_INSTRUCTIONS, MODEL_SYSTEM_MESSAGE, TRUSTCALL_INSTRUCTION
from pydantic import BaseModel, Field
from settings import aiosettings
from trustcall import create_extractor
from trustcall._base import ExtractionOutputs, InputsLike


# import nest_asyncio

# nest_asyncio.apply()  # Required for Jupyter Notebook to run async functions


class Memory(BaseModel):
    """A single memory entry containing user-related information.

    Attributes:
        content (str): The main content of the memory (e.g., "User expressed interest in learning about French")
    """
    content: str = Field(description="The main content of the memory. For example: User expressed interest in learning about French.")

class MemoryCollection(BaseModel):
    """A collection of memories about the user.

    Attributes:
        memories (list[Memory]): A list of Memory objects containing user-related information
    """
    memories: list[Memory] = Field(description="A list of memories about the user.")


## Utilities

# SOURCE: https://github.com/langchain-ai/langchain-academy/blob/main/module-5/memory_agent.ipynb
# Visibility into Trustcall updates
# Trustcall creates and updates JSON schemas.
# What if we want visibility into the specific changes made by Trustcall?
# For example, we saw before that Trustcall has some of its own tools to:
# Self-correct from validation failures -- see trace example here
# Update existing documents -- see trace example here
# Visibility into these tools can be useful for the agent we're going to build.
# Below, we'll show how to do this!


# ---------------------------------------------------------------------------------------
# We can add a listener to the Trustcall extractor.
# This will pass runs from the extractor's execution to a class, Spy, that we will define.
# Our Spy class will extract information about what tool calls were made by Trustcall.
# ---------------------------------------------------------------------------------------
# Inspect the tool calls for Trustcall
class Spy:
    """A class to monitor and collect tool calls made by the Trustcall extractor.

    This class acts as a listener for the Trustcall extractor's execution runs,
    collecting information about what tool calls were made during execution.

    Attributes:
        called_tools (list): A list to store tool calls made during execution.
    """

    def __init__(self) -> None:
        """Initialize the Spy with an empty list of called tools."""
        self.called_tools: list = []

    def __call__(self, run: Any) -> None:
        """Process a run and extract tool calls from chat model outputs.

        Traverses the run tree and collects tool calls from chat model outputs.

        Args:
            run: The run object containing execution information.
        """
        logger.info(f"Spy: {run}")
        logger.info(f"Spy type: {type(run)}")
        q: list = [run]
        while q:
            r = q.pop()
            if r.child_runs:
                q.extend(r.child_runs)
            if r.run_type == "chat_model":
                self.called_tools.append(
                    r.outputs["generation"][0][0]["message"]["kwargs"]["tool_calls"]
                )

# DISABLED: from module-5
# # Extract information from tool calls for both patches and new memories in Trustcall
# def extract_tool_info(tool_calls: list[list[dict[str, Any]]], schema_name: str = "Memory") -> str:
#     """Extract information from tool calls for both patches and new memories.

#     This function processes tool calls to extract information about document updates
#     and new memory creation. It formats the extracted information into a human-readable
#     string.

#     Args:
#         tool_calls: List of tool call groups, where each group contains tool call
#             dictionaries with information about patches or new memory creation
#         schema_name: Name of the schema tool (e.g., "Memory", "ToDo", "Profile")

#     Returns:
#         A formatted string containing information about all document updates and
#         new memory creations
#     """
#     # Initialize list of changes
#     changes: list[dict[str, Any]] = []

#     for call_group in tool_calls:
#         for call in call_group:
#             if call['name'] == 'PatchDoc':
#                 changes.append({
#                     'type': 'update',
#                     'doc_id': call['args']['json_doc_id'],
#                     'planned_edits': call['args']['planned_edits'],
#                     'value': call['args']['patches'][0]['value']
#                 })
#             elif call['name'] == schema_name:
#                 changes.append({
#                     'type': 'new',
#                     'value': call['args']
#                 })

#     # Format results as a single string
#     result_parts: list[str] = []
#     for change in changes:
#         if change['type'] == 'update':
#             result_parts.append(
#                 f"Document {change['doc_id']} updated:\n"
#                 f"Plan: {change['planned_edits']}\n"
#                 f"Added content: {change['value']}"
#             )
#         else:
#             result_parts.append(
#                 f"New {schema_name} created:\n"
#                 f"Content: {change['value']}"
#             )

#     return "\n\n".join(result_parts)


# module-6
# Extract information from tool calls for both patches and new memories in Trustcall
def extract_tool_info(tool_calls: list[list[dict[str, Any]]], schema_name: str = "Memory") -> str:
    """Extract information from tool calls for both patches and new memories.

    Args:
        tool_calls: List of tool calls from the model
        schema_name: Name of the schema tool (e.g., "Memory", "ToDo", "Profile")
    """
    # Initialize list of changes
    changes = []

    for call_group in tool_calls:
        for call in call_group:
            if call['name'] == 'PatchDoc':
                # Check if there are any patches
                if call['args']['patches']:
                    changes.append({
                        'type': 'update',
                        'doc_id': call['args']['json_doc_id'],
                        'planned_edits': call['args']['planned_edits'],
                        'value': call['args']['patches'][0]['value']
                    })
                else:
                    # Handle case where no changes were needed
                    changes.append({
                        'type': 'no_update',
                        'doc_id': call['args']['json_doc_id'],
                        'planned_edits': call['args']['planned_edits']
                    })
            elif call['name'] == schema_name:
                changes.append({
                    'type': 'new',
                    'value': call['args']
                })

    # Format results as a single string
    result_parts = []
    for change in changes:
        if change['type'] == 'update':
            result_parts.append(
                f"Document {change['doc_id']} updated:\n"
                f"Plan: {change['planned_edits']}\n"
                f"Added content: {change['value']}"
            )
        elif change['type'] == 'no_update':
            result_parts.append(
                f"Document {change['doc_id']} unchanged:\n"
                f"{change['planned_edits']}"
            )
        else:
            result_parts.append(
                f"New {schema_name} created:\n"
                f"Content: {change['value']}"
            )

    return "\n\n".join(result_parts)

## Schema definitions

# Creating an agent
# There are many different agent architectures to choose from.

# Here, we'll implement something simple, a ReAct agent.

# This agent will be a helpful companion for creating and managing a ToDo list.

# This agent can make a decision to update three types of long-term memory:

# (a) Create or update a user profile with general user information

# (b) Add or update items in a ToDo list collection

# (c) Update its own instructions on how to update items to the ToDo list

# User profile schema
class Profile(BaseModel):
    """This is the profile of the user you are chatting with"""
    name: str | None = Field(description="The user's name", default=None)
    location: str | None = Field(description="The user's location", default=None)
    job: str | None = Field(description="The user's job", default=None)
    connections: list[str] = Field(
        description="Personal connection of the user, such as family members, friends, or coworkers",
        default_factory=list
    )
    interests: list[str] = Field(
        description="Interests that the user has",
        default_factory=list
    )

# ToDo schema
class ToDo(BaseModel):
    task: str = Field(description="The task to be completed.")
    time_to_complete: int | None = Field(description="Estimated time to complete the task (minutes).")
    deadline: datetime | None = Field(
        description="When the task needs to be completed by (if applicable)",
        default=None
    )
    solutions: list[str] = Field(
        description="List of specific, actionable solutions (e.g., specific ideas, service providers, or concrete options relevant to completing the task)",
        min_items=1,
        default_factory=list
    )
    status: Literal["not started", "in progress", "done", "archived"] = Field(
        description="Current status of the task",
        default="not started"
    )

## Initialize the model and tools

# Update memory tool
class UpdateMemory(TypedDict):
    """Decision on what memory type to update.

    Attributes:
        update_type (Literal['user', 'todo', 'instructions']): The type of memory to update
    """
    update_type: Literal['user', 'todo', 'instructions']

# Initialize the model

# model = ChatOpenAI(model="gpt-4o", temperature=0)
# model: BaseChatModel = init_chat_model("gpt-4o", model_provider=aiosettings.llm_provider, temperature=0.0) # pyright: ignore[reportUndefinedVariable]
# TODO: Use this to get embeddings
# tokenizer = tiktoken.encoding_for_model("gpt-4o")

# -----------------------------------------------------------------------------------
# async nodes
# -----------------------------------------------------------------------------------




## Node definitions

async def aio_tasks_democracy_ai(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[BaseMessage]]:
    """Load memories from the store and use them to personalize the chatbot's response.

    This function retrieves user profile, todo list, and custom instructions from the store
    and uses them to generate a personalized chatbot response.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing the list of messages with the chatbot's response
        Format: {"messages": [response]}
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )

    # Retrieve profile memory from the store
    namespace = ("profile", user_id)
    # DISABLED: # memories = store.search(namespace)

    memories = store.search(namespace)
    if memories:
        user_profile = memories[0].value
    else:
        user_profile = None

    # Retrieve people memory from the store
    namespace = ("todo", user_id)
    memories = store.search(namespace)
    todo = "\n".join(f"{mem.value}" for mem in memories)

    # Retrieve custom instructions
    namespace = ("instructions", user_id)
    memories = store.search(namespace)
    if memories:
        instructions = memories[0].value
    else:
        instructions = ""

    # system_msg = configurable.system_prompt.format(user_profile=user_profile, todo=todo, instructions=instructions)
    system_msg = MODEL_SYSTEM_MESSAGE.format(user_profile=user_profile, todo=todo, instructions=instructions)
    logger.error(f"system_msg: {system_msg}")
    # ---------------------------------------------------------------------------------------
    # Note: Passing the config through explicitly is required for python < 3.11
    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
    # ---------------------------------------------------------------------------------------

    # Respond using memory as well as the chat history
    # response = await model.bind_tools([UpdateMemory], parallel_tool_calls=False).ainvoke([SystemMessage(content=system_msg)]+state["messages"])

    # # Respond using memory as well as the chat history
    # response = model.bind_tools([UpdateMemory], parallel_tool_calls=False).invoke([SystemMessage(content=system_msg)]+state["messages"])

    # Respond using memory as well as the chat history
    response = await model.bind_tools([UpdateMemory], parallel_tool_calls=False).ainvoke([SystemMessage(content=system_msg)]+state["messages"], config=config)

    return {"messages": [response]}

async def aio_update_profile(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[dict[str, str]]]:
    """Reflect on the chat history and update the user profile in memory.

    This function processes the chat history to extract and update user profile information
    in the store using the Trustcall extractor.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing a tool message confirming the profile update
        Format: {
            "messages": [{
                "role": "tool",
                "content": "updated profile",
                "tool_call_id": str
            }]
        }
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )


    # Define the namespace for the memories
    namespace: tuple[str, str] = ("profile", user_id)

    # Retrieve the most recent memories for context
    existing_items = store.search(namespace)

    # Format the existing memories for the Trustcall extractor
    tool_name: str = "Profile"
    existing_memories: list[tuple[str, str, Any]] | None = (
        [(existing_item.key, tool_name, existing_item.value)
         for existing_item in existing_items]
        if existing_items
        else None
    )

    # Merge the chat history and the instruction
    trustcall_instruction_formatted: str = TRUSTCALL_INSTRUCTION.format(
        time=datetime.now().isoformat()
    )
    updated_messages: list[BaseMessage] = list(
        merge_message_runs(
            messages=[SystemMessage(content=trustcall_instruction_formatted)] + state["messages"][:-1]
        )
    )

    # ---------------------------------------------------------------------------------------
    # Note: Passing the config through explicitly is required for python < 3.11
    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
    # response = await model.ainvoke(messages, config)
    # Invoke the extractor
    # ---------------------------------------------------------------------------------------
    # # Invoke the extractor
    # result = profile_extractor.invoke({
    #     "messages": updated_messages,
    #     "existing": existing_memories
    # })

    # Invoke the extractor
    result = await profile_extractor.ainvoke({
        "messages": updated_messages,
        "existing": existing_memories
    }, config=config)

    # Save the memories from Trustcall to the store
    for r, rmeta in zip(result["responses"], result["response_metadata"], strict=False):
        store.put(
            namespace,
            rmeta.get("json_doc_id", str(uuid.uuid4())),
            r.model_dump(mode="json"),
        )

    tool_calls = state['messages'][-1].tool_calls
    # Return tool message with update verification
    return {
        "messages": [{
            "role": "tool",
            "content": "updated profile",
            "tool_call_id": tool_calls[0]['id']
        }]
    }

async def aio_update_todos(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[dict[str, str]]]:
    """Reflect on the chat history and update the todo list in memory.

    This function processes the chat history to extract and update todo items
    in the store using the Trustcall extractor. It also tracks changes made
    using a Spy instance.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing a tool message with update details
        Format: {
            "messages": [{
                "role": "tool",
                "content": str,  # Contains details of updates made
                "tool_call_id": str
            }]
        }
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )


    # Define the namespace for the memories
    namespace: tuple[str, str] = ("todo", user_id)

    # Retrieve the most recent memories for context
    existing_items = store.search(namespace)

    # Format the existing memories for the Trustcall extractor
    tool_name: str = "ToDo"
    existing_memories: list[tuple[str, str, Any]] | None = (
        [(existing_item.key, tool_name, existing_item.value)
         for existing_item in existing_items]
        if existing_items
        else None
    )

    # Merge the chat history and the instruction
    trustcall_instruction_formatted: str = TRUSTCALL_INSTRUCTION.format(
        time=datetime.now().isoformat()
    )
    updated_messages: list[BaseMessage] = list(
        merge_message_runs(
            messages=[SystemMessage(content=trustcall_instruction_formatted)] + state["messages"][:-1]
        )
    )

    # Initialize the spy for visibility into the tool calls made by Trustcall
    spy: Spy = Spy()

    # Create the Trustcall extractor for updating the ToDo list
    todo_extractor: Runnable[InputsLike, ExtractionOutputs] = create_extractor(
        model,
        tools=[ToDo],
        tool_choice=tool_name,
        enable_inserts=True
    ).with_listeners(on_end=spy)

    # # Invoke the extractor
    # result = todo_extractor.invoke({
    #     "messages": updated_messages,
    #     "existing": existing_memories
    # })

    # Note: Passing the config through explicitly is required for python < 3.11
    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
    # response = await model.ainvoke(messages, config)
    # Invoke the extractor
    result = await todo_extractor.ainvoke({
        "messages": updated_messages,
        "existing": existing_memories
    }, config=config)

    # Save the memories from Trustcall to the store
    for r, rmeta in zip(result["responses"], result["response_metadata"], strict=False):
        store.put(
            namespace,
            rmeta.get("json_doc_id", str(uuid.uuid4())),
            r.model_dump(mode="json"),
        )

    # Respond to the tool call made in tasks_democracy_ai, confirming the update
    tool_calls = state['messages'][-1].tool_calls

    # Extract the changes made by Trustcall and add to the ToolMessage returned to tasks_democracy_ai
    todo_update_msg: str = extract_tool_info(spy.called_tools, tool_name)
    return {
        "messages": [{
            "role": "tool",
            "content": todo_update_msg,
            "tool_call_id": tool_calls[0]['id']
        }]
    }

async def aio_update_instructions(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[dict[str, str]]]:
    """Reflect on the chat history and update the instructions in memory.

    This function processes the chat history to extract and update user-specified
    preferences for managing the todo list. It stores these instructions for future
    reference.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing a tool message confirming the instructions update
        Format: {
            "messages": [{
                "role": "tool",
                "content": "updated instructions",
                "tool_call_id": str
            }]
        }
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )

    namespace: tuple[str, str] = ("instructions", user_id)

    existing_memory = store.get(namespace, "user_instructions")

    # Format the memory in the system prompt
    system_msg: str = CREATE_INSTRUCTIONS.format(
        current_instructions=existing_memory.value if existing_memory else None
    )

    # # # Respond using memory as well as the chat history
    # new_memory: BaseMessage = model.invoke(
    #     [SystemMessage(content=system_msg)] +
    #     state['messages'][:-1] +
    #     [HumanMessage(content="Please update the instructions based on the conversation")]
    # )
    # # Respond using memory as well as the chat history
    new_memory: BaseMessage = await model.ainvoke(
        [SystemMessage(content=system_msg)] +
        state['messages'][:-1] +
        [HumanMessage(content="Please update the instructions based on the conversation")],
        config=config
    )

    # Overwrite the existing memory in the store
    key: str = "user_instructions"
    store.put(namespace, key, {"memory": new_memory.content})

    tool_calls = state['messages'][-1].tool_calls

    # Return tool message with update verification
    return {
        "messages": [{
            "role": "tool",
            "content": "updated instructions",
            "tool_call_id": tool_calls[0]['id']
        }]
    }


# -----------------------------------------------------------------------------------
# sync nodes
# -----------------------------------------------------------------------------------




## Node definitions

def tasks_democracy_ai(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[BaseMessage]]:
    """Load memories from the store and use them to personalize the chatbot's response.

    This function retrieves user profile, todo list, and custom instructions from the store
    and uses them to generate a personalized chatbot response.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing the list of messages with the chatbot's response
        Format: {"messages": [response]}
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )

    # Retrieve profile memory from the store
    namespace = ("profile", user_id)
    # DISABLED: # memories = store.search(namespace)

    memories = store.search(namespace)
    if memories:
        user_profile = memories[0].value
    else:
        user_profile = None

    # Retrieve people memory from the store
    namespace = ("todo", user_id)
    memories = store.search(namespace)
    todo = "\n".join(f"{mem.value}" for mem in memories)

    # Retrieve custom instructions
    namespace = ("instructions", user_id)
    memories = store.search(namespace)
    if memories:
        instructions = memories[0].value
    else:
        instructions = ""

    # system_msg = configurable.system_prompt.format(user_profile=user_profile, todo=todo, instructions=instructions)
    system_msg = MODEL_SYSTEM_MESSAGE.format(user_profile=user_profile, todo=todo, instructions=instructions)

    # ---------------------------------------------------------------------------------------
    # Note: Passing the config through explicitly is required for python < 3.11
    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
    # ---------------------------------------------------------------------------------------

    # Respond using memory as well as the chat history
    # response = await model.bind_tools([UpdateMemory], parallel_tool_calls=False).ainvoke([SystemMessage(content=system_msg)]+state["messages"])

    # # Respond using memory as well as the chat history
    response = model.bind_tools([UpdateMemory], parallel_tool_calls=False).invoke([SystemMessage(content=system_msg)]+state["messages"], config=config)

    # Respond using memory as well as the chat history
    # response = await model.bind_tools([UpdateMemory], parallel_tool_calls=False).ainvoke([SystemMessage(content=system_msg)]+state["messages"], config=config)

    return {"messages": [response]}

def update_profile(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[dict[str, str]]]:
    """Reflect on the chat history and update the user profile in memory.

    This function processes the chat history to extract and update user profile information
    in the store using the Trustcall extractor.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing a tool message confirming the profile update
        Format: {
            "messages": [{
                "role": "tool",
                "content": "updated profile",
                "tool_call_id": str
            }]
        }
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )


    # Define the namespace for the memories
    namespace: tuple[str, str] = ("profile", user_id)

    # Retrieve the most recent memories for context
    existing_items = store.search(namespace)

    # Format the existing memories for the Trustcall extractor
    tool_name: str = "Profile"
    existing_memories: list[tuple[str, str, Any]] | None = (
        [(existing_item.key, tool_name, existing_item.value)
         for existing_item in existing_items]
        if existing_items
        else None
    )

    # Merge the chat history and the instruction
    trustcall_instruction_formatted: str = TRUSTCALL_INSTRUCTION.format(
        time=datetime.now().isoformat()
    )
    updated_messages: list[BaseMessage] = list(
        merge_message_runs(
            messages=[SystemMessage(content=trustcall_instruction_formatted)] + state["messages"][:-1]
        )
    )

    # ---------------------------------------------------------------------------------------
    # Note: Passing the config through explicitly is required for python < 3.11
    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
    # response = await model.ainvoke(messages, config)
    # Invoke the extractor
    # ---------------------------------------------------------------------------------------
    # # Invoke the extractor
    result = profile_extractor.invoke({
        "messages": updated_messages,
        "existing": existing_memories
    }, config=config)

    # # Invoke the extractor
    # result = await profile_extractor.ainvoke({
    #     "messages": updated_messages,
    #     "existing": existing_memories
    # }, config=config)

    # Save the memories from Trustcall to the store
    for r, rmeta in zip(result["responses"], result["response_metadata"], strict=False):
        store.put(
            namespace,
            rmeta.get("json_doc_id", str(uuid.uuid4())),
            r.model_dump(mode="json"),
        )

    tool_calls = state['messages'][-1].tool_calls
    # Return tool message with update verification
    return {
        "messages": [{
            "role": "tool",
            "content": "updated profile",
            "tool_call_id": tool_calls[0]['id']
        }]
    }

def update_todos(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[dict[str, str]]]:
    """Reflect on the chat history and update the todo list in memory.

    This function processes the chat history to extract and update todo items
    in the store using the Trustcall extractor. It also tracks changes made
    using a Spy instance.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing a tool message with update details
        Format: {
            "messages": [{
                "role": "tool",
                "content": str,  # Contains details of updates made
                "tool_call_id": str
            }]
        }
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )


    # Define the namespace for the memories
    namespace: tuple[str, str] = ("todo", user_id)

    # Retrieve the most recent memories for context
    existing_items = store.search(namespace)

    # Format the existing memories for the Trustcall extractor
    tool_name: str = "ToDo"
    existing_memories: list[tuple[str, str, Any]] | None = (
        [(existing_item.key, tool_name, existing_item.value)
         for existing_item in existing_items]
        if existing_items
        else None
    )

    # Merge the chat history and the instruction
    trustcall_instruction_formatted: str = TRUSTCALL_INSTRUCTION.format(
        time=datetime.now().isoformat()
    )
    updated_messages: list[BaseMessage] = list(
        merge_message_runs(
            messages=[SystemMessage(content=trustcall_instruction_formatted)] + state["messages"][:-1]
        )
    )

    # Initialize the spy for visibility into the tool calls made by Trustcall
    spy: Spy = Spy()

    # Create the Trustcall extractor for updating the ToDo list
    todo_extractor: Runnable[InputsLike, ExtractionOutputs] = create_extractor(
        model,
        tools=[ToDo],
        tool_choice=tool_name,
        enable_inserts=True
    ).with_listeners(on_end=spy)

    # Invoke the extractor
    result = todo_extractor.invoke({
        "messages": updated_messages,
        "existing": existing_memories
    }, config=config)

    # Note: Passing the config through explicitly is required for python < 3.11
    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
    # response = await model.ainvoke(messages, config)
    # Invoke the extractor
    # result = await todo_extractor.ainvoke({
    #     "messages": updated_messages,
    #     "existing": existing_memories
    # }, config=config)

    # Save the memories from Trustcall to the store
    for r, rmeta in zip(result["responses"], result["response_metadata"], strict=False):
        store.put(
            namespace,
            rmeta.get("json_doc_id", str(uuid.uuid4())),
            r.model_dump(mode="json"),
        )

    # Respond to the tool call made in tasks_democracy_ai, confirming the update
    tool_calls = state['messages'][-1].tool_calls

    # Extract the changes made by Trustcall and add to the ToolMessage returned to tasks_democracy_ai
    todo_update_msg: str = extract_tool_info(spy.called_tools, tool_name)
    return {
        "messages": [{
            "role": "tool",
            "content": todo_update_msg,
            "tool_call_id": tool_calls[0]['id']
        }]
    }

def update_instructions(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[dict[str, str]]]:
    """Reflect on the chat history and update the instructions in memory.

    This function processes the chat history to extract and update user-specified
    preferences for managing the todo list. It stores these instructions for future
    reference.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing a tool message confirming the instructions update
        Format: {
            "messages": [{
                "role": "tool",
                "content": "updated instructions",
                "tool_call_id": str
            }]
        }
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )

    namespace: tuple[str, str] = ("instructions", user_id)

    existing_memory = store.get(namespace, "user_instructions")

    # Format the memory in the system prompt
    system_msg: str = CREATE_INSTRUCTIONS.format(
        current_instructions=existing_memory.value if existing_memory else None
    )

    # # # Respond using memory as well as the chat history
    new_memory: BaseMessage = model.invoke(
        [SystemMessage(content=system_msg)] +
        state['messages'][:-1] +
        [HumanMessage(content="Please update the instructions based on the conversation")],
        config=config
    )
    # # Respond using memory as well as the chat history
    # new_memory: BaseMessage = await model.ainvoke(
    #     [SystemMessage(content=system_msg)] +
    #     state['messages'][:-1] +
    #     [HumanMessage(content="Please update the instructions based on the conversation")],
    #     config=config
    # )

    # Overwrite the existing memory in the store
    key: str = "user_instructions"
    store.put(namespace, key, {"memory": new_memory.content})

    tool_calls = state['messages'][-1].tool_calls

    # Return tool message with update verification
    return {
        "messages": [{
            "role": "tool",
            "content": "updated instructions",
            "tool_call_id": tool_calls[0]['id']
        }]
    }

# Conditional edge
def route_message(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> Literal[END, "update_todos", "update_instructions", "update_profile"]:
    """Route messages to appropriate memory update functions based on tool call type.

    This function examines the latest message in the state and determines which memory
    update function should handle it based on the tool call's update_type.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Literal indicating which node should process the message next:
        - END: No tool calls present
        - "update_todos": Route to todo list update
        - "update_instructions": Route to instructions update
        - "update_profile": Route to profile update

    Raises:
        ValueError: If the tool call's update_type is not recognized
    """
    message = state['messages'][-1]
    if len(message.tool_calls) == 0:
        return END
    else:
        tool_call = message.tool_calls[0]
        if tool_call['args']['update_type'] == "user":
            return "update_profile"
        elif tool_call['args']['update_type'] == "todo":
            return "update_todos"
        elif tool_call['args']['update_type'] == "instructions":
            return "update_instructions"
        else:
            raise ValueError("Unknown update_type in tool call")

# SOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/create-react-agent-memory.ipynb
def print_stream(stream):
    for s in stream:
        message = s["messages"][-1]
        if isinstance(message, tuple):
            print(message)
        else:
            message.pretty_print()

# Create the graph + all nodes
builder = StateGraph(MessagesState, config_schema=configuration.Configuration)

# Define the flow of the memory extraction process
builder.add_node(tasks_democracy_ai)
builder.add_node(update_todos)
builder.add_node(update_profile)
builder.add_node(update_instructions)

# Define the flow
builder.add_edge(START, "tasks_democracy_ai")
builder.add_conditional_edges("tasks_democracy_ai", route_message)
builder.add_edge("update_todos", "tasks_democracy_ai")
builder.add_edge("update_profile", "tasks_democracy_ai")
builder.add_edge("update_instructions", "tasks_democracy_ai")


# Store for long-term (across-thread) memory
across_thread_memory = InMemoryStore()

# Checkpointer for short-term (within-thread) memory
within_thread_memory = MemorySaver()

# Compile the graph
# graph: CompiledStateGraph = builder.compile()
graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory, debug=True)
graph.name = "DemocracyExeAI"

print(graph.get_graph().print_ascii())

if __name__ == "__main__":  # pragma: no cover
    import rich
    # config = {}
    # configurable = Configuration.from_runnable_config(config)
    # rich.print(f"configurable: {configurable}")
    # We supply a thread ID for short-term (within-thread) memory
    # We supply a user ID for long-term (across-thread) memory
    config = {"configurable": {"thread_id": "1", "user_id": "1"}}

    # User input
    input_messages = [HumanMessage(content="Hi, my name is Heron and I like apple pie")]

    # Run the graph
    for chunk in graph.stream({"messages": input_messages}, config, stream_mode="values"):
        chunk["messages"][-1].pretty_print()

</document_content>
</document>
<document index="76">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/react/prompts.py</source>
<document_content>
"""Define default prompts."""


## Prompts

# Chatbot instruction for choosing what to update and what tools to call
from __future__ import annotations


MODEL_SYSTEM_MESSAGE = """You are a helpful chatbot.

You are designed to be a companion to a user, helping them keep track of their ToDo list.

You have a long term memory which keeps track of three things:
1. The user's profile (general information about them)
2. The user's ToDo list
3. General instructions for updating the ToDo list

Here is the current User Profile (may be empty if no information has been collected yet):
<user_profile>
{user_profile}
</user_profile>

Here is the current ToDo List (may be empty if no tasks have been added yet):
<todo>
{todo}
</todo>

Here are the current user-specified preferences for updating the ToDo list (may be empty if no preferences have been specified yet):
<instructions>
{instructions}
</instructions>

Here are your instructions for reasoning about the user's messages:

1. Reason carefully about the user's messages as presented below.

2. Decide whether any of the your long-term memory should be updated:
- If personal information was provided about the user, update the user's profile by calling UpdateMemory tool with type `user`
- If tasks are mentioned, update the ToDo list by calling UpdateMemory tool with type `todo`
- If the user has specified preferences for how to update the ToDo list, update the instructions by calling UpdateMemory tool with type `instructions`

3. Tell the user that you have updated your memory, if appropriate:
- Do not tell the user you have updated the user's profile
- Tell the user them when you update the todo list
- Do not tell the user that you have updated instructions

4. Err on the side of updating the todo list. No need to ask for explicit permission.

5. Respond naturally to user user after a tool call was made to save memories, or if no tool call was made."""

# Trustcall instruction
TRUSTCALL_INSTRUCTION = """Reflect on following interaction.

Use the provided tools to retain any necessary memories about the user.

Use parallel tool calling to handle updates and insertions simultaneously.

System Time: {time}"""

# Instructions for updating the ToDo list
CREATE_INSTRUCTIONS = """Reflect on the following interaction.

Based on this interaction, update your instructions for how to update ToDo list items. Use any feedback from the user to update how they like to have items added, etc.

Your current instructions are:

<current_instructions>
{current_instructions}
</current_instructions>"""

</document_content>
</document>
<document index="77">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/react/react_agent.ipynb</source>
<document_content>
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Load environment variables from .env file for API access\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-19 18:09:14.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpre_update\u001b[0m:\u001b[36m827\u001b[0m - \u001b[1mllm_model_name: None\u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpre_update\u001b[0m:\u001b[36m828\u001b[0m - \u001b[1mllm_embedding_model_name: None\u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpre_update\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1msetting default llm_model_name: gpt-4o-mini\u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpre_update\u001b[0m:\u001b[36m841\u001b[0m - \u001b[1msetting default llm_embedding_model_name: text-embedding-3-large\u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpost_root\u001b[0m:\u001b[36m856\u001b[0m - \u001b[1mbefore redis_path: \u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpost_root\u001b[0m:\u001b[36m857\u001b[0m - \u001b[1mbefore redis_pass: None\u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpost_root\u001b[0m:\u001b[36m858\u001b[0m - \u001b[1mbefore redis_user: None\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               +-----------+                                           \n",
      "                                               | __start__ |                                           \n",
      "                                               +-----------+                                           \n",
      "                                                      *                                                \n",
      "                                                      *                                                \n",
      "                                                      *                                                \n",
      "                                          +--------------------+                                       \n",
      "                                          | tasks_democracy_ai |                                       \n",
      "                                    ******+--------------------+........                               \n",
      "                             *******            **           ...        .......                        \n",
      "                      *******                 **                ...            ........                \n",
      "                  ****                       *                     ..                  ....            \n",
      "+---------------------+           +----------------+           +--------------+           +---------+  \n",
      "| update_instructions |           | update_profile |           | update_todos |           | __end__ |  \n",
      "+---------------------+           +----------------+           +--------------+           +---------+  \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"React Agent with Long-Term Memory.\n",
    "\n",
    "This module implements a React agent with long-term memory capabilities using LangChain and LangGraph.\n",
    "It manages user profiles, todo lists, and custom instructions through a state graph architecture.\n",
    "\"\"\"\n",
    "# pyright: reportUninitializedInstanceVariable=false\n",
    "# pyright: reportUndefinedVariable=false\n",
    "# pyright: reportAttributeAccessIssue=false\n",
    "# pyright: reportInvalidTypeForm=false\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "from datetime import UTC, datetime, timezone\n",
    "from typing import Any, Dict, List, Literal, Optional, Tuple, TypedDict, Union\n",
    "\n",
    "import configuration\n",
    "import langsmith\n",
    "import rich\n",
    "import tiktoken\n",
    "\n",
    "from _utils import get_message_text, load_chat_model\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, merge_message_runs\n",
    "from langchain_core.messages.utils import get_buffer_string\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableLambda\n",
    "from langchain_core.runnables.config import RunnableConfig, ensure_config, get_executor_for_config\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.tracers.schemas import Run\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from loguru import logger\n",
    "from pydantic import BaseModel, Field\n",
    "from settings import aiosettings\n",
    "from trustcall import create_extractor\n",
    "from trustcall._base import ExtractionOutputs, InputsLike\n",
    "\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    \"\"\"A single memory entry containing user-related information.\n",
    "\n",
    "    Attributes:\n",
    "        content (str): The main content of the memory (e.g., \"User expressed interest in learning about French\")\n",
    "    \"\"\"\n",
    "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    \"\"\"A collection of memories about the user.\n",
    "\n",
    "    Attributes:\n",
    "        memories (list[Memory]): A list of Memory objects containing user-related information\n",
    "    \"\"\"\n",
    "    memories: list[Memory] = Field(description=\"A list of memories about the user.\")\n",
    "\n",
    "\n",
    "## Utilities\n",
    "\n",
    "# SOURCE: https://github.com/langchain-ai/langchain-academy/blob/main/module-5/memory_agent.ipynb\n",
    "# Visibility into Trustcall updates\n",
    "# Trustcall creates and updates JSON schemas.\n",
    "# What if we want visibility into the specific changes made by Trustcall?\n",
    "# For example, we saw before that Trustcall has some of its own tools to:\n",
    "# Self-correct from validation failures -- see trace example here\n",
    "# Update existing documents -- see trace example here\n",
    "# Visibility into these tools can be useful for the agent we're going to build.\n",
    "# Below, we'll show how to do this!\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# We can add a listener to the Trustcall extractor.\n",
    "# This will pass runs from the extractor's execution to a class, Spy, that we will define.\n",
    "# Our Spy class will extract information about what tool calls were made by Trustcall.\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# Inspect the tool calls for Trustcall\n",
    "class Spy:\n",
    "    \"\"\"A class to monitor and collect tool calls made by the Trustcall extractor.\n",
    "\n",
    "    This class acts as a listener for the Trustcall extractor's execution runs,\n",
    "    collecting information about what tool calls were made during execution.\n",
    "\n",
    "    Attributes:\n",
    "        called_tools (list): A list to store tool calls made during execution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the Spy with an empty list of called tools.\"\"\"\n",
    "        self.called_tools: list = []\n",
    "\n",
    "    def __call__(self, run: Any) -> None:\n",
    "        \"\"\"Process a run and extract tool calls from chat model outputs.\n",
    "\n",
    "        Traverses the run tree and collects tool calls from chat model outputs.\n",
    "\n",
    "        Args:\n",
    "            run: The run object containing execution information.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Spy: {run}\")\n",
    "        logger.info(f\"Spy type: {type(run)}\")\n",
    "        q: list = [run]\n",
    "        while q:\n",
    "            r = q.pop()\n",
    "            if r.child_runs:\n",
    "                q.extend(r.child_runs)\n",
    "            if r.run_type == \"chat_model\":\n",
    "                self.called_tools.append(\n",
    "                    r.outputs[\"generation\"][0][0][\"message\"][\"kwargs\"][\"tool_calls\"]\n",
    "                )\n",
    "\n",
    "# Extract information from tool calls for both patches and new memories in Trustcall\n",
    "def extract_tool_info(tool_calls: list[list[dict[str, Any]]], schema_name: str = \"Memory\") -> str:\n",
    "    \"\"\"Extract information from tool calls for both patches and new memories.\n",
    "\n",
    "    This function processes tool calls to extract information about document updates\n",
    "    and new memory creation. It formats the extracted information into a human-readable\n",
    "    string.\n",
    "\n",
    "    Args:\n",
    "        tool_calls: List of tool call groups, where each group contains tool call\n",
    "            dictionaries with information about patches or new memory creation\n",
    "        schema_name: Name of the schema tool (e.g., \"Memory\", \"ToDo\", \"Profile\")\n",
    "\n",
    "    Returns:\n",
    "        A formatted string containing information about all document updates and\n",
    "        new memory creations\n",
    "    \"\"\"\n",
    "    # Initialize list of changes\n",
    "    changes: list[dict[str, Any]] = []\n",
    "\n",
    "    for call_group in tool_calls:\n",
    "        for call in call_group:\n",
    "            if call['name'] == 'PatchDoc':\n",
    "                changes.append({\n",
    "                    'type': 'update',\n",
    "                    'doc_id': call['args']['json_doc_id'],\n",
    "                    'planned_edits': call['args']['planned_edits'],\n",
    "                    'value': call['args']['patches'][0]['value']\n",
    "                })\n",
    "            elif call['name'] == schema_name:\n",
    "                changes.append({\n",
    "                    'type': 'new',\n",
    "                    'value': call['args']\n",
    "                })\n",
    "\n",
    "    # Format results as a single string\n",
    "    result_parts: list[str] = []\n",
    "    for change in changes:\n",
    "        if change['type'] == 'update':\n",
    "            result_parts.append(\n",
    "                f\"Document {change['doc_id']} updated:\\n\"\n",
    "                f\"Plan: {change['planned_edits']}\\n\"\n",
    "                f\"Added content: {change['value']}\"\n",
    "            )\n",
    "        else:\n",
    "            result_parts.append(\n",
    "                f\"New {schema_name} created:\\n\"\n",
    "                f\"Content: {change['value']}\"\n",
    "            )\n",
    "\n",
    "    return \"\\n\\n\".join(result_parts)\n",
    "\n",
    "## Schema definitions\n",
    "\n",
    "# Creating an agent\n",
    "# There are many different agent architectures to choose from.\n",
    "\n",
    "# Here, we'll implement something simple, a ReAct agent.\n",
    "\n",
    "# This agent will be a helpful companion for creating and managing a ToDo list.\n",
    "\n",
    "# This agent can make a decision to update three types of long-term memory:\n",
    "\n",
    "# (a) Create or update a user profile with general user information\n",
    "\n",
    "# (b) Add or update items in a ToDo list collection\n",
    "\n",
    "# (c) Update its own instructions on how to update items to the ToDo list\n",
    "\n",
    "# User profile schema\n",
    "class Profile(BaseModel):\n",
    "    \"\"\"This is the profile of the user you are chatting with\"\"\"\n",
    "    name: str | None = Field(description=\"The user's name\", default=None)\n",
    "    location: str | None = Field(description=\"The user's location\", default=None)\n",
    "    job: str | None = Field(description=\"The user's job\", default=None)\n",
    "    connections: list[str] = Field(\n",
    "        description=\"Personal connection of the user, such as family members, friends, or coworkers\",\n",
    "        default_factory=list\n",
    "    )\n",
    "    interests: list[str] = Field(\n",
    "        description=\"Interests that the user has\",\n",
    "        default_factory=list\n",
    "    )\n",
    "\n",
    "# ToDo schema\n",
    "class ToDo(BaseModel):\n",
    "    task: str = Field(description=\"The task to be completed.\")\n",
    "    time_to_complete: int | None = Field(description=\"Estimated time to complete the task (minutes).\")\n",
    "    deadline: datetime | None = Field(\n",
    "        description=\"When the task needs to be completed by (if applicable)\",\n",
    "        default=None\n",
    "    )\n",
    "    solutions: list[str] = Field(\n",
    "        description=\"List of specific, actionable solutions (e.g., specific ideas, service providers, or concrete options relevant to completing the task)\",\n",
    "        min_items=1,\n",
    "        default_factory=list\n",
    "    )\n",
    "    status: Literal[\"not started\", \"in progress\", \"done\", \"archived\"] = Field(\n",
    "        description=\"Current status of the task\",\n",
    "        default=\"not started\"\n",
    "    )\n",
    "\n",
    "## Initialize the model and tools\n",
    "\n",
    "# Update memory tool\n",
    "class UpdateMemory(TypedDict):\n",
    "    \"\"\"Decision on what memory type to update.\n",
    "\n",
    "    Attributes:\n",
    "        update_type (Literal['user', 'todo', 'instructions']): The type of memory to update\n",
    "    \"\"\"\n",
    "    update_type: Literal['user', 'todo', 'instructions']\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "model: BaseChatModel = init_chat_model(\"gpt-4o\", model_provider=aiosettings.llm_provider, temperature=0.0) # pyright: ignore[reportUndefinedVariable]\n",
    "# TODO: Use this to get embeddings\n",
    "# tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "## Create the Trustcall extractors for updating the user profile and ToDo list\n",
    "profile_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Profile],\n",
    "    tool_choice=\"Profile\",\n",
    ")\n",
    "\n",
    "## Prompts\n",
    "\n",
    "# Chatbot instruction for choosing what to update and what tools to call\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful chatbot.\n",
    "\n",
    "You are designed to be a companion to a user, helping them keep track of their ToDo list.\n",
    "\n",
    "You have a long term memory which keeps track of three things:\n",
    "1. The user's profile (general information about them)\n",
    "2. The user's ToDo list\n",
    "3. General instructions for updating the ToDo list\n",
    "\n",
    "Here is the current User Profile (may be empty if no information has been collected yet):\n",
    "<user_profile>\n",
    "{user_profile}\n",
    "</user_profile>\n",
    "\n",
    "Here is the current ToDo List (may be empty if no tasks have been added yet):\n",
    "<todo>\n",
    "{todo}\n",
    "</todo>\n",
    "\n",
    "Here are the current user-specified preferences for updating the ToDo list (may be empty if no preferences have been specified yet):\n",
    "<instructions>\n",
    "{instructions}\n",
    "</instructions>\n",
    "\n",
    "Here are your instructions for reasoning about the user's messages:\n",
    "\n",
    "1. Reason carefully about the user's messages as presented below.\n",
    "\n",
    "2. Decide whether any of the your long-term memory should be updated:\n",
    "- If personal information was provided about the user, update the user's profile by calling UpdateMemory tool with type `user`\n",
    "- If tasks are mentioned, update the ToDo list by calling UpdateMemory tool with type `todo`\n",
    "- If the user has specified preferences for how to update the ToDo list, update the instructions by calling UpdateMemory tool with type `instructions`\n",
    "\n",
    "3. Tell the user that you have updated your memory, if appropriate:\n",
    "- Do not tell the user you have updated the user's profile\n",
    "- Tell the user them when you update the todo list\n",
    "- Do not tell the user that you have updated instructions\n",
    "\n",
    "4. Err on the side of updating the todo list. No need to ask for explicit permission.\n",
    "\n",
    "5. Respond naturally to user user after a tool call was made to save memories, or if no tool call was made.\"\"\"\n",
    "\n",
    "# Trustcall instruction\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"Reflect on following interaction.\n",
    "\n",
    "Use the provided tools to retain any necessary memories about the user.\n",
    "\n",
    "Use parallel tool calling to handle updates and insertions simultaneously.\n",
    "\n",
    "System Time: {time}\"\"\"\n",
    "\n",
    "# Instructions for updating the ToDo list\n",
    "CREATE_INSTRUCTIONS = \"\"\"Reflect on the following interaction.\n",
    "\n",
    "Based on this interaction, update your instructions for how to update ToDo list items. Use any feedback from the user to update how they like to have items added, etc.\n",
    "\n",
    "Your current instructions are:\n",
    "\n",
    "<current_instructions>\n",
    "{current_instructions}\n",
    "</current_instructions>\"\"\"\n",
    "\n",
    "## Node definitions\n",
    "\n",
    "\n",
    "async def call_model(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> dict[str, list[BaseMessage]]:\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\n",
    "\n",
    "    This function retrieves user profile, todo list, and custom instructions from the store\n",
    "    and uses them to generate a personalized chatbot response.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Dict containing the list of messages with the chatbot's response\n",
    "        Format: {\"messages\": [response]}\n",
    "    \"\"\"\n",
    "    # Get the user ID from the config\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    user_id = configurable.user_id\n",
    "\n",
    "    # Retrieve profile memory from the store\n",
    "    namespace = (\"profile\", user_id)\n",
    "    # DISABLED: # memories = store.search(namespace)\n",
    "\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        user_profile = memories[0].value\n",
    "    else:\n",
    "        user_profile = None\n",
    "\n",
    "    # Retrieve people memory from the store\n",
    "    namespace = (\"todo\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    todo = \"\\n\".join(f\"{mem.value}\" for mem in memories)\n",
    "\n",
    "    # Retrieve custom instructions\n",
    "    namespace = (\"instructions\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        instructions = memories[0].value\n",
    "    else:\n",
    "        instructions = \"\"\n",
    "\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(user_profile=user_profile, todo=todo, instructions=instructions)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = await model.bind_tools([UpdateMemory], parallel_tool_calls=False).ainvoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "async def tasks_democracy_ai(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> dict[str, list[BaseMessage]]:\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\n",
    "\n",
    "    This function retrieves user profile, todo list, and custom instructions from the store\n",
    "    and uses them to generate a personalized chatbot response.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Dict containing the list of messages with the chatbot's response\n",
    "        Format: {\"messages\": [response]}\n",
    "    \"\"\"\n",
    "    # Get the user ID from the config\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    user_id = configurable.user_id\n",
    "\n",
    "    # Retrieve profile memory from the store\n",
    "    namespace = (\"profile\", user_id)\n",
    "    # DISABLED: # memories = store.search(namespace)\n",
    "\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        user_profile = memories[0].value\n",
    "    else:\n",
    "        user_profile = None\n",
    "\n",
    "    # Retrieve people memory from the store\n",
    "    namespace = (\"todo\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    todo = \"\\n\".join(f\"{mem.value}\" for mem in memories)\n",
    "\n",
    "    # Retrieve custom instructions\n",
    "    namespace = (\"instructions\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        instructions = memories[0].value\n",
    "    else:\n",
    "        instructions = \"\"\n",
    "\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(user_profile=user_profile, todo=todo, instructions=instructions)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = await model.bind_tools([UpdateMemory], parallel_tool_calls=False).ainvoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def update_profile(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> dict[str, list[dict[str, str]]]:\n",
    "    \"\"\"Reflect on the chat history and update the user profile in memory.\n",
    "\n",
    "    This function processes the chat history to extract and update user profile information\n",
    "    in the store using the Trustcall extractor.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Dict containing a tool message confirming the profile update\n",
    "        Format: {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"updated profile\",\n",
    "                \"tool_call_id\": str\n",
    "            }]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Get the user ID from the config\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    user_id = configurable.user_id\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace: tuple[str, str] = (\"profile\", user_id)\n",
    "\n",
    "    # Retrieve the most recent memories for context\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # Format the existing memories for the Trustcall extractor\n",
    "    tool_name: str = \"Profile\"\n",
    "    existing_memories: list[tuple[str, str, Any]] | None = (\n",
    "        [(existing_item.key, tool_name, existing_item.value)\n",
    "         for existing_item in existing_items]\n",
    "        if existing_items\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Merge the chat history and the instruction\n",
    "    trustcall_instruction_formatted: str = TRUSTCALL_INSTRUCTION.format(\n",
    "        time=datetime.now().isoformat()\n",
    "    )\n",
    "    updated_messages: list[BaseMessage] = list(\n",
    "        merge_message_runs(\n",
    "            messages=[SystemMessage(content=trustcall_instruction_formatted)] + state[\"messages\"][:-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Invoke the extractor\n",
    "    result = profile_extractor.invoke({\n",
    "        \"messages\": updated_messages,\n",
    "        \"existing\": existing_memories\n",
    "    })\n",
    "\n",
    "    # Save the memories from Trustcall to the store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"], strict=False):\n",
    "        store.put(\n",
    "            namespace,\n",
    "            rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "            r.model_dump(mode=\"json\"),\n",
    "        )\n",
    "\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "    # Return tool message with update verification\n",
    "    return {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"updated profile\",\n",
    "            \"tool_call_id\": tool_calls[0]['id']\n",
    "        }]\n",
    "    }\n",
    "\n",
    "def update_todos(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> dict[str, list[dict[str, str]]]:\n",
    "    \"\"\"Reflect on the chat history and update the todo list in memory.\n",
    "\n",
    "    This function processes the chat history to extract and update todo items\n",
    "    in the store using the Trustcall extractor. It also tracks changes made\n",
    "    using a Spy instance.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Dict containing a tool message with update details\n",
    "        Format: {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": str,  # Contains details of updates made\n",
    "                \"tool_call_id\": str\n",
    "            }]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Get the user ID from the config\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    user_id = configurable.user_id\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace: tuple[str, str] = (\"todo\", user_id)\n",
    "\n",
    "    # Retrieve the most recent memories for context\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # Format the existing memories for the Trustcall extractor\n",
    "    tool_name: str = \"ToDo\"\n",
    "    existing_memories: list[tuple[str, str, Any]] | None = (\n",
    "        [(existing_item.key, tool_name, existing_item.value)\n",
    "         for existing_item in existing_items]\n",
    "        if existing_items\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Merge the chat history and the instruction\n",
    "    trustcall_instruction_formatted: str = TRUSTCALL_INSTRUCTION.format(\n",
    "        time=datetime.now().isoformat()\n",
    "    )\n",
    "    updated_messages: list[BaseMessage] = list(\n",
    "        merge_message_runs(\n",
    "            messages=[SystemMessage(content=trustcall_instruction_formatted)] + state[\"messages\"][:-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Initialize the spy for visibility into the tool calls made by Trustcall\n",
    "    spy: Spy = Spy()\n",
    "\n",
    "    # Create the Trustcall extractor for updating the ToDo list\n",
    "    todo_extractor: Runnable[InputsLike, ExtractionOutputs] = create_extractor(\n",
    "        model,\n",
    "        tools=[ToDo],\n",
    "        tool_choice=tool_name,\n",
    "        enable_inserts=True\n",
    "    ).with_listeners(on_end=spy)\n",
    "\n",
    "    # Invoke the extractor\n",
    "    result = todo_extractor.invoke({\n",
    "        \"messages\": updated_messages,\n",
    "        \"existing\": existing_memories\n",
    "    })\n",
    "\n",
    "    # Save the memories from Trustcall to the store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"], strict=False):\n",
    "        store.put(\n",
    "            namespace,\n",
    "            rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "            r.model_dump(mode=\"json\"),\n",
    "        )\n",
    "\n",
    "    # Respond to the tool call made in tasks_democracy_ai, confirming the update\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "\n",
    "    # Extract the changes made by Trustcall and add to the ToolMessage returned to tasks_democracy_ai\n",
    "    todo_update_msg: str = extract_tool_info(spy.called_tools, tool_name)\n",
    "    return {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": todo_update_msg,\n",
    "            \"tool_call_id\": tool_calls[0]['id']\n",
    "        }]\n",
    "    }\n",
    "\n",
    "def update_instructions(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> dict[str, list[dict[str, str]]]:\n",
    "    \"\"\"Reflect on the chat history and update the instructions in memory.\n",
    "\n",
    "    This function processes the chat history to extract and update user-specified\n",
    "    preferences for managing the todo list. It stores these instructions for future\n",
    "    reference.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Dict containing a tool message confirming the instructions update\n",
    "        Format: {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"updated instructions\",\n",
    "                \"tool_call_id\": str\n",
    "            }]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Get the user ID from the config\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    user_id = configurable.user_id\n",
    "\n",
    "    namespace: tuple[str, str] = (\"instructions\", user_id)\n",
    "\n",
    "    existing_memory = store.get(namespace, \"user_instructions\")\n",
    "\n",
    "    # Format the memory in the system prompt\n",
    "    system_msg: str = CREATE_INSTRUCTIONS.format(\n",
    "        current_instructions=existing_memory.value if existing_memory else None\n",
    "    )\n",
    "    new_memory: BaseMessage = model.invoke(\n",
    "        [SystemMessage(content=system_msg)] +\n",
    "        state['messages'][:-1] +\n",
    "        [HumanMessage(content=\"Please update the instructions based on the conversation\")]\n",
    "    )\n",
    "\n",
    "    # Overwrite the existing memory in the store\n",
    "    key: str = \"user_instructions\"\n",
    "    store.put(namespace, key, {\"memory\": new_memory.content})\n",
    "\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "\n",
    "    # Return tool message with update verification\n",
    "    return {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"updated instructions\",\n",
    "            \"tool_call_id\": tool_calls[0]['id']\n",
    "        }]\n",
    "    }\n",
    "\n",
    "# Conditional edge\n",
    "def route_message(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> Literal[END, \"update_todos\", \"update_instructions\", \"update_profile\"]:\n",
    "    \"\"\"Route messages to appropriate memory update functions based on tool call type.\n",
    "\n",
    "    This function examines the latest message in the state and determines which memory\n",
    "    update function should handle it based on the tool call's update_type.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Literal indicating which node should process the message next:\n",
    "        - END: No tool calls present\n",
    "        - \"update_todos\": Route to todo list update\n",
    "        - \"update_instructions\": Route to instructions update\n",
    "        - \"update_profile\": Route to profile update\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the tool call's update_type is not recognized\n",
    "    \"\"\"\n",
    "    message = state['messages'][-1]\n",
    "    if len(message.tool_calls) == 0:\n",
    "        return END\n",
    "    else:\n",
    "        tool_call = message.tool_calls[0]\n",
    "        if tool_call['args']['update_type'] == \"user\":\n",
    "            return \"update_profile\"\n",
    "        elif tool_call['args']['update_type'] == \"todo\":\n",
    "            return \"update_todos\"\n",
    "        elif tool_call['args']['update_type'] == \"instructions\":\n",
    "            return \"update_instructions\"\n",
    "        else:\n",
    "            raise ValueError(\"Unknown update_type in tool call\")\n",
    "\n",
    "# SOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/create-react-agent-memory.ipynb\n",
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "\n",
    "# Create the graph + all nodes\n",
    "builder = StateGraph(MessagesState, config_schema=configuration.Configuration)\n",
    "\n",
    "# Define the flow of the memory extraction process\n",
    "builder.add_node(tasks_democracy_ai)\n",
    "builder.add_node(update_todos)\n",
    "builder.add_node(update_profile)\n",
    "builder.add_node(update_instructions)\n",
    "\n",
    "# Define the flow\n",
    "builder.add_edge(START, \"tasks_democracy_ai\")\n",
    "builder.add_conditional_edges(\"tasks_democracy_ai\", route_message)\n",
    "builder.add_edge(\"update_todos\", \"tasks_democracy_ai\")\n",
    "builder.add_edge(\"update_profile\", \"tasks_democracy_ai\")\n",
    "builder.add_edge(\"update_instructions\", \"tasks_democracy_ai\")\n",
    "\n",
    "# Compile the graph\n",
    "graph: CompiledStateGraph = builder.compile()\n",
    "\n",
    "print(graph.get_graph().print_ascii())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected dict, got [HumanMessage(content='I have 30 minutes, what tasks can I get done?', additional_kwargs={}, response_metadata={})]\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Chat with the chatbot\u001b[39;00m\n\u001b[1;32m      6\u001b[0m input_messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI have 30 minutes, what tasks can I get done?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m----> 8\u001b[0m \u001b[43mprint_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Run the graph\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     chunk[\"messages\"][-1].pretty_print()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# inputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print_stream(graph.stream(inputs, stream_mode=\"values\"))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 638\u001b[0m, in \u001b[0;36mprint_stream\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_stream\u001b[39m(stream):\n\u001b[0;32m--> 638\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1573\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1568\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1569\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1571\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1573\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1580\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:104\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    102\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, writer)\u001b[0m\n\u001b[1;32m     38\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:176\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m    175\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m--> 176\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    178\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/pregel/write.py:85\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     writes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     80\u001b[0m         ChannelWriteEntry(write\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;28minput\u001b[39m, write\u001b[38;5;241m.\u001b[39mskip_none, write\u001b[38;5;241m.\u001b[39mmapper)\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m write\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[1;32m     84\u001b[0m     ]\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_at_least_one_of\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/pregel/write.py:130\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[0;34m(config, writes, require_at_least_one_of)\u001b[0m\n\u001b[1;32m    127\u001b[0m entries \u001b[38;5;241m=\u001b[39m [write \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m writes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry)]\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# process entries into values\u001b[39;00m\n\u001b[1;32m    129\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 130\u001b[0m     \u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m write\u001b[38;5;241m.\u001b[39mmapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m entries\n\u001b[1;32m    132\u001b[0m ]\n\u001b[1;32m    133\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    134\u001b[0m     (write\u001b[38;5;241m.\u001b[39mchannel, val)\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val, write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, entries)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m write\u001b[38;5;241m.\u001b[39mskip_none \u001b[38;5;129;01mor\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m ]\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# filter out SKIP_WRITE values\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/graph/state.py:649\u001b[0m, in \u001b[0;36mCompiledStateGraph.attach_node.<locals>._get_state_key\u001b[0;34m(input, key)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    645\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[1;32m    646\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    647\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mINVALID_GRAPH_NODE_RETURN_VALUE,\n\u001b[1;32m    648\u001b[0m     )\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(msg)\n",
      "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got [HumanMessage(content='I have 30 minutes, what tasks can I get done?', additional_kwargs={}, response_metadata={})]\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"Lance\"}}\n",
    "\n",
    "# Chat with the chatbot\n",
    "input_messages = [HumanMessage(content=\"I have 30 minutes, what tasks can I get done?\")]\n",
    "\n",
    "print_stream(graph.stream(input_messages, stream_mode=\"values\"))\n",
    "# Run the graph\n",
    "# for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "#     chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "\n",
    "# inputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\n",
    "# print_stream(graph.stream(inputs, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

</document_content>
</document>
<document index="78">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/react/requirements.txt</source>
<document_content>
langgraph
langchain-core
langchain-community
langchain-openai
trustcall
langchain-fireworks
pinecone-client
tiktoken
loguru
typing-extensions
python-dotenv
uuid
aiohttp
discord.py
rich
pydantic_settings
yarl
pinecone-text
grandalf
nest_asyncio

</document_content>
</document>
<document index="79">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/studio/react/settings.py</source>
<document_content>
"""copy of settings for use in Langgraph Studio"""

# pylint: disable=no-name-in-module
# pyright: reportInvalidTypeForm=false
# pyright: reportUndefinedVariable=false
from __future__ import annotations

import enum
import os
import pathlib

from collections.abc import Callable
from pathlib import Path
from tempfile import gettempdir
from typing import Annotated, Any, Dict, List, Literal, Optional, Set, Union, cast

import structlog


logger = structlog.get_logger(__name__)
from pydantic import (
    AliasChoices,
    AmqpDsn,
    BaseModel,
    Field,
    ImportString,
    Json,
    PostgresDsn,
    RedisDsn,
    SecretBytes,
    SecretStr,
    field_serializer,
    model_validator,
)
from pydantic_settings import BaseSettings, SettingsConfigDict
from rich.console import Console
from rich.table import Table
from typing_extensions import Self, TypedDict
from yarl import URL


TEMP_DIR = Path(gettempdir())
# SOURCE: https://github.com/taikinman/langrila/blob/main/src/langrila/openai/model_config.py
# SOURCE: https://github.com/taikinman/langrila/blob/main/src/langrila/openai/model_config.py
# TODO: Look at this https://github.com/h2oai/h2ogpt/blob/542543dc23aa9eb7d4ce7fe6b9af1204a047b50f/src/enums.py#L386 and see if we can add some more models
_TOKENS_PER_TILE = 170
_TILE_SIZE = 512

_OLDER_MODEL_CONFIG = {
    "gpt-4-0613": {
        "max_tokens": 8192,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00003,
        "completion_cost_per_token": 0.00006,
    },
    "gpt-4-32k-0314": {
        "max_tokens": 32768,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00006,
        "completion_cost_per_token": 0.00012,
    },
    "gpt-4-32k-0613": {
        "max_tokens": 32768,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00006,
        "completion_cost_per_token": 0.00012,
    },
    "gpt-3.5-turbo-0301": {
        "max_tokens": 4096,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.0000015,
        "completion_cost_per_token": 0.000002,
    },
    "gpt-3.5-turbo-0613": {
        "max_tokens": 4096,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.0000015,
        "completion_cost_per_token": 0.000002,
    },
    "gpt-3.5-turbo-16k-0613": {
        "max_tokens": 16384,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.000003,
        "completion_cost_per_token": 0.000004,
    },
    "gpt-3.5-turbo-instruct": {
        "max_tokens": 4096,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.0000015,
        "completion_cost_per_token": 0.000002,
    },
}


_NEWER_MODEL_CONFIG = {
    "claude-3-5-sonnet-20240620": {
        "max_tokens": 2048,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.0000025,
        "completion_cost_per_token": 0.00001,
    },
    "claude-3-opus-20240229": {
        "max_tokens": 2048,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.0000025,
        "completion_cost_per_token": 0.00001,
    },
    "claude-3-sonnet-20240229": {
        "max_tokens": 2048,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.0000025,
        "completion_cost_per_token": 0.00001,
    },
    "claude-3-haiku-20240307": {
        "max_tokens": 2048,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.0000025,
        "completion_cost_per_token": 0.00001,
    },
    "gpt-4o-2024-08-06": {
        "max_tokens": 128000,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.0000025,
        "completion_cost_per_token": 0.00001,
    },
    "gpt-4o-mini-2024-07-18": {
        # "max_tokens": 128000,
        "max_tokens": 900,
        "max_output_tokens": 16384,
        "prompt_cost_per_token": 0.000000150,
        "completion_cost_per_token": 0.00000060,
    },
    "gpt-4o-2024-05-13": {
        "max_tokens": 128000,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.000005,
        "completion_cost_per_token": 0.000015,
    },
    "gpt-4-turbo-2024-04-09": {
        "max_tokens": 128000,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00001,
        "completion_cost_per_token": 0.00003,
    },
    "gpt-4-0125-preview": {
        "max_tokens": 128000,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00001,
        "completion_cost_per_token": 0.00003,
    },
    "gpt-4-1106-preview": {
        "max_tokens": 128000,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00001,
        "completion_cost_per_token": 0.00003,
    },
    "gpt-4-vision-preview": {
        "max_tokens": 128000,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.00001,
        "completion_cost_per_token": 0.00003,
    },
    "gpt-3.5-turbo-0125": {
        "max_tokens": 16384,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.0000005,
        "completion_cost_per_token": 0.0000015,
    },
    "gpt-3.5-turbo-1106": {
        "max_tokens": 16384,
        "max_output_tokens": 4096,
        "prompt_cost_per_token": 0.000001,
        "completion_cost_per_token": 0.000002,
    },
}

_NEWER_EMBEDDING_CONFIG = {
    "text-embedding-3-small": {
        "max_tokens": 8192,
        "prompt_cost_per_token": 0.00000002,
    },
    "text-embedding-3-large": {
        "max_tokens": 8192,
        "prompt_cost_per_token": 0.00000013,
    },
}

_OLDER_EMBEDDING_CONFIG = {
    "text-embedding-ada-002": {
        "max_tokens": 8192,
        "prompt_cost_per_token": 0.0000001,
    },
}


EMBEDDING_CONFIG = {}
EMBEDDING_CONFIG.update(_OLDER_EMBEDDING_CONFIG)
EMBEDDING_CONFIG.update(_NEWER_EMBEDDING_CONFIG)

MODEL_CONFIG = {}
MODEL_CONFIG.update(_OLDER_MODEL_CONFIG)
MODEL_CONFIG.update(_NEWER_MODEL_CONFIG)

MODEL_POINT = {
    "gpt-4o-mini": "gpt-4o-mini-2024-07-18",
    "gpt-4o": "gpt-4o-2024-08-06",
    "gpt-4-turbo": "gpt-4-turbo-2024-04-09",
    "gpt-4": "gpt-4-0613",
    "gpt-4-32k": "gpt-4-32k-0613",
    "gpt-4-vision": "gpt-4-vision-preview",
    "gpt-3.5-turbo": "gpt-3.5-turbo-0125",
    "gpt-3.5-turbo-16k": "gpt-3.5-turbo-16k-0613",
    "claude-3-opus": "claude-3-opus-20240229",
    "claude-3-sonnet": "claude-3-sonnet-20240229",
    "claude-3-haiku": "claude-3-haiku-20240307",
    "claude-3-5-sonnet": "claude-3-5-sonnet-20240620",
}

_MODEL_POINT_CONFIG = {
    "gpt-4o-mini": MODEL_CONFIG[MODEL_POINT["gpt-4o-mini"]],
    "gpt-4o": MODEL_CONFIG[MODEL_POINT["gpt-4o"]],
    "gpt-4-turbo": MODEL_CONFIG[MODEL_POINT["gpt-4-turbo"]],
    "gpt-4": MODEL_CONFIG[MODEL_POINT["gpt-4"]],
    "gpt-4-32k": MODEL_CONFIG[MODEL_POINT["gpt-4-32k"]],
    "gpt-4-vision": MODEL_CONFIG[MODEL_POINT["gpt-4-vision"]],
    "gpt-3.5-turbo": MODEL_CONFIG[MODEL_POINT["gpt-3.5-turbo"]],
    "gpt-3.5-turbo-16k": MODEL_CONFIG[MODEL_POINT["gpt-3.5-turbo-16k"]],
    "claude-3-opus": MODEL_CONFIG[MODEL_POINT["claude-3-opus"]],
    "claude-3-5-sonnet": MODEL_CONFIG[MODEL_POINT["claude-3-5-sonnet"]],
    "claude-3-sonnet": MODEL_CONFIG[MODEL_POINT["claude-3-sonnet"]],
    "claude-3-haiku": MODEL_CONFIG[MODEL_POINT["claude-3-haiku"]],
}

# contains all the models and embeddings info
MODEL_CONFIG.update(_MODEL_POINT_CONFIG)

# produces a list of all models and embeddings available
MODEL_ZOO = set(MODEL_CONFIG.keys()) | set(EMBEDDING_CONFIG.keys())

# SOURCE: https://github.com/JuliusHenke/autopentest/blob/ca822f723a356ec974d2dff332c2d92389a4c5e3/src/text_embeddings.py#L19
# https://platform.openai.com/docs/guides/embeddings/embedding-models
EMBEDDING_MODEL_DIMENSIONS_DATA = {
    "text-embedding-ada-002": 1536,
    "text-embedding-3-small": 512,
    "text-embedding-3-large": 1024,
}


# NOTE: DIRTY HACK TO GET AROUND CIRCULAR IMPORTS
# NOTE: There is a bug in pydantic that prevents us from using the `tilda` package and dealing with circular imports
def tilda(obj):
    """
    Wrapper for linux ~/ shell notation

    Args:
    ----
        obj (_type_): _description_

    Returns:
    -------
        _type_: _description_

    """
    if isinstance(obj, list):
        return [str(pathlib.Path(o).expanduser()) if isinstance(o, str) else o for o in obj]
    elif isinstance(obj, str):
        return str(pathlib.Path(obj).expanduser())
    else:
        return obj


def normalize_settings_path(file_path: str) -> str:
    """
    field_validator used to detect shell tilda notation and expand field automatically

    Args:
    ----
        file_path (str): _description_

    Returns:
    -------
        pathlib.PosixPath | str: _description_

    """
    # prevent circular import
    # from democracy_exe.utils import file_functions

    return tilda(file_path) if file_path.startswith("~") else file_path


def get_rich_console() -> Console:
    """
    _summary_

    Returns
    -------
        Console: _description_

    """
    return Console()


class LogLevel(str, enum.Enum):
    """Possible log levels."""

    NOTSET = "NOTSET"
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    FATAL = "FATAL"


class AioSettings(BaseSettings):
    """
    Application settings.

    These parameters can be configured
    with environment variables.
    """

    # By default, the environment variable name is the same as the field name.

    # You can change the prefix for all environment variables by setting the env_prefix config setting, or via the _env_prefix keyword argument on instantiation:

    # add a comment to each line in model_config explaining what it does
    model_config = SettingsConfigDict(
        env_prefix="DEMOCRACY_EXE_CONFIG_",
        env_file=(".env", ".envrc"),
        env_file_encoding="utf-8",
        extra="allow",
        arbitrary_types_allowed=True,
        json_schema_extra={
            "properties": {
                "llm_retriever_type": {
                    "type": "string",
                    "default": "vector_store",
                    "description": "Type of retriever to use",
                }
            }
        },
    )

    monitor_host: str = "localhost"
    monitor_port: int = 50102

    debug_langchain: bool | None = False

    # tweetpik_background_image = "510"  # a image that you want to use as background. you need to use this as a valid url like https://mysite.com/image.png and it should not be protected by cors
    audit_log_send_channel: str = ""

    # ***************************************************
    # NOTE: these are grouped together
    # ***************************************************
    # token: str = ""
    prefix: str = "?"
    discord_command_prefix: str = "?"

    discord_admin_user_id: int | None = None

    discord_general_channel: int = 908894727779258390

    discord_server_id: int = 0
    discord_client_id: int | str = 0

    discord_token: SecretStr = ""

    vector_store_type: Literal["pgvector", "chroma", "pinecone", "sklearn"] = "pgvector"

    # openai_token: str = ""
    openai_api_key: SecretStr = ""

    discord_admin_user_invited: bool = False

    debug: bool = True
    log_pii: bool = True

    personalization_file: str = Field(
        env="PERSONALIZATION_FILE",
        description="Path to the personalization JSON file",
        default="./personalization.json",
    )
    scratch_pad_dir: str = Field(
        env="SCRATCH_PAD_DIR",
        description="Directory for scratch pad files",
        default="./scratchpad",
    )
    active_memory_file: str = Field(
        env="ACTIVE_MEMORY_FILE",
        description="Path to the active memory JSON file",
        default="./active_memory.json",
    )

    changelogs_github_api_token: SecretStr = Field(
        env="CHANGELOGS_GITHUB_API_TOKEN", description="GitHub API token for Changelogs", default=""
    )
    firecrawl_api_key: SecretStr = Field(env="FIRECRAWL_API_KEY", description="Firecrawl API key", default="")

    # pylint: disable=redundant-keyword-arg
    better_exceptions: bool = Field(env="BETTER_EXCEPTIONS", description="Enable better exceptions", default=1)
    pythonasynciodebug: bool = Field(
        env="PYTHONASYNCIODEBUG", description="enable or disable asyncio debugging", default=0
    )
    pythondevmode: bool = Field(
        env="PYTHONDEVMODE",
        description="The Python Development Mode introduces additional runtime checks that are too expensive to be enabled by default. It should not be more verbose than the default if the code is correct; new warnings are only emitted when an issue is detected.",
        default=0,
    )
    langchain_debug_logs: bool = Field(
        env="LANGCHAIN_DEBUG_LOGS", description="enable or disable langchain debug logs", default=0
    )

    enable_ai: bool = False
    http_client_debug_enabled: bool = False

    localfilestore_root_path: str = Field(
        env="LOCALFILESTORE_ROOT_PATH", description="root path for local file store", default="./local_file_store"
    )

    # Try loading patchmatch
    globals_try_patchmatch: bool = True

    # Use CPU even if GPU is available (main use case is for debugging MPS issues)
    globals_always_use_cpu: bool = False

    # Whether the internet is reachable for dynamic downloads
    # The CLI will test connectivity at startup time.
    globals_internet_available: bool = True

    # whether we are forcing full precision
    globals_full_precision: bool = False

    # whether we should convert ckpt files into diffusers models on the fly
    globals_ckpt_convert: bool = False

    # logging tokenization everywhere
    globals_log_tokenization: bool = False

    bot_name: str = "SandboxAgentAI"

    # Variables for Redis
    redis_host: str = "localhost"
    redis_port: int = 8600
    redis_user: str | None = None
    redis_pass: SecretStr | None = None
    redis_base: int | None = None
    enable_redis: bool = False
    redis_url: URL | str | None = None

    sentry_dsn: SecretStr = ""
    enable_sentry: bool = False

    # Variables for ChromaDB


    chroma_host: str = "localhost"
    chroma_port: str = "9010"
    enable_chroma: bool = True

    dev_mode: bool = Field(env="DEV_MODE", description="enable dev mode", default=False)

    llm_temperature: float = 0.0

    vision_model: str = "gpt-4o"
    chat_model: str = "gpt-4o-mini"

    chat_history_buffer: int = 10

    retry_stop_after_attempt: int = 3
    retry_wait_exponential_multiplier: int | float = 2
    retry_wait_exponential_max: int | float = 5
    retry_wait_exponential_min: int | float = 1
    retry_wait_fixed: int | float = 15

    pinecone_api_key: SecretStr = Field(env="PINECONE_API_KEY", description="pinecone api key", default="")
    pinecone_env: str = Field(env="PINECONE_ENV", description="pinecone env", default="local")
    pinecone_index: str = Field(env="PINECONE_INDEX", description="pinecone index", default="")
    pinecone_namespace: str = Field(env="PINECONE_NAMESPACE", description="pinecone namespace", default="ns1")
    pinecone_index_name: str = Field(env="PINECONE_INDEX_NAME", description="pinecone index name", default="democracy-exe")
    pinecone_url: str = Field(env="PINECONE_URL", description="pinecone url", default="https://democracy-exe-dxt6ijd.svc.aped-4627-b74a.pinecone.io")

    chatbot_type: Literal["terminal", "discord"] = Field(env="CHATBOT_TYPE", description="chatbot type", default="terminal")

    unstructured_api_key: SecretStr = Field(env="UNSTRUCTURED_API_KEY", description="unstructured api key", default="")
    unstructured_api_url: str = Field(
        env="UNSTRUCTURED_API_URL",
        description="unstructured api url",
        default="https://api.unstructured.io/general/v0/general",
    )
    brave_search_api_key: SecretStr = Field(env="BRAVE_SEARCH_API_KEY", description="Brave Search API key", default="")

    anthropic_api_key: SecretStr = Field(env="ANTHROPIC_API_KEY", description="claude api key", default="")
    groq_api_key: SecretStr = Field(env="GROQ_API_KEY", description="groq api key", default="")
    cohere_api_key: SecretStr = Field(env="COHERE_API_KEY", description="cohere api key", default="")
    tavily_api_key: SecretStr = Field(env="TAVILY_API_KEY", description="Tavily API key", default="")
    brave_search_api_key: SecretStr = Field(env="BRAVE_SEARCH_API_KEY", description="Brave Search API key", default="")

    langchain_endpoint: str = Field(
        env="LANGCHAIN_ENDPOINT", description="langchain endpoint", default="https://api.smith.langchain.com"
    )
    langchain_tracing_v2: bool = Field(
        env="LANGCHAIN_TRACING_V2", description="langchain tracing version", default=False
    )
    langchain_api_key: SecretStr = Field(
        env="LANGCHAIN_API_KEY", description="langchain api key for langsmith", default=""
    )
    langchain_hub_api_url: str = Field(
        env="LANGCHAIN_HUB_API_URL",
        description="langchain hub api url for langsmith",
        default="https://api.hub.langchain.com",
    )
    langchain_hub_api_key: SecretStr = Field(
        env="LANGCHAIN_HUB_API_KEY", description="langchain hub api key for langsmith", default=""
    )
    langchain_project: str = Field(
        env="LANGCHAIN_PROJECT", description="langsmith project name", default="democracy_exe"
    )
    debug_aider: bool = Field(env="DEBUG_AIDER", description="debug tests stuff written by aider", default=False)

    local_test_debug: bool = Field(env="LOCAL_TEST_DEBUG", description="enable local debug testing", default=False)
    local_test_enable_evals: bool = Field(
        env="LOCAL_TEST_ENABLE_EVALS", description="enable local debug testing with evals", default=False
    )
    python_debug: bool = Field(env="PYTHON_DEBUG", description="enable bpdb on cli", default=False)
    experimental_redis_memory: bool = Field(
        env="EXPERIMENTAL_REDIS_MEMORY", description="enable experimental redis memory", default=False
    )

    oco_openai_api_key: SecretStr = Field(env="OCO_OPENAI_API_KEY", description="opencommit api key", default="")
    oco_tokens_max_input: int = Field(env="OCO_TOKENS_MAX_INPUT", description="OCO_TOKENS_MAX_INPUT", default=4096)
    oco_tokens_max_output: int = Field(env="OCO_TOKENS_MAX_OUTPUT", description="OCO_TOKENS_MAX_OUTPUT", default=500)
    oco_model: str = Field(env="OCO_MODEL", description="OCO_MODEL", default="gpt-4o")
    oco_language: str = Field(env="OCO_LANGUAGE", description="OCO_LANGUAGE", default="en")
    oco_prompt_module: str = Field(
        env="OCO_PROMPT_MODULE", description="OCO_PROMPT_MODULE", default="conventional-commit"
    )
    oco_ai_provider: str = Field(env="OCO_AI_PROVIDER", description="OCO_AI_PROVIDER", default="openai")

    openai_embeddings_model: str = Field(
        env="OPENAI_EMBEDDINGS_MODEL", description="openai embeddings model", default="text-embedding-3-large"
    )

    editor: str = Field(env="EDITOR", description="EDITOR", default="vim")
    visual: str = Field(env="VISUAL", description="VISUAL", default="vim")
    git_editor: str = Field(env="GIT_EDITOR", description="GIT_EDITOR", default="vim")

    llm_streaming: bool = Field(env="LLM_STREAMING", description="Enable streaming for LLM", default=False)
    llm_provider: str = Field(
        env="LLM_PROVIDER", description="LLM provider (e.g., openai, anthropic)", default="openai"
    )
    llm_max_retries: int = Field(
        env="LLM_MAX_RETRIES", description="Maximum number of retries for LLM API calls", default=3
    )
    llm_recursion_limit: int = Field(env="LLM_RECURSION_LIMIT", description="Recursion limit for LLM", default=50)
    llm_document_loader_type: Literal["pymupdf", "web", "directory"] = Field(
        env="LLM_DOCUMENT_LOADER_TYPE", description="Document loader type", default="pymupdf"
    )
    llm_text_splitter_type: Literal["recursive_text", "recursive_character"] = Field(
        env="LLM_TEXT_SPLITTER_TYPE", description="Text splitter type", default="recursive_text"
    )
    llm_vectorstore_type: Literal["pgvector", "chroma", "pinecone", "sklearn"] = Field(
        env="LLM_VECTORSTORE_TYPE", description="Vector store type", default="pgvector"
    )
    llm_embedding_model_type: Literal["text-embedding-3-large", "text-embedding-ada-002"] = Field(
        env="LLM_EMBEDDING_MODEL_TYPE", description="Embedding model type", default="text-embedding-3-large"
    )
    llm_key_value_stores_type: Literal["redis", "dynamodb"] = Field(
        env="LLM_KEY_VALUE_STORES_TYPE", description="Key-value stores type", default="redis"
    )
    # Variables for Postgres/pgvector
    pgvector_driver: str = Field(
        env="PGVECTOR_DRIVER",
        description="The database driver to use for pgvector (e.g., psycopg)",
        default="psycopg",
    )
    pgvector_host: str = Field(
        env="PGVECTOR_HOST",
        description="The hostname or IP address of the pgvector database server",
        default="localhost",
    )
    pgvector_port: int = Field(
        env="PGVECTOR_PORT",
        description="The port number of the pgvector database server",
        default=6432,
    )
    pgvector_database: str = Field(
        env="PGVECTOR_DATABASE",
        description="The name of the pgvector database",
        default="langchain",
    )
    pgvector_user: str = Field(
        env="PGVECTOR_USER",
        description="The username to connect to the pgvector database",
        default="langchain",
    )
    pgvector_password: SecretStr = Field(
        env="PGVECTOR_PASSWORD",
        description="The password to connect to the pgvector database",
        default="langchain",
    )
    pgvector_pool_size: int = Field(
        env="PGVECTOR_POOL_SIZE",
        description="The size of the connection pool for the pgvector database",
        default=10,
    )
    pgvector_dsn_uri: str = Field(
        env="PGVECTOR_DSN_URI",
        description="optional DSN URI, if set other pgvector_* settings are ignored",
        default="",
    )

    # Index - text splitter settings
    text_chunk_size: int = 2000
    text_chunk_overlap: int = 200
    text_splitter: Json[dict[str, Any]] = "{}"  # custom splitter settings

    # LLM settings
    qa_completion_llm: Json[dict[str, Any]] = """{
        "_type": "openai-chat",
        "model_name": "gpt-4o-mini",
        "temperature": 0,
        "max_tokens": 1000,
        "verbose": true
    }"""
    qa_followup_llm: Json[dict[str, Any]] = """{
        "_type": "openai-chat",
        "model_name": "gpt-4o-mini",
        "temperature": 0,
        "max_tokens": 200,
        "verbose": true
    }"""
    summarize_llm: Json[dict[str, Any]] = """{
        "_type": "openai-chat",
        "model_name": "gpt-4o",
        "temperature": 0,
        "max_tokens": 2000
    }"""


    postgres_host: str = "localhost"
    postgres_port: int = 8432
    postgres_password: str | None = "langchain"
    postgres_driver: str | None = "psycopg"
    postgres_database: str | None = "langchain"
    postgres_collection_name: str | None = "langchain"
    postgres_user: str | None = "langchain"
    enable_postgres: bool = True

    # QA
    qa_no_chat_history: bool = False  # don't load chat history
    qa_followup_sim_threshold: float = 0.735  # similitude threshold in followup
    qa_retriever: Json[dict[str, Any]] = "{}"  # custom retriever settings

    # Summarization
    summ_default_chain: str = "stuff"
    summ_token_splitter: int = 4000
    summ_token_overlap: int = 500

    sklearn_persist_path: str = Field(
        env="SKLEARN_PERSIST_PATH",
        description="Path to persist the SKLearn vector store",
        default="./db.db",
    )
    sklearn_serializer: Literal["json", "bson", "parquet"] = Field(
        env="SKLEARN_SERIALIZER",
        description="Serializer for the SKLearn vector store",
        default="json",
    )
    sklearn_metric: str = Field(
        env="SKLEARN_METRIC",
        description="Metric for the SKLearn vector store",
        default="cosine",
    )

    # Evaluation settings
    eval_max_concurrency: int = Field(
        env="EVAL_MAX_CONCURRENCY", description="Maximum number of concurrent evaluations", default=4
    )
    llm_model_name: str = Field(
        env="LLM_MODEL_NAME", description="Name of the LLM model to use", default="gpt-4o-mini", init=True
    )
    llm_json_model_name: str = Field(
        env="LLM_JSON_MODEL_NAME", description="Name of the LLM model to use", default="gpt-4o-mini", init=True
    )
    provider: str = Field(env="PROVIDER", description="AI provider (openai or anthropic)", default="openai")
    chunk_size: int = Field(env="CHUNK_SIZE", description="Size of each text chunk", default=1000)
    chunk_overlap: int = Field(env="CHUNK_OVERLAP", description="Overlap between text chunks", default=200)
    add_start_index: bool = Field(
        env="ADD_START_INDEX", description="Whether to add start index to text chunks", default=False
    )
    llm_embedding_model_name: str = Field(
        env="LLM_EMBEDDING_MODEL_NAME",
        description="Name of the embedding model to use",
        default="text-embedding-3-large",
    )
    llm_retriever_type: str = Field(
        env="LLM_RETRIEVER_TYPE",
        description="Type of retriever to use",
        default="vector_store",
    )
    default_search_kwargs: dict[str, int] = Field(
        env="DEFAULT_SEARCH_KWARGS",
        description="Default arguments for similarity search",
        default_factory=lambda: {"k": 2},
    )
    question_to_ask: str = Field(
        env="QUESTION_TO_ASK",
        description="Question to ask for evaluation",
        default="What is the main cause of climate change?",
    )
    dataset_name: str = Field(
        env="DATASET_NAME", description="Name of the dataset to use for evaluation", default="Climate Change Q&A"
    )

    # Model-specific settings
    max_tokens: int = Field(env="MAX_TOKENS", description="Maximum number of tokens for the model", default=900)
    max_retries: int = Field(env="MAX_RETRIES", description="Maximum number of retries for API calls", default=9)

    # # Evaluation feature flags
    compare_models_feature_flag: bool = Field(
        env="COMPARE_MODELS_FEATURE_FLAG", description="Enable comparing different models", default=False
    )
    rag_answer_v_reference_feature_flag: bool = Field(
        env="RAG_ANSWER_V_REFERENCE_FEATURE_FLAG", description="Enable comparing RAG answer to reference", default=False
    )
    helpfulness_feature_flag: bool = Field(
        env="HELPFULNESS_FEATURE_FLAG", description="Enable helpfulness evaluation", default=False
    )
    rag_answer_hallucination_feature_flag: bool = Field(
        env="RAG_ANSWER_HALLUCINATION_FEATURE_FLAG",
        description="Enable evaluating RAG answer hallucination",
        default=False,
    )
    rag_doc_relevance_feature_flag: bool = Field(
        env="RAG_DOC_RELEVANCE_FEATURE_FLAG", description="Enable evaluating RAG document relevance", default=False
    )
    rag_doc_relevance_and_hallucination_feature_flag: bool = Field(
        env="RAG_DOC_RELEVANCE_AND_HALLUCINATION_FEATURE_FLAG",
        description="Enable evaluating RAG document relevance and hallucination",
        default=False,
    )
    rag_answer_accuracy_feature_flag: bool = Field(
        env="RAG_ANSWER_ACCURACY_FEATURE_FLAG", description="Enable evaluating RAG answer accuracy", default=True
    )
    helpfulness_testing_feature_flag: bool = Field(
        env="HELPFULNESS_TESTING_FEATURE_FLAG", description="Enable helpfulness testing", default=False
    )
    rag_string_embedding_distance_metrics_feature_flag: bool = Field(
        env="RAG_STRING_EMBEDDING_DISTANCE_METRICS_FEATURE_FLAG",
        description="Enable evaluating RAG string embedding distance metrics",
        default=False,
    )

    llm_memory_type: str = Field(env="LLM_MEMORY_TYPE", description="Type of memory to use", default="memorysaver")
    llm_memory_enabled: bool = Field(env="LLM_MEMORY_ENABLED", description="Enable memory", default=True)
    llm_human_loop_enabled: bool = Field(env="LLM_HUMAN_LOOP_ENABLED", description="Enable human loop", default=False)
    # Tool allowlist
    tool_allowlist: list[str] = ["tavily_search", "magic_function"]

    # Tool-specific configuration
    tavily_search_max_results: int = 3

    agent_type: Literal["plan_and_execute", "basic", "advanced", "adaptive_rag"] = Field(
        env="AGENT_TYPE", description="Type of agent to use", default="adaptive_rag"
    )

    tweetpik_api_key: SecretStr = Field(env="TWEETPIK_API_KEY", description="TweetPik API key", default="")

    tweetpik_authorization: SecretStr = Field(env="TWEETPIK_AUTHORIZATION", description="TweetPik authorization", default="")
    tweetpik_bucket_id: str = Field(env="TWEETPIK_BUCKET_ID", description="TweetPik bucket ID", default="323251495115948625")
    # change the background color of the tweet screenshot
    tweetpik_background_color: str = "#ffffff"

    # Theme and dimension settings
    tweetpik_theme: str = Field(env="TWEETPIK_THEME", description="Theme for tweet screenshots", default="dim")
    tweetpik_dimension: str = Field(env="TWEETPIK_DIMENSION", description="Dimension for tweet screenshots", default="instagramFeed")

    # Color settings
    tweetpik_background_color: str = Field(env="TWEETPIK_BACKGROUND_COLOR", description="Background color for tweet screenshots", default="#15212b")
    tweetpik_text_primary_color: str = Field(env="TWEETPIK_TEXT_PRIMARY_COLOR", description="Primary text color", default="#FFFFFF")
    tweetpik_text_secondary_color: str = Field(env="TWEETPIK_TEXT_SECONDARY_COLOR", description="Secondary text color", default="#8899a6")
    tweetpik_link_color: str = Field(env="TWEETPIK_LINK_COLOR", description="Color for links and mentions", default="#1b95e0")
    tweetpik_verified_icon_color: str = Field(env="TWEETPIK_VERIFIED_ICON_COLOR", description="Color for verified badge", default="#1b95e0")

    # Display settings
    tweetpik_display_verified: str = Field(env="TWEETPIK_DISPLAY_VERIFIED", description="Show verified badge", default="default")
    tweetpik_display_metrics: bool = Field(env="TWEETPIK_DISPLAY_METRICS", description="Show metrics (likes, retweets)", default=False)
    tweetpik_display_embeds: bool = Field(env="TWEETPIK_DISPLAY_EMBEDS", description="Show embedded content", default=True)

    # Content settings
    tweetpik_content_scale: float = Field(env="TWEETPIK_CONTENT_SCALE", description="Scale factor for content", default=0.77)
    tweetpik_content_width: int = Field(env="TWEETPIK_CONTENT_WIDTH", description="Width of content in percentage", default=100)

    # any number higher than zero. this value is used in pixels(px) units
    tweetpik_canvas_width: str = "510"
    tweetpik_dimension_ig_feed: str = "1:1"
    tweetpik_dimension_ig_story: str = "9:16"
    tweetpik_display_likes: bool = False
    tweetpik_display_link_preview: bool = True
    tweetpik_display_media_images: bool = True
    tweetpik_display_replies: bool = False
    tweetpik_display_retweets: bool = False
    tweetpik_display_source: bool = True
    tweetpik_display_time: bool = True
    tweetpik_display_verified: bool = True

    # change the link colors used for the links, hashtags and mentions
    tweetpik_link_color: str = "#1b95e0"

    tweetpik_text_primary_color: str = (
        "#000000"  # change the text primary color used for the main text of the tweet and user's name
    )
    tweetpik_text_secondary_color: str = (
        "#5b7083"  # change the text secondary used for the secondary info of the tweet like the username
    )

    # any number higher than zero. this value is representing a percentage
    tweetpik_text_width: str = "100"

    tweetpik_timezone: str = "america/new_york"

    # change the verified icon color
    tweetpik_verified_icon: str = "#1b95e0"

    @model_validator(mode="before")
    @classmethod
    def pre_update(cls, values: dict[str, Any]) -> dict[str, Any]:
        llm_model_name = values.get("llm_model_name")
        llm_embedding_model_name = values.get("llm_embedding_model_name")
        logger.info(f"llm_model_name: {llm_model_name}")
        logger.info(f"llm_embedding_model_name: {llm_embedding_model_name}")
        if llm_model_name:
            values["max_tokens"] = MODEL_CONFIG[llm_model_name]["max_tokens"]
            values["max_output_tokens"] = MODEL_CONFIG[llm_model_name]["max_output_tokens"]
            values["prompt_cost_per_token"] = MODEL_CONFIG[llm_model_name]["prompt_cost_per_token"]
            values["completion_cost_per_token"] = MODEL_CONFIG[llm_model_name]["completion_cost_per_token"]
            if llm_embedding_model_name:
                values["embedding_max_tokens"] = EMBEDDING_MODEL_DIMENSIONS_DATA[llm_embedding_model_name]
                values["embedding_model_dimensions"] = EMBEDDING_MODEL_DIMENSIONS_DATA[llm_embedding_model_name]
        else:
            llm_model_name = "gpt-4o-mini"
            llm_embedding_model_name = "text-embedding-3-large"
            logger.info(f"setting default llm_model_name: {llm_model_name}")
            logger.info(f"setting default llm_embedding_model_name: {llm_embedding_model_name}")
            values["max_tokens"] = MODEL_CONFIG[llm_model_name]["max_tokens"]
            values["max_output_tokens"] = MODEL_CONFIG[llm_model_name]["max_output_tokens"]
            values["prompt_cost_per_token"] = MODEL_CONFIG[llm_model_name]["prompt_cost_per_token"]
            values["completion_cost_per_token"] = MODEL_CONFIG[llm_model_name]["completion_cost_per_token"]
            values["embedding_max_tokens"] = EMBEDDING_MODEL_DIMENSIONS_DATA[llm_embedding_model_name]
            values["embedding_model_dimensions"] = EMBEDDING_MODEL_DIMENSIONS_DATA[llm_embedding_model_name]

        return values

    @model_validator(mode="after")
    def post_root(self) -> Self:
        redis_path = f"/{self.redis_base}" if self.redis_base is not None else ""
        redis_pass = self.redis_pass if self.redis_pass is not None else None
        redis_user = self.redis_user if self.redis_user is not None else None
        logger.info(f"before redis_path: {redis_path}")
        logger.info(f"before redis_pass: {redis_pass}")
        logger.info(f"before redis_user: {redis_user}")
        if redis_pass is None and redis_user is None:
            self.redis_url = URL.build(
                scheme="redis",
                host=self.redis_host,
                port=self.redis_port,
                path=redis_path,
            )
        else:
            self.redis_url = URL.build(
                scheme="redis",
                host=self.redis_host,
                port=self.redis_port,
                path=redis_path,
                user=redis_user,
                password=redis_pass.get_secret_value(),
            )

        return self

    @property
    def postgres_url(self) -> URL:
        """
        Assemble postgres URL from settings.

        :return: postgres URL.
        """
        return f"postgresql+{self.postgres_driver}://{self.postgres_user}:{self.postgres_password}@{self.postgres_host}:{self.postgres_port}/{self.postgres_database}"

    # @property
    # def redis_url(self) -> URL:
    #     """
    #     Assemble REDIS URL from settings.

    #     :return: redis URL.
    #     """
    #     path = f"/{self.redis_base}" if self.redis_base is not None else ""
    #     return URL.build(
    #         scheme="redis",
    #         host=self.redis_host,
    #         port=self.redis_port,
    #         user=self.redis_user,
    #         password=self.redis_pass.get_secret_value(),
    #         path=path,
    #     )

    @field_serializer(
        "discord_token",
        "openai_api_key",
        "redis_pass",
        "pinecone_api_key",
        "langchain_api_key",
        "langchain_hub_api_key",
        when_used="json",
    )
    def dump_secret(self, v):
        return v.get_secret_value()


aiosettings = AioSettings()  # sourcery skip: docstrings-for-classes, avoid-global-variables

</document_content>
</document>
<document index="80">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/__init__.py</source>
<document_content>

</document_content>
</document>
<document index="81">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/create_directory_tool.py</source>
<document_content>

</document_content>
</document>
<document index="82">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/create_file_tool.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
"""File creation tool for LangChain/LangGraph integration."""
from __future__ import annotations

import os
import pathlib

from typing import Any, Dict, Optional, Type, Union

import aiofiles
import structlog

from langchain_core.callbacks import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun
from langchain_core.tools import BaseTool


logger = structlog.get_logger(__name__)
from pydantic import BaseModel, Field

from democracy_exe.utils.file_functions import fix_path, is_file


class CreateFileInput(BaseModel):
    """Schema for file creation tool input.

    Attributes:
        file_name: Name of the file to create
        content: Content to write to the file
        directory: Optional target directory (defaults to ./scratchpad)
    """
    file_name: str = Field(description="Name of the file to create")
    content: str = Field(description="Content to write to the file")
    directory: str | None = Field(
        default="./scratchpad",
        description="Target directory for file creation"
    )


class CreateFileResponse(BaseModel):
    """Schema for file creation tool response.

    Attributes:
        file_path: Full path to the created file
        status: Status of the file creation operation
        error: Optional error message if operation failed
    """
    file_path: str = Field(description="Full path to the created file")
    status: str = Field(description="Status of the file creation operation")
    error: str | None = Field(
        default=None,
        description="Error message if operation failed"
    )


class CreateFileTool(BaseTool):
    """Tool for creating files with specified content.

    This tool handles file creation operations with proper validation and error handling.
    It supports both synchronous and asynchronous operations using aiofiles.

    Attributes:
        name: Tool name for LangChain
        description: Tool description for LLM context
        args_schema: Pydantic model for argument validation
        return_direct: Whether to return results directly to user
    """
    name: str = "file_creator"
    description: str = """Create a new file with specified content in a given directory.
    Handles file creation with proper validation and error handling."""
    args_schema: type[BaseModel] = CreateFileInput
    return_direct: bool = True

    def _validate_path(self, directory: str, file_name: str) -> tuple[pathlib.Path, str]:
        """Validate and prepare the file path.

        Args:
            directory: Target directory path
            file_name: Name of the file to create

        Returns:
            tuple[pathlib.Path, str]: Tuple of (directory path, full file path)

        Raises:
            ValueError: If path validation fails
        """
        try:
            # Normalize directory path
            dir_path = pathlib.Path(fix_path(directory)).resolve()
            logger.debug(f"Normalized directory path: {dir_path}")

            # Create full file path
            file_path = dir_path / file_name
            logger.debug(f"Full file path: {file_path}")

            # Validate directory exists
            if not dir_path.exists():
                logger.warning(f"Directory does not exist: {dir_path}")
                raise ValueError(f"Directory does not exist: {dir_path}")

            # Validate file doesn't exist
            if is_file(str(file_path)):
                logger.warning(f"File already exists: {file_path}")
                raise ValueError(f"File already exists: {file_path}")

            return dir_path, str(file_path)
        except Exception as e:
            logger.error(f"Path validation failed: {e!s}")
            raise ValueError(f"Path validation failed: {e!s}")

    def _run(
        self,
        file_name: str,
        content: str,
        directory: str = "./scratchpad",
        run_manager: CallbackManagerForToolRun | None = None
    ) -> dict[str, Any]:
        """Create a file synchronously.

        Args:
            file_name: Name of the file to create
            content: Content to write to the file
            directory: Target directory (defaults to ./scratchpad)
            run_manager: Callback manager for tool execution

        Returns:
            Dict[str, Any]: Response containing file creation status

        Raises:
            ValueError: If file creation fails
        """
        logger.info(f"Starting synchronous file creation for {file_name} in {directory}")
        try:
            # Ensure directory exists
            dir_path = pathlib.Path(fix_path(directory)).resolve()
            logger.debug(f"Creating directory if not exists: {dir_path}")
            os.makedirs(dir_path, exist_ok=True)
            logger.debug("Directory creation complete")

            # Validate paths
            logger.debug("Validating paths...")
            dir_path, file_path = self._validate_path(str(dir_path), file_name)
            logger.debug(f"Path validation successful. Directory: {dir_path}, File: {file_path}")

            # Write file content
            logger.debug(f"Writing content to file: {file_path}")
            content_length = len(content)
            logger.debug(f"Content length: {content_length} characters")

            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            logger.info(f"Successfully wrote {content_length} characters to {file_path}")

            response = CreateFileResponse(
                file_path=file_path,
                status="success"
            ).model_dump()
            logger.info("Synchronous file creation completed successfully")
            return response
        except Exception as e:
            logger.exception(f"File creation failed: {e!s}")
            error_response = CreateFileResponse(
                file_path="",
                status="error",
                error=str(e)
            ).model_dump()
            logger.error(f"Returning error response: {error_response}")
            return error_response

    async def _arun(
        self,
        file_name: str,
        content: str,
        directory: str = "./scratchpad",
        run_manager: AsyncCallbackManagerForToolRun | None = None
    ) -> dict[str, Any]:
        """Create a file asynchronously.

        Args:
            file_name: Name of the file to create
            content: Content to write to the file
            directory: Target directory (defaults to ./scratchpad)
            run_manager: Callback manager for async tool execution

        Returns:
            Dict[str, Any]: Response containing file creation status

        Raises:
            ValueError: If file creation fails
        """
        logger.info(f"Starting asynchronous file creation for {file_name} in {directory}")
        try:
            # Validate paths
            logger.debug("Validating paths...")
            dir_path, file_path = self._validate_path(directory, file_name)
            logger.debug(f"Path validation successful. Directory: {dir_path}, File: {file_path}")

            # Ensure directory exists
            logger.debug(f"Creating directory if not exists: {dir_path}")
            os.makedirs(dir_path, exist_ok=True)
            logger.debug("Directory creation/verification complete")

            # Write file content asynchronously
            logger.debug(f"Writing content to file asynchronously: {file_path}")
            content_length = len(content)
            logger.debug(f"Content length: {content_length} characters")

            async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                await f.write(content)
            logger.info(f"Successfully wrote {content_length} characters to {file_path}")

            response = CreateFileResponse(
                file_path=file_path,
                status="success"
            ).model_dump()
            logger.info("File creation completed successfully")
            return response
        except Exception as e:
            logger.exception(f"Asynchronous file creation failed: {e!s}")
            error_response = CreateFileResponse(
                file_path="",
                status="error",
                error=str(e)
            ).dict()
            logger.error(f"Returning error response: {error_response}")
            return error_response


if __name__ == "__main__":
    import asyncio

    async def main() -> None:
        """Test the file creation tool."""
        # Configure logger for testing
        # logger.add(
        #     "file_creation.log",
        #     rotation="1 MB",
        #     retention="10 days",
        #     level="DEBUG"
        # )

        tool = CreateFileTool()
        logger.info("Starting file creation tool tests")

        # Test async file creation
        logger.info("Testing async file creation...")
        result = await tool.arun({
            "file_name": "test.txt",
            "content": "Hello, World!",
            "directory": "./scratchpad"
        })
        logger.info(f"Async Result: {result}")
        print(f"Async Result: {result}")

        # Test sync file creation
        logger.info("Testing sync file creation...")
        result = tool.run({
            "file_name": "test2.txt",
            "content": "Hello again!",
            "directory": "./scratchpad"
        })
        logger.info(f"Sync Result: {result}")
        print(f"Sync Result: {result}")

        logger.info("File creation tool tests completed")

    asyncio.run(main())

</document_content>
</document>
<document index="83">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/delete_directory_tool.py</source>
<document_content>

</document_content>
</document>
<document index="84">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/delete_file_tool.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
"""File deletion tool for LangChain/LangGraph integration."""
from __future__ import annotations

import os
import pathlib

from typing import Any, Dict, Optional, Type, Union

import structlog

from langchain_core.callbacks import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun
from langchain_core.tools import BaseTool


logger = structlog.get_logger(__name__)
from pydantic import BaseModel, Field

from democracy_exe.utils.file_functions import fix_path, is_file


class DeleteFileInput(BaseModel):
    """Schema for file deletion tool input.

    Attributes:
        file_name: Name of the file to delete
        directory: Optional target directory (defaults to ./scratchpad)
        force: Whether to force deletion without confirmation
    """
    file_name: str = Field(description="Name of the file to delete")
    directory: str | None = Field(
        default="./scratchpad",
        description="Target directory containing the file"
    )
    force: bool = Field(
        default=False,
        description="Whether to force deletion without confirmation"
    )


class DeleteFileResponse(BaseModel):
    """Schema for file deletion tool response.

    Attributes:
        file_path: Full path to the deleted file
        status: Status of the file deletion operation
        error: Optional error message if operation failed
        requires_confirmation: Whether confirmation is required before deletion
    """
    file_path: str = Field(description="Full path to the deleted file")
    status: str = Field(description="Status of the file deletion operation")
    error: str | None = Field(
        default=None,
        description="Error message if operation failed"
    )
    requires_confirmation: bool = Field(
        default=False,
        description="Whether confirmation is required before deletion"
    )


class DeleteFileTool(BaseTool):
    """Tool for deleting files with proper validation and confirmation.

    This tool handles file deletion operations with proper validation and error handling.
    It supports both synchronous and asynchronous operations and includes a confirmation
    mechanism for safe deletions.

    Attributes:
        name: Tool name for LangChain
        description: Tool description for LLM context
        args_schema: Pydantic model for argument validation
        return_direct: Whether to return results directly to user
    """
    name: str = "file_deleter"
    description: str = """Delete a file from a given directory with optional force flag.
    Handles file deletion with proper validation and confirmation checks."""
    args_schema: type[BaseModel] = DeleteFileInput
    return_direct: bool = True

    def _validate_path(self, directory: str, file_name: str) -> tuple[pathlib.Path, str]:
        """Validate and prepare the file path.

        Args:
            directory: Target directory path
            file_name: Name of the file to delete

        Returns:
            tuple[pathlib.Path, str]: Tuple of (directory path, full file path)

        Raises:
            ValueError: If path validation fails
        """
        try:
            # Normalize directory path
            dir_path = pathlib.Path(fix_path(directory)).resolve()
            logger.debug(f"Normalized directory path: {dir_path}")

            # Create full file path
            file_path = dir_path / file_name
            logger.debug(f"Full file path: {file_path}")

            # Validate directory exists
            if not dir_path.exists():
                raise ValueError(f"Directory does not exist: {dir_path}")

            # Validate file exists
            if not is_file(str(file_path)):
                raise ValueError(f"File does not exist: {file_path}")

            return dir_path, str(file_path)
        except Exception as e:
            logger.error(f"Path validation failed: {e!s}")
            raise ValueError(f"Path validation failed: {e!s}")

    def _run(
        self,
        file_name: str,
        directory: str = "./scratchpad",
        force: bool = False,
        run_manager: CallbackManagerForToolRun | None = None
    ) -> dict[str, Any]:
        """Delete a file synchronously.

        Args:
            file_name: Name of the file to delete
            directory: Target directory (defaults to ./scratchpad)
            force: Whether to force deletion without confirmation
            run_manager: Callback manager for tool execution

        Returns:
            Dict[str, Any]: Response containing file deletion status

        Raises:
            ValueError: If file deletion fails
        """
        logger.info(f"Starting synchronous file deletion for {file_name} in {directory}")
        try:
            # Validate paths
            logger.debug("Validating paths...")
            dir_path, file_path = self._validate_path(directory, file_name)
            logger.debug(f"Path validation successful. Directory: {dir_path}, File: {file_path}")

            # Check if confirmation is required
            if not force:
                logger.info("Force flag not set, requiring confirmation")
                return DeleteFileResponse(
                    file_path=file_path,
                    status="confirmation_required",
                    requires_confirmation=True
                ).dict()

            # Delete the file
            logger.debug(f"Deleting file: {file_path}")
            os.remove(file_path)
            logger.info(f"Successfully deleted file: {file_path}")

            response = DeleteFileResponse(
                file_path=file_path,
                status="success"
            ).model_dump()
            logger.info("File deletion completed successfully")
            return response
        except Exception as e:
            logger.exception(f"File deletion failed: {e!s}")
            error_response = DeleteFileResponse(
                file_path="",
                status="error",
                error=str(e)
            ).dict()
            logger.error(f"Returning error response: {error_response}")
            return error_response

    async def _arun(
        self,
        file_name: str,
        directory: str = "./scratchpad",
        force: bool = False,
        run_manager: AsyncCallbackManagerForToolRun | None = None
    ) -> dict[str, Any]:
        """Delete a file asynchronously.

        Args:
            file_name: Name of the file to delete
            directory: Target directory (defaults to ./scratchpad)
            force: Whether to force deletion without confirmation
            run_manager: Callback manager for async tool execution

        Returns:
            Dict[str, Any]: Response containing file deletion status

        Raises:
            ValueError: If file deletion fails
        """
        logger.info(f"Starting asynchronous file deletion for {file_name} in {directory}")
        try:
            # Validate paths
            logger.debug("Validating paths...")
            dir_path, file_path = self._validate_path(directory, file_name)
            logger.debug(f"Path validation successful. Directory: {dir_path}, File: {file_path}")

            # Check if confirmation is required
            if not force:
                logger.info("Force flag not set, requiring confirmation")
                return DeleteFileResponse(
                    file_path=file_path,
                    status="confirmation_required",
                    requires_confirmation=True
                ).dict()

            # Delete the file
            logger.debug(f"Deleting file: {file_path}")
            os.remove(file_path)  # os.remove is fast enough to not need async
            logger.info(f"Successfully deleted file: {file_path}")

            response = DeleteFileResponse(
                file_path=file_path,
                status="success"
            ).dict()
            logger.info("Asynchronous file deletion completed successfully")
            return response
        except Exception as e:
            logger.exception(f"Asynchronous file deletion failed: {e!s}")
            error_response = DeleteFileResponse(
                file_path="",
                status="error",
                error=str(e)
            ).dict()
            logger.error(f"Returning error response: {error_response}")
            return error_response


if __name__ == "__main__":
    import asyncio

    async def main() -> None:
        """Test the file deletion tool."""
        # Configure logger for testing
        # logger.add(
        #     "file_deletion.log",
        #     rotation="1 MB",
        #     retention="10 days",
        #     level="DEBUG"
        # )

        tool = DeleteFileTool()
        logger.info("Starting file deletion tool tests")

        # Create a test file first
        test_dir = pathlib.Path("./scratchpad")
        test_dir.mkdir(exist_ok=True)
        test_file = test_dir / "test.txt"
        test_file.write_text("Test content")

        # Test deletion without force flag
        logger.info("Testing deletion without force flag...")
        result = await tool.arun({
            "file_name": "test.txt",
            "directory": "./scratchpad",
            "force": False
        })
        logger.info(f"No-force Result: {result}")
        print(f"No-force Result: {result}")

        # Test forced deletion
        logger.info("Testing forced deletion...")
        result = await tool.arun({
            "file_name": "test.txt",
            "directory": "./scratchpad",
            "force": True
        })
        logger.info(f"Force Result: {result}")
        print(f"Force Result: {result}")

        logger.info("File deletion tool tests completed")

    asyncio.run(main())

</document_content>
</document>
<document index="85">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/discuss_file_tool.py</source>
<document_content>
# @timeit_decorator
# async def discuss_file(prompt: str, model: ModelName = ModelName.base_model) -> dict:
#     """
#     Discuss a file's content based on the user's prompt, considering the current memory content.
#     """
#     scratch_pad_dir = os.getenv("SCRATCH_PAD_DIR", "./scratchpad")
#     focus_file = personalization.get("focus_file")

#     if focus_file:
#         file_path = os.path.join(scratch_pad_dir, focus_file)
#         if not os.path.exists(file_path):
#             return {"status": "Focus file not found", "file_name": focus_file}
#     else:
#         # List available files in SCRATCH_PAD_DIR
#         available_files = os.listdir(scratch_pad_dir)
#         available_files_str = ", ".join(available_files)

#         # Build the structured prompt to select the file
#         select_file_prompt = f"""
# <purpose>
#     Select a file from the available files based on the user's prompt.
# </purpose>

# <instructions>
#     <instruction>Based on the user's prompt and the list of available files, infer which file the user wants to discuss.</instruction>
#     <instruction>If no file matches, return an empty string for 'file'.</instruction>
# </instructions>

# <available-files>
#     {available_files_str}
# </available-files>

# <user-prompt>
#     {prompt}
# </user-prompt>
#         """

#         # Call the LLM to select the file
#         file_selection_response = structured_output_prompt(
#             select_file_prompt,
#             FileReadResponse,
#             llm_model=model_name_to_id[ModelName.fast_model],
#         )

#         if not file_selection_response.file:
#             return {"status": "No matching file found"}

#         file_path = os.path.join(scratch_pad_dir, file_selection_response.file)

#     # Read the content of the file
#     with open(file_path, "r") as f:
#         file_content = f.read()

#     # Get all memory content
#     memory_content = memory_manager.get_xml_for_prompt(["*"])

#     # Build the structured prompt to discuss the file content
#     discuss_file_prompt = f"""
# <purpose>
#     Discuss the content of the file based on the user's prompt and the current memory content.
# </purpose>

# <instructions>
#     <instruction>Based on the user's prompt, the file content, and the current memory content, provide a relevant discussion or analysis.</instruction>
#     <instruction>Be concise and focus on the aspects mentioned in the user's prompt.</instruction>
#     <instruction>Consider the current memory content when discussing the file, if relevant.</instruction>
#     <instruction>Keep responses short and concise. Keep response under 3 sentences for concise conversations.</instruction>
# </instructions>

# <file-content>
# {file_content}
# </file-content>

# {memory_content}

# <user-prompt>
# {prompt}
# </user-prompt>
#     """

#     # Call the LLM to discuss the file content
#     discussion = chat_prompt(discuss_file_prompt, model_name_to_id[model])

#     return {
#         "status": "File discussed",
#         "file_name": os.path.basename(file_path),
#         "discussion": discussion,
#     }

</document_content>
</document>
<document index="86">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/get_current_time_tool.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
"""Time retrieval tool for LangChain/LangGraph integration."""
from __future__ import annotations

import re

from datetime import datetime
from typing import Any, Dict, Optional, Type, Union

import structlog

from langchain_core.callbacks import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun
from langchain_core.tools import BaseTool


logger = structlog.get_logger(__name__)
from pydantic import BaseModel, Field


class GetCurrentTimeInput(BaseModel):
    """Schema for current time tool input.

    Attributes:
        format: Optional datetime format string (defaults to "%Y-%m-%d %H:%M:%S")
    """
    format: str = Field(
        default="%Y-%m-%d %H:%M:%S",
        description="Format string for datetime output"
    )


class GetCurrentTimeResponse(BaseModel):
    """Schema for current time tool response.

    Attributes:
        current_time: Formatted current time string
        timestamp: Unix timestamp
        error: Optional error message if operation failed
    """
    current_time: str = Field(description="Formatted current time string")
    timestamp: float = Field(description="Unix timestamp")
    error: str | None = Field(
        default=None,
        description="Error message if operation failed"
    )


class GetCurrentTimeTool(BaseTool):
    """Tool for retrieving the current time in various formats.

    This tool provides the current time with optional format specification.
    It supports both synchronous and asynchronous operations.

    Attributes:
        name: Tool name for LangChain
        description: Tool description for LLM context
        args_schema: Pydantic model for argument validation
        return_direct: Whether to return results directly to user
    """
    name: str = "current_time"
    description: str = """Get the current time in a specified format.
    Returns both formatted time string and Unix timestamp."""
    args_schema: type[BaseModel] = GetCurrentTimeInput
    return_direct: bool = True

    # def _get_time(self, format: str = "%Y-%m-%d %H:%M:%S") -> tuple[str, float]:
    #     """Get the current time in specified format.

    #     Args:
    #         format: Format string for datetime output

    #     Returns:
    #         tuple[str, float]: Tuple of (formatted time string, Unix timestamp)

    #     Raises:
    #         ValueError: If format string is invalid
    #     """
    #     try:
    #         # Validate format string by attempting to use it
    #         datetime.now().strftime(format)
    #     except Exception as e:
    #         logger.error(f"Error formatting time: {e!s}")
    #         raise ValueError(f"Invalid time format: {e!s}")

    #     now = datetime.now()
    #     logger.debug(f"Getting current time with format: {format}")
    #     formatted_time = now.strftime(format)
    #     timestamp = now.timestamp()
    #     logger.debug(f"Current time: {formatted_time}, Timestamp: {timestamp}")
    #     return formatted_time, timestamp


    def _get_time(self, format: str = "%Y-%m-%d %H:%M:%S") -> tuple[str, float]:
        """Get the current time in specified format.

        Args:
            format: Format string for datetime output

        Returns:
            tuple[str, float]: Tuple of (formatted time string, Unix timestamp)

        Raises:
            ValueError: If format string is invalid
        """
        logger.debug(f"Getting current time with format: {format}")

        try:
            now = datetime.now()
            # This will raise ValueError if format string is invalid
            formatted_time = now.strftime(format)
            timestamp = now.timestamp()

            logger.debug(f"Current time: {formatted_time}, Timestamp: {timestamp}")
            return formatted_time, timestamp

        except (ValueError, TypeError) as e:
            logger.error(f"Error formatting time: {e}")
            raise ValueError(f"Invalid time format: {e}")

    def _run(
        self,
        format: str = "%Y-%m-%d %H:%M:%S",
        run_manager: CallbackManagerForToolRun | None = None
    ) -> dict[str, Any]:
        """Get current time synchronously.

        Args:
            format: Format string for datetime output
            run_manager: Callback manager for tool execution

        Returns:
            Dict[str, Any]: Response containing current time information

        Raises:
            ValueError: If time formatting fails
        """
        logger.info("Getting current time synchronously")
        try:
            formatted_time, timestamp = self._get_time(format)
            response = GetCurrentTimeResponse(
                current_time=formatted_time,
                timestamp=timestamp
            ).model_dump()
            logger.info(f"Successfully retrieved current time: {formatted_time}")
            return response
        except Exception as e:
            logger.exception("Failed to get current time")
            error_response = GetCurrentTimeResponse(
                current_time="",
                timestamp=0.0,
                error=str(e)
            ).model_dump()
            logger.error(f"Returning error response: {error_response}")
            return error_response

    async def _arun(
        self,
        format: str = "%Y-%m-%d %H:%M:%S",
        run_manager: AsyncCallbackManagerForToolRun | None = None
    ) -> dict[str, Any]:
        """Get current time asynchronously.

        Args:
            format: Format string for datetime output
            run_manager: Callback manager for async tool execution

        Returns:
            Dict[str, Any]: Response containing current time information

        Raises:
            ValueError: If time formatting fails
        """
        logger.info("Getting current time asynchronously")
        try:
            formatted_time, timestamp = self._get_time(format)
            response = GetCurrentTimeResponse(
                current_time=formatted_time,
                timestamp=timestamp
            ).model_dump()
            logger.info(f"Successfully retrieved current time: {formatted_time}")
            return response
        except Exception as e:
            logger.exception("Failed to get current time")
            error_response = GetCurrentTimeResponse(
                current_time="",
                timestamp=0.0,
                error=str(e)
            ).model_dump()
            logger.error(f"Returning error response: {error_response}")
            return error_response


if __name__ == "__main__":
    import asyncio

    async def main() -> None:
        """Test the current time tool."""
        # Configure logger for testing
        # logger.add(
        #     "current_time.log",
        #     rotation="1 MB",
        #     retention="10 days",
        #     level="DEBUG"
        # )

        tool = GetCurrentTimeTool()
        logger.info("Starting current time tool tests")

        # Test default format
        logger.info("Testing default time format...")
        result = await tool.arun({})
        logger.info(f"Default format result: {result}")
        print(f"Default format result: {result}")

        # Test custom format
        logger.info("Testing custom time format...")
        result = await tool.arun({"format": "%H:%M:%S"})
        logger.info(f"Custom format result: {result}")
        print(f"Custom format result: {result}")

        # Test invalid format
        logger.info("Testing invalid time format...")
        result = await tool.arun({"format": "invalid"})
        logger.info(f"Invalid format result: {result}")
        print(f"Invalid format result: {result}")

        logger.info("Current time tool tests completed")

    asyncio.run(main())

</document_content>
</document>
<document index="87">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/get_random_number_tool.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
"""Random number generation tool for LangChain/LangGraph integration."""
from __future__ import annotations

import random

from typing import Any, Dict, Optional, Type, Union

import structlog

from langchain_core.callbacks import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun
from langchain_core.tools import BaseTool


logger = structlog.get_logger(__name__)
from pydantic import BaseModel, Field, field_validator


class GetRandomNumberInput(BaseModel):
    """Schema for random number tool input.

    Attributes:
        min_value: Minimum value (inclusive, defaults to 1)
        max_value: Maximum value (inclusive, defaults to 100)
        seed: Optional random seed for reproducibility
    """
    min_value: int = Field(
        default=1,
        description="Minimum value (inclusive)"
    )
    max_value: int = Field(
        default=100,
        description="Maximum value (inclusive)"
    )
    seed: int | None = Field(
        default=None,
        description="Random seed for reproducibility"
    )



class GetRandomNumberResponse(BaseModel):
    """Schema for random number tool response.

    Attributes:
        random_number: Generated random number
        min_value: Minimum value used
        max_value: Maximum value used
        seed: Seed used (if any)
        error: Optional error message if operation failed
    """
    random_number: int = Field(description="Generated random number")
    min_value: int = Field(description="Minimum value used")
    max_value: int = Field(description="Maximum value used")
    seed: int | None = Field(
        default=None,
        description="Seed used (if any)"
    )
    error: str | None = Field(
        default=None,
        description="Error message if operation failed"
    )


class GetRandomNumberTool(BaseTool):
    """Tool for generating random numbers within a specified range.

    This tool provides random number generation with optional seed for reproducibility.
    It supports both synchronous and asynchronous operations.

    Attributes:
        name: Tool name for LangChain
        description: Tool description for LLM context
        args_schema: Pydantic model for argument validation
        return_direct: Whether to return results directly to user
    """
    name: str = "random_number"
    description: str = """Generate a random number within a specified range.
    Supports optional seed for reproducibility."""
    args_schema: type[BaseModel] = GetRandomNumberInput
    return_direct: bool = True

    def _validate_range(self, min_value: int, max_value: int) -> None:
        """Validate the input range.

        Args:
            min_value: Minimum value to validate
            max_value: Maximum value to validate

        Raises:
            ValueError: If range is invalid
        """
        if max_value <= min_value:
            logger.error("Invalid range for random number generation")
            raise ValueError("Maximum value must be greater than minimum value")

    def _generate_number(
        self,
        min_value: int = 1,
        max_value: int = 100,
        seed: int | None = None
    ) -> int:
        """Generate a random number within the specified range.

        Args:
            min_value: Minimum value (inclusive)
            max_value: Maximum value (inclusive)
            seed: Optional random seed

        Returns:
            int: Generated random number

        Raises:
            ValueError: If range is invalid
        """
        # Validate range first
        self._validate_range(min_value, max_value)

        try:
            if seed is not None:
                logger.debug("Generating random number with seed")
                random.seed(seed)
            else:
                logger.debug("Generating random number without seed")

            number = random.randint(min_value, max_value) # noqa: S311
            logger.debug(f"Generated number: {number}")
            return number
        except Exception as e:
            logger.error(f"Error generating random number: {e!s}")
            raise ValueError(f"Failed to generate random number: {e!s}")

    def _run(
        self,
        min_value: int = 1,
        max_value: int = 100,
        seed: int | None = None,
        run_manager: CallbackManagerForToolRun | None = None
    ) -> dict[str, Any]:
        """Generate random number synchronously.

        Args:
            min_value: Minimum value (inclusive)
            max_value: Maximum value (inclusive)
            seed: Optional random seed
            run_manager: Callback manager for tool execution

        Returns:
            Dict[str, Any]: Response containing generated number

        Raises:
            ValueError: If number generation fails
        """
        logger.info("Generating random number synchronously")
        try:
            self._validate_range(min_value, max_value)
            number = self._generate_number(min_value, max_value, seed)
            response = GetRandomNumberResponse(
                random_number=number,
                min_value=min_value,
                max_value=max_value,
                seed=seed
            ).model_dump()
            logger.info(f"Successfully generated random number: {number}")
            return response
        except Exception as e:
            logger.exception("Failed to generate random number")
            error_response = GetRandomNumberResponse(
                random_number=0,
                min_value=min_value,
                max_value=max_value,
                seed=seed,
                error=str(e)
            ).model_dump()
            logger.error(f"Returning error response: {error_response}")
            return error_response

    async def _arun(
        self,
        min_value: int = 1,
        max_value: int = 100,
        seed: int | None = None,
        run_manager: AsyncCallbackManagerForToolRun | None = None
    ) -> dict[str, Any]:
        """Generate random number asynchronously.

        Args:
            min_value: Minimum value (inclusive)
            max_value: Maximum value (inclusive)
            seed: Optional random seed
            run_manager: Callback manager for async tool execution

        Returns:
            Dict[str, Any]: Response containing generated number

        Raises:
            ValueError: If number generation fails
        """
        logger.info("Generating random number asynchronously")
        try:
            self._validate_range(min_value, max_value)
            number = self._generate_number(min_value, max_value, seed)
            response = GetRandomNumberResponse(
                random_number=number,
                min_value=min_value,
                max_value=max_value,
                seed=seed
            ).model_dump()
            logger.info(f"Successfully generated random number: {number}")
            return response
        except Exception as e:
            logger.exception("Failed to generate random number")
            error_response = GetRandomNumberResponse(
                random_number=0,
                min_value=min_value,
                max_value=max_value,
                seed=seed,
                error=str(e)
            ).model_dump()
            logger.error(f"Returning error response: {error_response}")
            return error_response


if __name__ == "__main__":
    import asyncio

    async def main() -> None:
        """Test the random number tool."""
        # Configure logger for testing
        # logger.add(
        #     "random_number.log",
        #     rotation="1 MB",
        #     retention="10 days",
        #     level="DEBUG"
        # )

        tool = GetRandomNumberTool()
        logger.info("Starting random number tool tests")

        # Test default range
        logger.info("Testing default range...")
        result = await tool.arun({})
        logger.info(f"Default range result: {result}")
        print(f"Default range result: {result}")

        # Test custom range
        logger.info("Testing custom range...")
        result = await tool.arun({
            "min_value": 1000,
            "max_value": 2000
        })
        logger.info(f"Custom range result: {result}")
        print(f"Custom range result: {result}")

        # Test with seed
        logger.info("Testing with seed...")
        result = await tool.arun({
            "min_value": 1,
            "max_value": 100,
            "seed": 42
        })
        logger.info(f"Seeded result: {result}")
        print(f"Seeded result: {result}")

        # Test invalid range
        logger.info("Testing invalid range...")
        result = await tool.arun({
            "min_value": 100,
            "max_value": 1  # This will fail validation
        })
        logger.info(f"Invalid range result: {result}")
        print(f"Invalid range result: {result}")

        logger.info("Random number tool tests completed")

    asyncio.run(main())

</document_content>
</document>
<document index="88">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/ingest_file_tool.py</source>
<document_content>
# @timeit_decorator
# async def ingest_file(prompt: str) -> dict:
#     """
#     Selects a file based on the user's prompt, reads its content, and returns the file data.
#     """
#     scratch_pad_dir = os.getenv("SCRATCH_PAD_DIR", "./scratchpad")

#     # Step 1: Select the file based on the prompt
#     select_file_prompt = f"""
# <purpose>
#     Select a file from the available files based on the user's prompt.
# </purpose>

# <instructions>
#     <instruction>Based on the user's prompt and the list of available files, infer which file the user wants to ingest.</instruction>
#     <instruction>If no file matches, return an empty string for 'file'.</instruction>
# </instructions>

# <available-files>
#     {", ".join(os.listdir(scratch_pad_dir))}
# </available-files>

# <user-prompt>
#     {prompt}
# </user-prompt>
#     """

#     file_selection_response = structured_output_prompt(
#         select_file_prompt,
#         FileReadResponse,
#         llm_model=model_name_to_id[ModelName.fast_model],
#     )

#     if not file_selection_response.file:
#         return {
#             "ingested_content": None,
#             "message": "No matching file found for the given prompt.",
#             "success": False,
#         }

#     file_path = os.path.join(scratch_pad_dir, file_selection_response.file)

#     if not os.path.exists(file_path):
#         return {
#             "ingested_content": None,
#             "message": f"File '{file_selection_response.file}' does not exist in '{scratch_pad_dir}'.",
#             "success": False,
#         }

#     # Read the file content
#     try:
#         with open(file_path, "r") as f:
#             file_content = f.read()
#     except Exception as e:
#         return {
#             "ingested_content": None,
#             "message": f"Failed to read the file: {str(e)}",
#             "success": False,
#         }

#     return {
#         "ingested_content": file_content,
#         "message": "Successfully ingested content",
#         "success": True,
#     }

</document_content>
</document>
<document index="89">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/list_directory_tool.py</source>
<document_content>

</document_content>
</document>
<document index="90">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/open_browser_tool.py</source>
<document_content>

</document_content>
</document>
<document index="91">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/run_python_code_tool.py</source>
<document_content>
# @timeit_decorator
# async def run_python(prompt: str) -> dict:
#     """
#     Executes a Python script from the scratch_pad_dir based on the user's prompt.
#     Returns the output and a success or failure status.
#     """
#     scratch_pad_dir = os.getenv("SCRATCH_PAD_DIR", "./scratchpad")
#     memory_content = memory_manager.get_xml_for_prompt(["*"])

#     # Step 1: Select the file based on the prompt
#     select_file_prompt = f"""
# <purpose>
#     Select a Python file to execute based on the user's prompt.
# </purpose>

# <instructions>
#     <instruction>Based on the user's prompt and the list of available Python files, infer which file the user wants to execute.</instruction>
#     <instruction>If no file matches, return an empty string for 'file'.</instruction>
# </instructions>

# <available-files>
#     {", ".join([f for f in os.listdir(scratch_pad_dir) if f.endswith('.py')])}
# </available-files>

# <memory-content>
#     {memory_content}
# </memory-content>

# <user-prompt>
#     {prompt}
# </user-prompt>
#     """

#     file_selection_response = structured_output_prompt(
#         select_file_prompt,
#         FileReadResponse,
#         llm_model=model_name_to_id[ModelName.fast_model],
#     )

#     if not file_selection_response.file:
#         return {
#             "status": "error",
#             "message": "No matching Python file found for the given prompt.",
#         }

#     file_path = os.path.join(scratch_pad_dir, file_selection_response.file)

#     if not os.path.exists(file_path):
#         return {
#             "status": "error",
#             "message": f"File '{file_selection_response.file}' does not exist in '{scratch_pad_dir}'.",
#         }

#     # Read the Python code from the selected file
#     try:
#         with open(file_path, "r") as f:
#             python_code = f.read()
#     except Exception as e:
#         return {"status": "error", "message": f"Failed to read the file: {str(e)}"}

#     # Execute the Python code using run_uv_script
#     output = run_uv_script(python_code)

#     # Save the output to a file with '_output' suffix
#     output_file_name = os.path.splitext(file_selection_response.file)[0] + "_output.txt"
#     output_file_path = os.path.join(scratch_pad_dir, output_file_name)
#     with open(output_file_path, "w") as f:
#         f.write(output)

#     # Determine success based on presence of errors
#     if "Traceback" in output or "Error" in output:
#         success = False
#         error_message = output
#     else:
#         success = True
#         error_message = None

#     return {
#         "status": "success" if success else "failure",
#         "error": error_message,
#         "file_name": file_selection_response.file,
#         "output_file": output_file_name,
#     }

</document_content>
</document>
<document index="92">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/run_shell_command_tool.py</source>
<document_content>

</document_content>
</document>
<document index="93">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/select_file_tool.py</source>
<document_content>

</document_content>
</document>
<document index="94">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/summarize_file_tool.py</source>
<document_content>

</document_content>
</document>
<document index="95">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/twitter_tool.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
"""Twitter tool for LangChain/LangGraph integration."""
from __future__ import annotations

from typing import Any, Optional, Type, Union

from langchain_core.callbacks import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun
from langchain_core.tools import BaseTool
from pydantic import BaseModel, Field

from democracy_exe.utils.twitter_utils.download import TweetDownloadMode, adownload_tweet, download_tweet
from democracy_exe.utils.twitter_utils.models import DownloadedContent, Tweet, TweetThread
from democracy_exe.utils.twitter_utils.types import DownloadResult


class TwitterToolInput(BaseModel):
    """Schema for Twitter tool input."""
    url: str = Field(description="Twitter/X post URL to process")
    mode: str = Field(
        description="Download mode: 'single' for one tweet, 'thread' for full thread",
        default="single"
    )


class TwitterTool(BaseTool):
    """Tool for downloading and processing Twitter/X content.

    Handles downloading of tweets, threads, and associated media.
    Supports both synchronous and asynchronous operations.

    Attributes:
        name: Tool name for LangChain
        description: Tool description for LLM context
        args_schema: Pydantic model for argument validation
        return_direct: Whether to return results directly to user
    """
    name: str = "twitter_processor"
    description: str = """Process Twitter/X posts. Can download single tweets or full threads.
    Handles text content, media attachments, and Twitter cards. Returns structured content."""
    args_schema: type[BaseModel] = TwitterToolInput
    return_direct: bool = True

    def _validate_mode(self, mode: str) -> None:
        """Validate the download mode.

        Args:
            mode: Download mode to validate

        Raises:
            ValueError: If mode is invalid
        """
        valid_modes = {"single", "thread"}
        if mode not in valid_modes:
            raise ValueError(f"Invalid mode: {mode}. Must be one of {valid_modes}")

    def _run(
        self,
        url: str,
        mode: str = "single",
        run_manager: CallbackManagerForToolRun | None = None
    ) -> Tweet | TweetThread:
        """Download and process Twitter content synchronously.

        Args:
            url: Twitter/X post URL
            mode: Download mode ('single' or 'thread')
            run_manager: Callback manager for tool execution

        Returns:
            Tweet or TweetThread object containing processed content

        Raises:
            ToolException: If download fails or content is invalid
            ValueError: If mode is invalid
        """
        self._validate_mode(mode)
        try:
            result = download_tweet(url, mode=mode)
            if isinstance(result, DownloadedContent):
                if result.error:
                    raise ValueError(f"Download failed: {result.error}")
                return result.content
            return result
        except Exception as e:
            raise ValueError(f"Failed to process Twitter content: {e!s}") from e

    async def _arun(
        self,
        url: str,
        mode: str = "single",
        run_manager: AsyncCallbackManagerForToolRun | None = None
    ) -> Tweet | TweetThread:
        """Download and process Twitter content asynchronously.

        Args:
            url: Twitter/X post URL
            mode: Download mode ('single' or 'thread')
            run_manager: Callback manager for async tool execution

        Returns:
            Tweet or TweetThread object containing processed content

        Raises:
            ToolException: If download fails or content is invalid
            ValueError: If mode is invalid
        """
        self._validate_mode(mode)
        try:
            result: DownloadResult = await adownload_tweet(url, mode=mode)
            if isinstance(result, DownloadedContent):
                if result.error:
                    raise ValueError(f"Download failed: {result.error}")
                return result.content
            return result
        except Exception as e:
            raise ValueError(f"Failed to process Twitter content: {e!s}") from e


if __name__ == "__main__":
    import asyncio

    async def main() -> None:
        """Run the Twitter tool asynchronously."""
        tool = TwitterTool()
        result = await tool.arun({"url": "https://x.com/Eminitybaba_/status/1868256259251863704"})
        print(f"Result: {result}")

    asyncio.run(main())

</document_content>
</document>
<document index="96">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/tools/update_file_tool.py</source>
<document_content>
# pyright: reportAttributeAccessIssue=false
"""File update tool for LangChain/LangGraph integration."""
from __future__ import annotations

import os
import pathlib

from typing import Any, Dict, Optional, Type, Union

import aiofiles
import structlog

from langchain_core.callbacks import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun
from langchain_core.tools import BaseTool


logger = structlog.get_logger(__name__)
from pydantic import BaseModel, Field

from democracy_exe.utils.file_functions import fix_path, is_file


class UpdateFileInput(BaseModel):
    """Schema for file update tool input.

    Attributes:
        file_name: Name of the file to update
        content: New content to write to the file
        directory: Optional target directory (defaults to ./scratchpad)
    """
    file_name: str = Field(description="Name of the file to update")
    content: str = Field(description="New content to write to the file")
    directory: str | None = Field(
        default="./scratchpad",
        description="Target directory containing the file"
    )


class UpdateFileResponse(BaseModel):
    """Schema for file update tool response.

    Attributes:
        file_path: Full path to the updated file
        status: Status of the file update operation
        error: Optional error message if operation failed
    """
    file_path: str = Field(description="Full path to the updated file")
    status: str = Field(description="Status of the file update operation")
    error: str | None = Field(
        default=None,
        description="Error message if operation failed"
    )


class UpdateFileTool(BaseTool):
    """Tool for updating existing files with new content.

    This tool handles file update operations with proper validation and error handling.
    It supports both synchronous and asynchronous operations using aiofiles.

    Attributes:
        name: Tool name for LangChain
        description: Tool description for LLM context
        args_schema: Pydantic model for argument validation
        return_direct: Whether to return results directly to user
    """
    name: str = "file_updater"
    description: str = """Update an existing file with new content in a given directory.
    Handles file updates with proper validation and error handling."""
    args_schema: type[BaseModel] = UpdateFileInput
    return_direct: bool = True

    def _validate_path(self, directory: str, file_name: str) -> tuple[pathlib.Path, str]:
        """Validate and prepare the file path.

        Args:
            directory: Target directory path
            file_name: Name of the file to update

        Returns:
            tuple[pathlib.Path, str]: Tuple of (directory path, full file path)

        Raises:
            ValueError: If path validation fails
        """
        try:
            # Normalize directory path
            dir_path = pathlib.Path(fix_path(directory)).resolve()
            logger.debug(f"Normalized directory path: {dir_path}")

            # Create full file path
            file_path = dir_path / file_name
            logger.debug(f"Full file path: {file_path}")

            # Validate directory exists
            if not dir_path.exists():
                raise ValueError(f"Directory does not exist: {dir_path}")

            # Validate file exists
            if not is_file(str(file_path)):
                raise ValueError(f"File does not exist: {file_path}")

            return dir_path, str(file_path)
        except Exception as e:
            logger.error(f"Path validation failed: {e!s}")
            raise ValueError(f"Path validation failed: {e!s}")

    def _run(
        self,
        file_name: str,
        content: str,
        directory: str = "./scratchpad",
        run_manager: CallbackManagerForToolRun | None = None
    ) -> dict[str, Any]:
        """Update a file synchronously.

        Args:
            file_name: Name of the file to update
            content: New content to write to the file
            directory: Target directory (defaults to ./scratchpad)
            run_manager: Callback manager for tool execution

        Returns:
            Dict[str, Any]: Response containing file update status

        Raises:
            ValueError: If file update fails
        """
        logger.info(f"Starting synchronous file update for {file_name} in {directory}")
        try:
            # Validate paths
            logger.debug("Validating paths...")
            dir_path, file_path = self._validate_path(directory, file_name)
            logger.debug(f"Path validation successful. Directory: {dir_path}, File: {file_path}")

            # Read existing content for backup/logging
            logger.debug(f"Reading existing content from: {file_path}")
            with open(file_path, encoding='utf-8') as f:
                old_content = f.read()
            logger.debug(f"Original content length: {len(old_content)} characters")

            # Write new content
            logger.debug(f"Writing new content to file: {file_path}")
            content_length = len(content)
            logger.debug(f"New content length: {content_length} characters")

            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            logger.info(f"Successfully wrote {content_length} characters to {file_path}")

            response = UpdateFileResponse(
                file_path=file_path,
                status="success"
            ).model_dump()
            logger.info("File update completed successfully")
            return response
        except Exception as e:
            logger.exception(f"File update failed: {e!s}")
            error_response = UpdateFileResponse(
                file_path="",
                status="error",
                error=str(e)
            ).model_dump()
            logger.error(f"Returning error response: {error_response}")
            return error_response

    async def _arun(
        self,
        file_name: str,
        content: str,
        directory: str = "./scratchpad",
        run_manager: AsyncCallbackManagerForToolRun | None = None
    ) -> dict[str, Any]:
        """Update a file asynchronously.

        Args:
            file_name: Name of the file to update
            content: New content to write to the file
            directory: Target directory (defaults to ./scratchpad)
            run_manager: Callback manager for async tool execution

        Returns:
            Dict[str, Any]: Response containing file update status

        Raises:
            ValueError: If file update fails
        """
        logger.info(f"Starting asynchronous file update for {file_name} in {directory}")
        try:
            # Validate paths
            logger.debug("Validating paths...")
            dir_path, file_path = self._validate_path(directory, file_name)
            logger.debug(f"Path validation successful. Directory: {dir_path}, File: {file_path}")

            # Read existing content for backup/logging
            logger.debug(f"Reading existing content from: {file_path}")
            async with aiofiles.open(file_path, encoding='utf-8') as f:
                old_content = await f.read()
            logger.debug(f"Original content length: {len(old_content)} characters")

            # Write new content asynchronously
            logger.debug(f"Writing new content to file asynchronously: {file_path}")
            content_length = len(content)
            logger.debug(f"New content length: {content_length} characters")

            async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                await f.write(content)
            logger.info(f"Successfully wrote {content_length} characters to {file_path}")

            response = UpdateFileResponse(
                file_path=file_path,
                status="success"
            ).model_dump()
            logger.info("Asynchronous file update completed successfully")
            return response
        except Exception as e:
            logger.exception(f"Asynchronous file update failed: {e!s}")
            error_response = UpdateFileResponse(
                file_path="",
                status="error",
                error=str(e)
            ).model_dump()
            logger.error(f"Returning error response: {error_response}")
            return error_response


if __name__ == "__main__":
    import asyncio

    async def main() -> None:
        """Test the file update tool."""
        # Configure logger for testing
        logger.add(
            "file_update.log",
            rotation="1 MB",
            retention="10 days",
            level="DEBUG"
        )

        tool = UpdateFileTool()
        logger.info("Starting file update tool tests")

        # Create a test file first
        test_dir = pathlib.Path("./scratchpad")
        test_dir.mkdir(exist_ok=True)
        test_file = test_dir / "test.txt"
        test_file.write_text("Original content")

        # Test async file update
        logger.info("Testing async file update...")
        result = await tool.arun({
            "file_name": "test.txt",
            "content": "Updated content!",
            "directory": "./scratchpad"
        })
        logger.info(f"Async Result: {result}")
        print(f"Async Result: {result}")

        # Test sync file update
        logger.info("Testing sync file update...")
        result = tool.run({
            "file_name": "test.txt",
            "content": "Updated again!",
            "directory": "./scratchpad"
        })
        logger.info(f"Sync Result: {result}")
        print(f"Sync Result: {result}")

        logger.info("File update tool tests completed")

    asyncio.run(main())

</document_content>
</document>
<document index="97">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/workflows/__init__.py</source>
<document_content>

</document_content>
</document>
<document index="98">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/workflows/react/__init__.py</source>
<document_content>

</document_content>
</document>
<document index="99">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/workflows/react/_constants.py</source>
<document_content>
from __future__ import annotations


PAYLOAD_KEY = "content"
PATH_KEY = "path"
PATCH_PATH = "user/{user_id}/core"
INSERT_PATH = "user/{user_id}/recall/{event_id}"
TIMESTAMP_KEY = "timestamp"
TYPE_KEY = "type"

</document_content>
</document>
<document index="100">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/workflows/react/_schemas.py</source>
<document_content>
from __future__ import annotations

from typing import Annotated, List

from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing_extensions import TypedDict


# SOURCE: lang-memgpt
class GraphConfig(TypedDict):
    model: str | None
    """The model to use for the memory assistant."""
    thread_id: str
    """The thread ID of the conversation."""
    user_id: str
    """The ID of the user to remember in the conversation."""

# SOURCE: lang-memgpt
# Define the schema for the state maintained throughout the conversation
class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
    """The messages in the conversation."""
    core_memories: list[str]
    """The core memories associated with the user."""
    recall_memories: list[str]
    """The recall memories retrieved for the current context."""


__all__ = [
    "State",
    "GraphConfig",
]

</document_content>
</document>
<document index="101">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/workflows/react/_utils.py</source>
<document_content>
# pyright: reportUninitializedInstanceVariable=false
# pyright: reportUndefinedVariable=false
# pyright: reportAttributeAccessIssue=false

from __future__ import annotations

import time
import uuid

from functools import lru_cache

import langsmith
import structlog

from langchain.chat_models import init_chat_model
from langchain_core.embeddings import Embeddings
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage
from langchain_core.runnables import RunnableConfig


logger = structlog.get_logger(__name__)
from pinecone import Pinecone, ServerlessSpec

import democracy_exe.agentic.workflows.react._schemas as schemas

from democracy_exe.aio_settings import aiosettings


def get_message_text(msg: BaseMessage) -> str:
    """Get the text content of a message."""
    content = msg.content
    if isinstance(content, str):
        return content
    elif isinstance(content, dict):
        return content.get("text", "")
    else:
        txts = [c if isinstance(c, str) else (c.get("text") or "") for c in content]
        return "".join(txts).strip()


def load_chat_model(fully_specified_name: str) -> BaseChatModel:
    """Load a chat model from a fully specified name.

    Args:
        fully_specified_name (str): String in the format 'provider/model'.
    """
    provider, model = fully_specified_name.split("/", maxsplit=1)
    return init_chat_model(model, model_provider=provider)


_DEFAULT_DELAY = 60  # seconds


def get_fake_thread_id(user_id: int = 1) -> str:
    """Generate a deterministic UUID for a thread based on user ID.

    Args:
        user_id (int): The user ID to generate a thread ID for. Defaults to 1.

    Returns:
        str: A UUID v5 string generated from the user ID.
    """
    namespace: uuid.UUID = uuid.NAMESPACE_DNS  # You can choose a different namespace if appropriate
    name: str = f"USER:{user_id}"
    generated_uuid: uuid.UUID = uuid.uuid5(namespace, name)
    logger.info(f"namespace: {namespace}")
    logger.info(f"name: {name}")
    logger.info(f"Generated fake thread ID: {generated_uuid}")
    return str(generated_uuid)

def get_index() -> Pinecone.Index:
    """Get a Pinecone index instance using settings from aiosettings.

    Returns:
        Pinecone.Index: A configured Pinecone index instance.
    """
    pc: Pinecone = get_or_create_index()
    # pc: Pinecone = Pinecone(api_key=aiosettings.pinecone_api_key.get_secret_value()) # pylint: disable=no-member
    # pc: Pinecone = Pinecone(api_key=aiosettings.pinecone_api_key.get_secret_value(), environment=aiosettings.pinecone_env) # pylint: disable=no-member
    return pc



def get_or_create_index() -> Pinecone.Index:
    """Get or create a Pinecone index instance using settings from aiosettings.

    This function checks if the index exists, creates it if it doesn't, and returns
    the index instance. It waits for the index to be ready before returning.

    Returns:
        Pinecone.Index: A configured Pinecone index instance.

    Note:
        If the index doesn't exist, it will be created with dimension=3072 and
        metric="cosine" in the us-east-1 region.
    """
    pc: Pinecone = Pinecone(api_key=aiosettings.pinecone_api_key.get_secret_value()) # pylint: disable=no-member
    index_name: str = aiosettings.pinecone_index_name

    existing_indexes: list[str] = [index_info["name"] for index_info in pc.list_indexes()]

    logger.info(f"Existing indexes: {existing_indexes}")

    if index_name not in existing_indexes:
        logger.info(f"Creating index: {index_name} with dimension=3072 and metric=cosine in us-east-1")
        pc.create_index(
            name=index_name,
            dimension=3072,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1"),
        )
        while not pc.describe_index(index_name).status["ready"]:
            time.sleep(1)

    return pc.Index(index_name)



@langsmith.traceable
def ensure_configurable(config: RunnableConfig) -> schemas.GraphConfig:
    """Merge the user-provided config with default values.

    Args:
        config (RunnableConfig): The configuration object containing user settings.

    Returns:
        schemas.GraphConfig: A merged configuration containing both user-provided and default values.

    Note:
        If chatbot_type is "terminal", it will generate a fake thread_id and user_id.
        Otherwise, it will use the provided discord configuration.
    """
    if aiosettings.chatbot_type == "terminal":
        user_id: int = 1
        thread_id: str = get_fake_thread_id(user_id=user_id)

        configurable: dict[str, str | int] = config.get("configurable", {"thread_id": thread_id, "user_id": user_id})
        logger.info(f"Using terminal config: {configurable}")
    else:
        configurable: dict = config.get("configurable", {})
        logger.info(f"Using discord config: {configurable}")

    return {
        **configurable,
        **schemas.GraphConfig(
            delay=configurable.get("delay", _DEFAULT_DELAY),
            model=configurable.get("model", "gpt-4o"),
            thread_id=configurable["thread_id"],
            user_id=configurable["user_id"],
        ),
    }


@lru_cache
def get_embeddings(model_name: str = "nomic-ai/nomic-embed-text-v1.5") -> Embeddings:
    if model_name == "nomic-ai/nomic-embed-text-v1.5":
        from langchain_fireworks import FireworksEmbeddings
        return FireworksEmbeddings(model="nomic-ai/nomic-embed-text-v1.5")
    elif model_name == "text-embedding-3-large":
        from langchain_openai import OpenAIEmbeddings
        return OpenAIEmbeddings(model="text-embedding-3-large")
    from langchain_fireworks import FireworksEmbeddings
    return FireworksEmbeddings(model=model_name)


# NOTE: via memory-agent
def split_model_and_provider(fully_specified_name: str) -> dict:
    """Initialize the configured chat model."""
    if "/" in fully_specified_name:
        provider, model = fully_specified_name.split("/", maxsplit=1)
    else:
        provider = None
        model = fully_specified_name
    return {"model": model, "provider": provider}


def make_text_encoder(model: str) -> Embeddings:
    """Connect to the configured text encoder."""
    provider, model = model.split("/", maxsplit=1)
    match provider:
        case "openai":
            from langchain_openai import OpenAIEmbeddings

            return OpenAIEmbeddings(model=model)
        # case "cohere":
        #     from langchain_cohere import CohereEmbeddings

        #     return CohereEmbeddings(model=model)  # type: ignore
        case _:
            raise ValueError(f"Unsupported embedding provider: {provider}")

def make_chat_model(name: str) -> BaseChatModel:
    """Connect to the configured chat model."""
    provider, model = name.split("/", maxsplit=1)
    return init_chat_model(model, model_provider=provider, temperature=0.0) # pyright: ignore[reportUndefinedVariable]

__all__ = ["ensure_configurable"]

</document_content>
</document>
<document index="102">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/workflows/react/configuration.py</source>
<document_content>
from __future__ import annotations

import os

from dataclasses import dataclass, field, fields
from typing import Annotated, Any, Optional, Type, TypeVar

import rich
import structlog

from langchain_core.runnables import RunnableConfig, ensure_config


logger = structlog.get_logger(__name__)
from rich.pretty import pprint

from democracy_exe.agentic.studio.react import prompts


def _update_configurable_for_backwards_compatibility(
    configurable: dict[str, Any],
) -> dict[str, Any]:
    update = {}
    # if "k" in configurable:
    #     update["search_kwargs"] = {"k": configurable["k"]}

    if "model" in configurable:
        logger.error(f"Pre Configurable - model: {configurable['model']}")
        update["model"] = "openai/gpt-4o"
        logger.error(f"Post Configurable - model: {update['model']}")

    if "delay" in configurable:
        logger.error(f"Pre Configurable - delay: {configurable['delay']}")
        update["delay"] = 60
        logger.error(f"Post Configurable - delay: {update['delay']}")

    if "system_prompt" in configurable:
        logger.error(f"Pre Configurable - system_prompt: {configurable['system_prompt']}")
        update["system_prompt"] = prompts.MODEL_SYSTEM_MESSAGE
        logger.error(f"Post Configurable - system_prompt: {update['system_prompt']}")
    if update:
        return {**configurable, **update}

    return configurable

@dataclass(kw_only=True)
class Configuration:
    """The configurable fields for the chatbot."""
    user_id: str = "default-user"

    """The ID of the user to remember in the conversation."""
    model: Annotated[str, {"__template_metadata__": {"kind": "llm"}}] = field(
        # default="anthropic/claude-3-5-sonnet-20240620",
        default="openai/gpt-4o",
        metadata={
            "description": "The name of the language model to use for the agent. "
            "Should be in the form: provider/model-name."
        },
    )
    # for debounding memory creation
    delay: int = 60

    system_prompt: str = prompts.MODEL_SYSTEM_MESSAGE
    # trustcall_instruction: str = prompts.TRUSTCALL_INSTRUCTION

    @classmethod
    def from_runnable_config(
        cls: type[T], config: RunnableConfig | None | None = None
    ) -> Configuration:
        """Create a Configuration instance from a RunnableConfig."""
        config = ensure_config(config)
        configurable = config.get("configurable") or {}
        # logger.error(f"Config - config: {config}")
        # configurable = (
        #     config["configurable"] if config and "configurable" in config else {}
        # )
        configurable = _update_configurable_for_backwards_compatibility(configurable)
        rich.print("configurable:")
        pprint(configurable)
        # logger.error(f"Configurable - configurable after _update_configurable_for_backwards_compatibility: {configurable}")
        values: dict[str, Any] = {
            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))
            for f in fields(cls)
            if f.init
        }
        # logger.error(f"Values - values: {values}")
        return cls(**{k: v for k, v in values.items() if v})

T = TypeVar("T", bound=Configuration)

if __name__ == "__main__":  # pragma: no cover
    import rich
    config = {}
    configurable = Configuration.from_runnable_config(config)
    rich.print(f"configurable: {configurable}")

</document_content>
</document>
<document index="103">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/workflows/react/graph.py</source>
<document_content>
"""React Agent with Long-Term Memory.

This module implements a React agent with long-term memory capabilities using LangChain and LangGraph.
It manages user profiles, todo lists, and custom instructions through a state graph architecture.
"""
# pyright: reportUninitializedInstanceVariable=false
# pyright: reportUndefinedVariable=false
# pyright: reportAttributeAccessIssue=false
# pyright: reportInvalidTypeForm=false

from __future__ import annotations

import asyncio
import json
import logging
import uuid

from datetime import UTC, datetime, timezone
from typing import Any, Dict, List, Literal, Optional, Tuple, TypedDict, Union

import langsmith
import rich

# from democracy_exe.aio_settings import aiosettings
import structlog
import tiktoken

from langchain.chat_models import init_chat_model
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, merge_message_runs
from langchain_core.messages.utils import get_buffer_string
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable, RunnableLambda
from langchain_core.runnables.config import RunnableConfig, ensure_config, get_executor_for_config
from langchain_core.tools import tool
from langchain_core.tracers.schemas import Run
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, START, MessagesState, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt import ToolNode
from langgraph.store.base import BaseStore
from langgraph.store.memory import InMemoryStore


logger = structlog.get_logger(__name__)


import structlog


logger = structlog.get_logger(__name__)
from pydantic import BaseModel, Field
from trustcall import create_extractor
from trustcall._base import ExtractionOutputs, InputsLike

from democracy_exe.agentic.workflows.react import _utils, configuration
from democracy_exe.agentic.workflows.react.prompts import (
    CREATE_INSTRUCTIONS,
    MODEL_SYSTEM_MESSAGE,
    TRUSTCALL_INSTRUCTION,
)


# if aiosettings.debug_langgraph_studio:
#     print("DEBUG_LANGGRAPH_STUDIO is True")
#     logger.remove()
#     from democracy_exe.bot_logger import get_logger, global_log_config
#     from langchain.globals import set_debug, set_verbose
#     # Setting the global debug flag will cause all LangChain components with callback support (chains, models, agents, tools, retrievers) to print the inputs they receive and outputs they generate. This is the most verbose setting and will fully log raw inputs and outputs.
#     set_debug(True)
#     # Setting the verbose flag will print out inputs and outputs in a slightly more readable format and will skip logging certain raw outputs (like the token usage stats for an LLM call) so that you can focus on application logic.
#     set_verbose(True)

#     # SOURCE: https://github.com/Delgan/loguru/blob/420704041797daf804b505e5220805528fe26408/docs/resources/recipes.rst#L1083
#     global_log_config(
#         log_level=logging.getLevelName("DEBUG"),
#         json=False,
#     )

# logger.remove()


# import nest_asyncio

# nest_asyncio.apply()  # Required for Jupyter Notebook to run async functions


class Memory(BaseModel):
    """A single memory entry containing user-related information.

    Attributes:
        content (str): The main content of the memory (e.g., "User expressed interest in learning about French")
    """
    content: str = Field(description="The main content of the memory. For example: User expressed interest in learning about French.")

class MemoryCollection(BaseModel):
    """A collection of memories about the user.

    Attributes:
        memories (list[Memory]): A list of Memory objects containing user-related information
    """
    memories: list[Memory] = Field(description="A list of memories about the user.")


## Utilities

# SOURCE: https://github.com/langchain-ai/langchain-academy/blob/main/module-5/memory_agent.ipynb
# Visibility into Trustcall updates
# Trustcall creates and updates JSON schemas.
# What if we want visibility into the specific changes made by Trustcall?
# For example, we saw before that Trustcall has some of its own tools to:
# Self-correct from validation failures -- see trace example here
# Update existing documents -- see trace example here
# Visibility into these tools can be useful for the agent we're going to build.
# Below, we'll show how to do this!


# ---------------------------------------------------------------------------------------
# We can add a listener to the Trustcall extractor.
# This will pass runs from the extractor's execution to a class, Spy, that we will define.
# Our Spy class will extract information about what tool calls were made by Trustcall.
# ---------------------------------------------------------------------------------------
# Inspect the tool calls for Trustcall
class Spy:
    """A class to monitor and collect tool calls made by the Trustcall extractor.

    This class acts as a listener for the Trustcall extractor's execution runs,
    collecting information about what tool calls were made during execution.

    Attributes:
        called_tools (list): A list to store tool calls made during execution.
    """

    def __init__(self) -> None:
        """Initialize the Spy with an empty list of called tools."""
        self.called_tools: list = []

    def __call__(self, run: Any) -> None:
        """Process a run and extract tool calls from chat model outputs.

        Traverses the run tree and collects tool calls from chat model outputs.

        Args:
            run: The run object containing execution information.
        """
        rich.print(f"Spy: {run}")
        rich.print(f"Spy type: {type(run)}")
        q: list = [run]
        while q:
            r = q.pop()
            if r.child_runs:
                q.extend(r.child_runs)
            if r.run_type == "chat_model":
                self.called_tools.append(
                    r.outputs["generation"][0][0]["message"]["kwargs"]["tool_calls"]
                )

# DISABLED: from module-5
# # Extract information from tool calls for both patches and new memories in Trustcall
# def extract_tool_info(tool_calls: list[list[dict[str, Any]]], schema_name: str = "Memory") -> str:
#     """Extract information from tool calls for both patches and new memories.

#     This function processes tool calls to extract information about document updates
#     and new memory creation. It formats the extracted information into a human-readable
#     string.

#     Args:
#         tool_calls: List of tool call groups, where each group contains tool call
#             dictionaries with information about patches or new memory creation
#         schema_name: Name of the schema tool (e.g., "Memory", "ToDo", "Profile")

#     Returns:
#         A formatted string containing information about all document updates and
#         new memory creations
#     """
#     # Initialize list of changes
#     changes: list[dict[str, Any]] = []

#     for call_group in tool_calls:
#         for call in call_group:
#             if call['name'] == 'PatchDoc':
#                 changes.append({
#                     'type': 'update',
#                     'doc_id': call['args']['json_doc_id'],
#                     'planned_edits': call['args']['planned_edits'],
#                     'value': call['args']['patches'][0]['value']
#                 })
#             elif call['name'] == schema_name:
#                 changes.append({
#                     'type': 'new',
#                     'value': call['args']
#                 })

#     # Format results as a single string
#     result_parts: list[str] = []
#     for change in changes:
#         if change['type'] == 'update':
#             result_parts.append(
#                 f"Document {change['doc_id']} updated:\n"
#                 f"Plan: {change['planned_edits']}\n"
#                 f"Added content: {change['value']}"
#             )
#         else:
#             result_parts.append(
#                 f"New {schema_name} created:\n"
#                 f"Content: {change['value']}"
#             )

#     return "\n\n".join(result_parts)


# module-6
# Extract information from tool calls for both patches and new memories in Trustcall
def extract_tool_info(tool_calls: list[list[dict[str, Any]]], schema_name: str = "Memory") -> str:
    """Extract information from tool calls for both patches and new memories.

    Args:
        tool_calls: List of tool calls from the model
        schema_name: Name of the schema tool (e.g., "Memory", "ToDo", "Profile")
    """
    # Initialize list of changes
    changes = []

    for call_group in tool_calls:
        for call in call_group:
            if call['name'] == 'PatchDoc':
                # Check if there are any patches
                if call['args']['patches']:
                    changes.append({
                        'type': 'update',
                        'doc_id': call['args']['json_doc_id'],
                        'planned_edits': call['args']['planned_edits'],
                        'value': call['args']['patches'][0]['value']
                    })
                else:
                    # Handle case where no changes were needed
                    changes.append({
                        'type': 'no_update',
                        'doc_id': call['args']['json_doc_id'],
                        'planned_edits': call['args']['planned_edits']
                    })
            elif call['name'] == schema_name:
                changes.append({
                    'type': 'new',
                    'value': call['args']
                })

    # Format results as a single string
    result_parts = []
    for change in changes:
        if change['type'] == 'update':
            result_parts.append(
                f"Document {change['doc_id']} updated:\n"
                f"Plan: {change['planned_edits']}\n"
                f"Added content: {change['value']}"
            )
        elif change['type'] == 'no_update':
            result_parts.append(
                f"Document {change['doc_id']} unchanged:\n"
                f"{change['planned_edits']}"
            )
        else:
            result_parts.append(
                f"New {schema_name} created:\n"
                f"Content: {change['value']}"
            )

    return "\n\n".join(result_parts)

## Schema definitions

# Creating an agent
# There are many different agent architectures to choose from.

# Here, we'll implement something simple, a ReAct agent.

# This agent will be a helpful companion for creating and managing a ToDo list.

# This agent can make a decision to update three types of long-term memory:

# (a) Create or update a user profile with general user information

# (b) Add or update items in a ToDo list collection

# (c) Update its own instructions on how to update items to the ToDo list

# User profile schema
class Profile(BaseModel):
    """This is the profile of the user you are chatting with"""
    name: str | None = Field(description="The user's name", default=None)
    location: str | None = Field(description="The user's location", default=None)
    job: str | None = Field(description="The user's job", default=None)
    connections: list[str] = Field(
        description="Personal connection of the user, such as family members, friends, or coworkers",
        default_factory=list
    )
    interests: list[str] = Field(
        description="Interests that the user has",
        default_factory=list
    )

# ToDo schema
class ToDo(BaseModel):
    task: str = Field(description="The task to be completed.")
    time_to_complete: int | None = Field(description="Estimated time to complete the task (minutes).")
    deadline: datetime | None = Field(
        description="When the task needs to be completed by (if applicable)",
        default=None
    )
    solutions: list[str] = Field(
        description="List of specific, actionable solutions (e.g., specific ideas, service providers, or concrete options relevant to completing the task)",
        min_items=1,
        default_factory=list
    )
    status: Literal["not started", "in progress", "done", "archived"] = Field(
        description="Current status of the task",
        default="not started"
    )

## Initialize the model and tools

# Update memory tool
class UpdateMemory(TypedDict):
    """Decision on what memory type to update.

    Attributes:
        update_type (Literal['user', 'todo', 'instructions']): The type of memory to update
    """
    update_type: Literal['user', 'todo', 'instructions']

# Initialize the model

# model = ChatOpenAI(model="gpt-4o", temperature=0)
# model: BaseChatModel = init_chat_model("gpt-4o", model_provider=aiosettings.llm_provider, temperature=0.0) # pyright: ignore[reportUndefinedVariable]
# TODO: Use this to get embeddings
# tokenizer = tiktoken.encoding_for_model("gpt-4o")

# -----------------------------------------------------------------------------------
# async nodes
# -----------------------------------------------------------------------------------




## Node definitions

async def aio_tasks_democracy_ai(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[BaseMessage]]:
    """Load memories from the store and use them to personalize the chatbot's response.

    This function retrieves user profile, todo list, and custom instructions from the store
    and uses them to generate a personalized chatbot response.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing the list of messages with the chatbot's response
        Format: {"messages": [response]}
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )

    # Retrieve profile memory from the store
    namespace = ("profile", user_id)
    # DISABLED: # memories = store.search(namespace)

    memories = store.search(namespace)
    if memories:
        user_profile = memories[0].value
    else:
        user_profile = None

    # Retrieve people memory from the store
    namespace = ("todo", user_id)
    memories = store.search(namespace)
    todo = "\n".join(f"{mem.value}" for mem in memories)

    # Retrieve custom instructions
    namespace = ("instructions", user_id)
    memories = store.search(namespace)
    if memories:
        instructions = memories[0].value
    else:
        instructions = ""

    # system_msg = configurable.system_prompt.format(user_profile=user_profile, todo=todo, instructions=instructions)
    system_msg = MODEL_SYSTEM_MESSAGE.format(user_profile=user_profile, todo=todo, instructions=instructions)
    rich.print(f"system_msg: {system_msg}")
    # ---------------------------------------------------------------------------------------
    # Note: Passing the config through explicitly is required for python < 3.11
    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
    # ---------------------------------------------------------------------------------------

    # Respond using memory as well as the chat history
    # response = await model.bind_tools([UpdateMemory], parallel_tool_calls=False).ainvoke([SystemMessage(content=system_msg)]+state["messages"])

    # # Respond using memory as well as the chat history
    # response = model.bind_tools([UpdateMemory], parallel_tool_calls=False).invoke([SystemMessage(content=system_msg)]+state["messages"])

    # Respond using memory as well as the chat history
    response = await model.bind_tools([UpdateMemory], parallel_tool_calls=False).ainvoke([SystemMessage(content=system_msg)]+state["messages"], config=config)


    return {"messages": [response]}

async def aio_update_profile(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[dict[str, str]]]:
    """Reflect on the chat history and update the user profile in memory.

    This function processes the chat history to extract and update user profile information
    in the store using the Trustcall extractor.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing a tool message confirming the profile update
        Format: {
            "messages": [{
                "role": "tool",
                "content": "updated profile",
                "tool_call_id": str
            }]
        }
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )


    # Define the namespace for the memories
    namespace: tuple[str, str] = ("profile", user_id)

    # Retrieve the most recent memories for context
    existing_items = store.search(namespace)

    # Format the existing memories for the Trustcall extractor
    tool_name: str = "Profile"
    existing_memories: list[tuple[str, str, Any]] | None = (
        [(existing_item.key, tool_name, existing_item.value)
         for existing_item in existing_items]
        if existing_items
        else None
    )

    # Merge the chat history and the instruction
    trustcall_instruction_formatted: str = TRUSTCALL_INSTRUCTION.format(
        time=datetime.now().isoformat()
    )
    updated_messages: list[BaseMessage] = list(
        merge_message_runs(
            messages=[SystemMessage(content=trustcall_instruction_formatted)] + state["messages"][:-1]
        )
    )

    # ---------------------------------------------------------------------------------------
    # Note: Passing the config through explicitly is required for python < 3.11
    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
    # response = await model.ainvoke(messages, config)
    # Invoke the extractor
    # ---------------------------------------------------------------------------------------
    # # Invoke the extractor
    # result = profile_extractor.invoke({
    #     "messages": updated_messages,
    #     "existing": existing_memories
    # })

    # Invoke the extractor
    result = await profile_extractor.ainvoke({
        "messages": updated_messages,
        "existing": existing_memories
    }, config=config)

    # Save the memories from Trustcall to the store
    for r, rmeta in zip(result["responses"], result["response_metadata"], strict=False):
        store.put(
            namespace,
            rmeta.get("json_doc_id", str(uuid.uuid4())),
            r.model_dump(mode="json"),
        )

    tool_calls = state['messages'][-1].tool_calls


    # Return tool message with update verification
    return {
        "messages": [{
            "role": "tool",
            "content": "updated profile",
            "tool_call_id": tool_calls[0]['id']
        }]
    }

async def aio_update_todos(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[dict[str, str]]]:
    """Reflect on the chat history and update the todo list in memory.

    This function processes the chat history to extract and update todo items
    in the store using the Trustcall extractor. It also tracks changes made
    using a Spy instance.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing a tool message with update details
        Format: {
            "messages": [{
                "role": "tool",
                "content": str,  # Contains details of updates made
                "tool_call_id": str
            }]
        }
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )


    # Define the namespace for the memories
    namespace: tuple[str, str] = ("todo", user_id)

    # Retrieve the most recent memories for context
    existing_items = store.search(namespace)

    # Format the existing memories for the Trustcall extractor
    tool_name: str = "ToDo"
    existing_memories: list[tuple[str, str, Any]] | None = (
        [(existing_item.key, tool_name, existing_item.value)
         for existing_item in existing_items]
        if existing_items
        else None
    )

    # Merge the chat history and the instruction
    trustcall_instruction_formatted: str = TRUSTCALL_INSTRUCTION.format(
        time=datetime.now().isoformat()
    )
    updated_messages: list[BaseMessage] = list(
        merge_message_runs(
            messages=[SystemMessage(content=trustcall_instruction_formatted)] + state["messages"][:-1]
        )
    )

    # Initialize the spy for visibility into the tool calls made by Trustcall
    spy: Spy = Spy()

    # Create the Trustcall extractor for updating the ToDo list
    todo_extractor: Runnable[InputsLike, ExtractionOutputs] = create_extractor(
        model,
        tools=[ToDo],
        tool_choice=tool_name,
        enable_inserts=True
    ).with_listeners(on_end=spy)

    # # Invoke the extractor
    # result = todo_extractor.invoke({
    #     "messages": updated_messages,
    #     "existing": existing_memories
    # })

    # Note: Passing the config through explicitly is required for python < 3.11
    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
    # response = await model.ainvoke(messages, config)
    # Invoke the extractor
    result = await todo_extractor.ainvoke({
        "messages": updated_messages,
        "existing": existing_memories
    }, config=config)

    # Save the memories from Trustcall to the store
    for r, rmeta in zip(result["responses"], result["response_metadata"], strict=False):
        store.put(
            namespace,
            rmeta.get("json_doc_id", str(uuid.uuid4())),
            r.model_dump(mode="json"),
        )

    # Respond to the tool call made in tasks_democracy_ai, confirming the update
    tool_calls = state['messages'][-1].tool_calls

    # Extract the changes made by Trustcall and add to the ToolMessage returned to tasks_democracy_ai
    todo_update_msg: str = extract_tool_info(spy.called_tools, tool_name)
    return {
        "messages": [{
            "role": "tool",
            "content": todo_update_msg,
            "tool_call_id": tool_calls[0]['id']
        }]
    }

async def aio_update_instructions(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[dict[str, str]]]:
    """Reflect on the chat history and update the instructions in memory.

    This function processes the chat history to extract and update user-specified
    preferences for managing the todo list. It stores these instructions for future
    reference.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing a tool message confirming the instructions update
        Format: {
            "messages": [{
                "role": "tool",
                "content": "updated instructions",
                "tool_call_id": str
            }]
        }
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )

    namespace: tuple[str, str] = ("instructions", user_id)

    existing_memory = store.get(namespace, "user_instructions")

    # Format the memory in the system prompt
    system_msg: str = CREATE_INSTRUCTIONS.format(
        current_instructions=existing_memory.value if existing_memory else None
    )

    # # # Respond using memory as well as the chat history
    # new_memory: BaseMessage = model.invoke(
    #     [SystemMessage(content=system_msg)] +
    #     state['messages'][:-1] +
    #     [HumanMessage(content="Please update the instructions based on the conversation")]
    # )
    # # Respond using memory as well as the chat history
    new_memory: BaseMessage = await model.ainvoke(
        [SystemMessage(content=system_msg)] +
        state['messages'][:-1] +
        [HumanMessage(content="Please update the instructions based on the conversation")],
        config=config
    )

    # Overwrite the existing memory in the store
    key: str = "user_instructions"
    store.put(namespace, key, {"memory": new_memory.content})

    tool_calls = state['messages'][-1].tool_calls

    # Return tool message with update verification
    return {
        "messages": [{
            "role": "tool",
            "content": "updated instructions",
            "tool_call_id": tool_calls[0]['id']
        }]
    }


# -----------------------------------------------------------------------------------
# sync nodes
# -----------------------------------------------------------------------------------




## Node definitions

def tasks_democracy_ai(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[BaseMessage]]:
    """Load memories from the store and use them to personalize the chatbot's response.

    This function retrieves user profile, todo list, and custom instructions from the store
    and uses them to generate a personalized chatbot response.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing the list of messages with the chatbot's response
        Format: {"messages": [response]}
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )

    # Retrieve profile memory from the store
    namespace = ("profile", user_id)
    # DISABLED: # memories = store.search(namespace)

    memories = store.search(namespace)
    if memories:
        user_profile = memories[0].value
    else:
        user_profile = None

    # Retrieve people memory from the store
    namespace = ("todo", user_id)
    memories = store.search(namespace)
    todo = "\n".join(f"{mem.value}" for mem in memories)

    # Retrieve custom instructions
    namespace = ("instructions", user_id)
    memories = store.search(namespace)
    if memories:
        instructions = memories[0].value
    else:
        instructions = ""

    # system_msg = configurable.system_prompt.format(user_profile=user_profile, todo=todo, instructions=instructions)
    system_msg = MODEL_SYSTEM_MESSAGE.format(user_profile=user_profile, todo=todo, instructions=instructions)

    # ---------------------------------------------------------------------------------------
    # Note: Passing the config through explicitly is required for python < 3.11
    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
    # ---------------------------------------------------------------------------------------

    # Respond using memory as well as the chat history
    # response = await model.bind_tools([UpdateMemory], parallel_tool_calls=False).ainvoke([SystemMessage(content=system_msg)]+state["messages"])

    # # Respond using memory as well as the chat history
    response = model.bind_tools([UpdateMemory], parallel_tool_calls=False).invoke([SystemMessage(content=system_msg)]+state["messages"], config=config)

    # Respond using memory as well as the chat history
    # response = await model.bind_tools([UpdateMemory], parallel_tool_calls=False).ainvoke([SystemMessage(content=system_msg)]+state["messages"], config=config)

    return {"messages": [response]}

def update_profile(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[dict[str, str]]]:
    """Reflect on the chat history and update the user profile in memory.

    This function processes the chat history to extract and update user profile information
    in the store using the Trustcall extractor.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing a tool message confirming the profile update
        Format: {
            "messages": [{
                "role": "tool",
                "content": "updated profile",
                "tool_call_id": str
            }]
        }
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )


    # Define the namespace for the memories
    namespace: tuple[str, str] = ("profile", user_id)

    # Retrieve the most recent memories for context
    existing_items = store.search(namespace)

    # Format the existing memories for the Trustcall extractor
    tool_name: str = "Profile"
    existing_memories: list[tuple[str, str, Any]] | None = (
        [(existing_item.key, tool_name, existing_item.value)
         for existing_item in existing_items]
        if existing_items
        else None
    )

    # Merge the chat history and the instruction
    trustcall_instruction_formatted: str = TRUSTCALL_INSTRUCTION.format(
        time=datetime.now().isoformat()
    )
    updated_messages: list[BaseMessage] = list(
        merge_message_runs(
            messages=[SystemMessage(content=trustcall_instruction_formatted)] + state["messages"][:-1]
        )
    )

    # ---------------------------------------------------------------------------------------
    # Note: Passing the config through explicitly is required for python < 3.11
    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
    # response = await model.ainvoke(messages, config)
    # Invoke the extractor
    # ---------------------------------------------------------------------------------------
    # # Invoke the extractor
    result = profile_extractor.invoke({
        "messages": updated_messages,
        "existing": existing_memories
    }, config=config)

    # # Invoke the extractor
    # result = await profile_extractor.ainvoke({
    #     "messages": updated_messages,
    #     "existing": existing_memories
    # }, config=config)

    # Save the memories from Trustcall to the store
    for r, rmeta in zip(result["responses"], result["response_metadata"], strict=False):
        store.put(
            namespace,
            rmeta.get("json_doc_id", str(uuid.uuid4())),
            r.model_dump(mode="json"),
        )

    tool_calls = state['messages'][-1].tool_calls
    # Return tool message with update verification
    return {
        "messages": [{
            "role": "tool",
            "content": "updated profile",
            "tool_call_id": tool_calls[0]['id']
        }]
    }

def update_todos(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[dict[str, str]]]:
    """Reflect on the chat history and update the todo list in memory.

    This function processes the chat history to extract and update todo items
    in the store using the Trustcall extractor. It also tracks changes made
    using a Spy instance.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing a tool message with update details
        Format: {
            "messages": [{
                "role": "tool",
                "content": str,  # Contains details of updates made
                "tool_call_id": str
            }]
        }
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )


    # Define the namespace for the memories
    namespace: tuple[str, str] = ("todo", user_id)

    # Retrieve the most recent memories for context
    existing_items = store.search(namespace)

    # Format the existing memories for the Trustcall extractor
    tool_name: str = "ToDo"
    existing_memories: list[tuple[str, str, Any]] | None = (
        [(existing_item.key, tool_name, existing_item.value)
         for existing_item in existing_items]
        if existing_items
        else None
    )

    # Merge the chat history and the instruction
    trustcall_instruction_formatted: str = TRUSTCALL_INSTRUCTION.format(
        time=datetime.now().isoformat()
    )
    updated_messages: list[BaseMessage] = list(
        merge_message_runs(
            messages=[SystemMessage(content=trustcall_instruction_formatted)] + state["messages"][:-1]
        )
    )

    # Initialize the spy for visibility into the tool calls made by Trustcall
    spy: Spy = Spy()

    # Create the Trustcall extractor for updating the ToDo list
    todo_extractor: Runnable[InputsLike, ExtractionOutputs] = create_extractor(
        model,
        tools=[ToDo],
        tool_choice=tool_name,
        enable_inserts=True
    ).with_listeners(on_end=spy)

    # Invoke the extractor
    result = todo_extractor.invoke({
        "messages": updated_messages,
        "existing": existing_memories
    }, config=config)

    # Note: Passing the config through explicitly is required for python < 3.11
    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
    # response = await model.ainvoke(messages, config)
    # Invoke the extractor
    # result = await todo_extractor.ainvoke({
    #     "messages": updated_messages,
    #     "existing": existing_memories
    # }, config=config)

    # Save the memories from Trustcall to the store
    for r, rmeta in zip(result["responses"], result["response_metadata"], strict=False):
        store.put(
            namespace,
            rmeta.get("json_doc_id", str(uuid.uuid4())),
            r.model_dump(mode="json"),
        )

    # Respond to the tool call made in tasks_democracy_ai, confirming the update
    tool_calls = state['messages'][-1].tool_calls

    # Extract the changes made by Trustcall and add to the ToolMessage returned to tasks_democracy_ai
    todo_update_msg: str = extract_tool_info(spy.called_tools, tool_name)
    return {
        "messages": [{
            "role": "tool",
            "content": todo_update_msg,
            "tool_call_id": tool_calls[0]['id']
        }]
    }

def update_instructions(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> dict[str, list[dict[str, str]]]:
    """Reflect on the chat history and update the instructions in memory.

    This function processes the chat history to extract and update user-specified
    preferences for managing the todo list. It stores these instructions for future
    reference.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Dict containing a tool message confirming the instructions update
        Format: {
            "messages": [{
                "role": "tool",
                "content": "updated instructions",
                "tool_call_id": str
            }]
        }
    """
    # Get the user ID from the config
    configurable = configuration.Configuration.from_runnable_config(config)
    user_id = configurable.user_id
    model = _utils.make_chat_model(configurable.model) # pylint: disable=no-member

    ## Create the Trustcall extractors for updating the user profile and ToDo list
    profile_extractor = create_extractor(
        model,
        tools=[Profile],
        tool_choice="Profile",
    )

    namespace: tuple[str, str] = ("instructions", user_id)

    existing_memory = store.get(namespace, "user_instructions")

    # Format the memory in the system prompt
    system_msg: str = CREATE_INSTRUCTIONS.format(
        current_instructions=existing_memory.value if existing_memory else None
    )

    # # # Respond using memory as well as the chat history
    new_memory: BaseMessage = model.invoke(
        [SystemMessage(content=system_msg)] +
        state['messages'][:-1] +
        [HumanMessage(content="Please update the instructions based on the conversation")],
        config=config
    )
    # # Respond using memory as well as the chat history
    # new_memory: BaseMessage = await model.ainvoke(
    #     [SystemMessage(content=system_msg)] +
    #     state['messages'][:-1] +
    #     [HumanMessage(content="Please update the instructions based on the conversation")],
    #     config=config
    # )

    # Overwrite the existing memory in the store
    key: str = "user_instructions"
    store.put(namespace, key, {"memory": new_memory.content})

    tool_calls = state['messages'][-1].tool_calls

    # Return tool message with update verification
    return {
        "messages": [{
            "role": "tool",
            "content": "updated instructions",
            "tool_call_id": tool_calls[0]['id']
        }]
    }

# Conditional edge
def route_message(
    state: MessagesState,
    config: RunnableConfig,
    store: BaseStore
) -> Literal[END, "update_todos", "update_instructions", "update_profile"]:
    """Route messages to appropriate memory update functions based on tool call type.

    This function examines the latest message in the state and determines which memory
    update function should handle it based on the tool call's update_type.

    Args:
        state: Current message state containing chat history
        config: Configuration object containing user settings and preferences
        store: Storage interface for accessing and managing memories

    Returns:
        Literal indicating which node should process the message next:
        - END: No tool calls present
        - "update_todos": Route to todo list update
        - "update_instructions": Route to instructions update
        - "update_profile": Route to profile update

    Raises:
        ValueError: If the tool call's update_type is not recognized
    """
    message = state['messages'][-1]
    if len(message.tool_calls) == 0:
        return END
    else:
        tool_call = message.tool_calls[0]
        if tool_call['args']['update_type'] == "user":
            return "update_profile"
        elif tool_call['args']['update_type'] == "todo":
            return "update_todos"
        elif tool_call['args']['update_type'] == "instructions":
            return "update_instructions"
        else:
            raise ValueError("Unknown update_type in tool call")

# SOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/create-react-agent-memory.ipynb
def print_stream(stream):
    for s in stream:
        message = s["messages"][-1]
        if isinstance(message, tuple):
            print(message)
        else:
            message.pretty_print()

# Create the graph + all nodes
builder = StateGraph(MessagesState, config_schema=configuration.Configuration)

# Define the flow of the memory extraction process
builder.add_node(tasks_democracy_ai)
builder.add_node(update_todos)
builder.add_node(update_profile)
builder.add_node(update_instructions)

# Define the flow
builder.add_edge(START, "tasks_democracy_ai")
builder.add_conditional_edges("tasks_democracy_ai", route_message)
builder.add_edge("update_todos", "tasks_democracy_ai")
builder.add_edge("update_profile", "tasks_democracy_ai")
builder.add_edge("update_instructions", "tasks_democracy_ai")


# Store for long-term (across-thread) memory
across_thread_memory = InMemoryStore()

# Checkpointer for short-term (within-thread) memory
within_thread_memory = MemorySaver()

# Compile the graph
# graph: CompiledStateGraph = builder.compile()
graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory, debug=True)
graph.name = "DemocracyExeAI"

print(graph.get_graph().print_ascii())

if __name__ == "__main__":  # pragma: no cover
    import rich
    # config = {}
    # configurable = Configuration.from_runnable_config(config)
    # rich.print(f"configurable: {configurable}")
    # We supply a thread ID for short-term (within-thread) memory
    # We supply a user ID for long-term (across-thread) memory
    config = {"configurable": {"thread_id": "1", "user_id": "1"}}

    # User input
    input_messages = [HumanMessage(content="Hi, my name is Heron and I like apple pie")]

    # Run the graph
    for chunk in graph.stream({"messages": input_messages}, config, stream_mode="values"):
        chunk["messages"][-1].pretty_print()

</document_content>
</document>
<document index="104">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/workflows/react/langgraph.json</source>
<document_content>
{
    "dockerfile_lines": [],
    "graphs": {
      "agent": "./graph.py:graph"
    },
    "env": "./.env",
    "python_version": "3.11",
    "dependencies": [
      "."
    ]
  }

</document_content>
</document>
<document index="105">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/workflows/react/prompts.py</source>
<document_content>
"""Define default prompts."""


## Prompts

# Chatbot instruction for choosing what to update and what tools to call
from __future__ import annotations


MODEL_SYSTEM_MESSAGE = """You are a helpful chatbot named DemocracyExeAI.

You are designed to be a companion to a user, helping them keep track of their ToDo list.

You have a long term memory which keeps track of three things:
1. The user's profile (general information about them)
2. The user's ToDo list
3. General instructions for updating the ToDo list

Here is the current User Profile (may be empty if no information has been collected yet):
<user_profile>
{user_profile}
</user_profile>

Here is the current ToDo List (may be empty if no tasks have been added yet):
<todo>
{todo}
</todo>

Here are the current user-specified preferences for updating the ToDo list (may be empty if no preferences have been specified yet):
<instructions>
{instructions}
</instructions>

Here are your instructions for reasoning about the user's messages:

1. Reason carefully about the user's messages as presented below.

2. Decide whether any of the your long-term memory should be updated:
- If personal information was provided about the user, update the user's profile by calling UpdateMemory tool with type `user`
- If tasks are mentioned, update the ToDo list by calling UpdateMemory tool with type `todo`
- If the user has specified preferences for how to update the ToDo list, update the instructions by calling UpdateMemory tool with type `instructions`

3. Tell the user that you have updated your memory, if appropriate:
- Do not tell the user you have updated the user's profile
- Tell the user them when you update the todo list
- Do not tell the user that you have updated instructions

4. Err on the side of updating the todo list. No need to ask for explicit permission.

5. Respond naturally to user user after a tool call was made to save memories, or if no tool call was made."""

# Trustcall instruction
TRUSTCALL_INSTRUCTION = """Reflect on following interaction.

Use the provided tools to retain any necessary memories about the user.

Use parallel tool calling to handle updates and insertions simultaneously.

System Time: {time}"""

# Instructions for updating the ToDo list
CREATE_INSTRUCTIONS = """Reflect on the following interaction.

Based on this interaction, update your instructions for how to update ToDo list items. Use any feedback from the user to update how they like to have items added, etc.

Your current instructions are:

<current_instructions>
{current_instructions}
</current_instructions>"""

</document_content>
</document>
<document index="106">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/workflows/react/react_agent.ipynb</source>
<document_content>
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Load environment variables from .env file for API access\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-19 18:09:14.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpre_update\u001b[0m:\u001b[36m827\u001b[0m - \u001b[1mllm_model_name: None\u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpre_update\u001b[0m:\u001b[36m828\u001b[0m - \u001b[1mllm_embedding_model_name: None\u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpre_update\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1msetting default llm_model_name: gpt-4o-mini\u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpre_update\u001b[0m:\u001b[36m841\u001b[0m - \u001b[1msetting default llm_embedding_model_name: text-embedding-3-large\u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpost_root\u001b[0m:\u001b[36m856\u001b[0m - \u001b[1mbefore redis_path: \u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpost_root\u001b[0m:\u001b[36m857\u001b[0m - \u001b[1mbefore redis_pass: None\u001b[0m\n",
      "\u001b[32m2024-11-19 18:09:14.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msettings\u001b[0m:\u001b[36mpost_root\u001b[0m:\u001b[36m858\u001b[0m - \u001b[1mbefore redis_user: None\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               +-----------+                                           \n",
      "                                               | __start__ |                                           \n",
      "                                               +-----------+                                           \n",
      "                                                      *                                                \n",
      "                                                      *                                                \n",
      "                                                      *                                                \n",
      "                                          +--------------------+                                       \n",
      "                                          | tasks_democracy_ai |                                       \n",
      "                                    ******+--------------------+........                               \n",
      "                             *******            **           ...        .......                        \n",
      "                      *******                 **                ...            ........                \n",
      "                  ****                       *                     ..                  ....            \n",
      "+---------------------+           +----------------+           +--------------+           +---------+  \n",
      "| update_instructions |           | update_profile |           | update_todos |           | __end__ |  \n",
      "+---------------------+           +----------------+           +--------------+           +---------+  \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"React Agent with Long-Term Memory.\n",
    "\n",
    "This module implements a React agent with long-term memory capabilities using LangChain and LangGraph.\n",
    "It manages user profiles, todo lists, and custom instructions through a state graph architecture.\n",
    "\"\"\"\n",
    "# pyright: reportUninitializedInstanceVariable=false\n",
    "# pyright: reportUndefinedVariable=false\n",
    "# pyright: reportAttributeAccessIssue=false\n",
    "# pyright: reportInvalidTypeForm=false\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "from datetime import UTC, datetime, timezone\n",
    "from typing import Any, Dict, List, Literal, Optional, Tuple, TypedDict, Union\n",
    "\n",
    "import configuration\n",
    "import langsmith\n",
    "import rich\n",
    "import tiktoken\n",
    "\n",
    "from _utils import get_message_text, load_chat_model\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, merge_message_runs\n",
    "from langchain_core.messages.utils import get_buffer_string\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableLambda\n",
    "from langchain_core.runnables.config import RunnableConfig, ensure_config, get_executor_for_config\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.tracers.schemas import Run\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from loguru import logger\n",
    "from pydantic import BaseModel, Field\n",
    "from settings import aiosettings\n",
    "from trustcall import create_extractor\n",
    "from trustcall._base import ExtractionOutputs, InputsLike\n",
    "\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    \"\"\"A single memory entry containing user-related information.\n",
    "\n",
    "    Attributes:\n",
    "        content (str): The main content of the memory (e.g., \"User expressed interest in learning about French\")\n",
    "    \"\"\"\n",
    "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    \"\"\"A collection of memories about the user.\n",
    "\n",
    "    Attributes:\n",
    "        memories (list[Memory]): A list of Memory objects containing user-related information\n",
    "    \"\"\"\n",
    "    memories: list[Memory] = Field(description=\"A list of memories about the user.\")\n",
    "\n",
    "\n",
    "## Utilities\n",
    "\n",
    "# SOURCE: https://github.com/langchain-ai/langchain-academy/blob/main/module-5/memory_agent.ipynb\n",
    "# Visibility into Trustcall updates\n",
    "# Trustcall creates and updates JSON schemas.\n",
    "# What if we want visibility into the specific changes made by Trustcall?\n",
    "# For example, we saw before that Trustcall has some of its own tools to:\n",
    "# Self-correct from validation failures -- see trace example here\n",
    "# Update existing documents -- see trace example here\n",
    "# Visibility into these tools can be useful for the agent we're going to build.\n",
    "# Below, we'll show how to do this!\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# We can add a listener to the Trustcall extractor.\n",
    "# This will pass runs from the extractor's execution to a class, Spy, that we will define.\n",
    "# Our Spy class will extract information about what tool calls were made by Trustcall.\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# Inspect the tool calls for Trustcall\n",
    "class Spy:\n",
    "    \"\"\"A class to monitor and collect tool calls made by the Trustcall extractor.\n",
    "\n",
    "    This class acts as a listener for the Trustcall extractor's execution runs,\n",
    "    collecting information about what tool calls were made during execution.\n",
    "\n",
    "    Attributes:\n",
    "        called_tools (list): A list to store tool calls made during execution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the Spy with an empty list of called tools.\"\"\"\n",
    "        self.called_tools: list = []\n",
    "\n",
    "    def __call__(self, run: Any) -> None:\n",
    "        \"\"\"Process a run and extract tool calls from chat model outputs.\n",
    "\n",
    "        Traverses the run tree and collects tool calls from chat model outputs.\n",
    "\n",
    "        Args:\n",
    "            run: The run object containing execution information.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Spy: {run}\")\n",
    "        logger.info(f\"Spy type: {type(run)}\")\n",
    "        q: list = [run]\n",
    "        while q:\n",
    "            r = q.pop()\n",
    "            if r.child_runs:\n",
    "                q.extend(r.child_runs)\n",
    "            if r.run_type == \"chat_model\":\n",
    "                self.called_tools.append(\n",
    "                    r.outputs[\"generation\"][0][0][\"message\"][\"kwargs\"][\"tool_calls\"]\n",
    "                )\n",
    "\n",
    "# Extract information from tool calls for both patches and new memories in Trustcall\n",
    "def extract_tool_info(tool_calls: list[list[dict[str, Any]]], schema_name: str = \"Memory\") -> str:\n",
    "    \"\"\"Extract information from tool calls for both patches and new memories.\n",
    "\n",
    "    This function processes tool calls to extract information about document updates\n",
    "    and new memory creation. It formats the extracted information into a human-readable\n",
    "    string.\n",
    "\n",
    "    Args:\n",
    "        tool_calls: List of tool call groups, where each group contains tool call\n",
    "            dictionaries with information about patches or new memory creation\n",
    "        schema_name: Name of the schema tool (e.g., \"Memory\", \"ToDo\", \"Profile\")\n",
    "\n",
    "    Returns:\n",
    "        A formatted string containing information about all document updates and\n",
    "        new memory creations\n",
    "    \"\"\"\n",
    "    # Initialize list of changes\n",
    "    changes: list[dict[str, Any]] = []\n",
    "\n",
    "    for call_group in tool_calls:\n",
    "        for call in call_group:\n",
    "            if call['name'] == 'PatchDoc':\n",
    "                changes.append({\n",
    "                    'type': 'update',\n",
    "                    'doc_id': call['args']['json_doc_id'],\n",
    "                    'planned_edits': call['args']['planned_edits'],\n",
    "                    'value': call['args']['patches'][0]['value']\n",
    "                })\n",
    "            elif call['name'] == schema_name:\n",
    "                changes.append({\n",
    "                    'type': 'new',\n",
    "                    'value': call['args']\n",
    "                })\n",
    "\n",
    "    # Format results as a single string\n",
    "    result_parts: list[str] = []\n",
    "    for change in changes:\n",
    "        if change['type'] == 'update':\n",
    "            result_parts.append(\n",
    "                f\"Document {change['doc_id']} updated:\\n\"\n",
    "                f\"Plan: {change['planned_edits']}\\n\"\n",
    "                f\"Added content: {change['value']}\"\n",
    "            )\n",
    "        else:\n",
    "            result_parts.append(\n",
    "                f\"New {schema_name} created:\\n\"\n",
    "                f\"Content: {change['value']}\"\n",
    "            )\n",
    "\n",
    "    return \"\\n\\n\".join(result_parts)\n",
    "\n",
    "## Schema definitions\n",
    "\n",
    "# Creating an agent\n",
    "# There are many different agent architectures to choose from.\n",
    "\n",
    "# Here, we'll implement something simple, a ReAct agent.\n",
    "\n",
    "# This agent will be a helpful companion for creating and managing a ToDo list.\n",
    "\n",
    "# This agent can make a decision to update three types of long-term memory:\n",
    "\n",
    "# (a) Create or update a user profile with general user information\n",
    "\n",
    "# (b) Add or update items in a ToDo list collection\n",
    "\n",
    "# (c) Update its own instructions on how to update items to the ToDo list\n",
    "\n",
    "# User profile schema\n",
    "class Profile(BaseModel):\n",
    "    \"\"\"This is the profile of the user you are chatting with\"\"\"\n",
    "    name: str | None = Field(description=\"The user's name\", default=None)\n",
    "    location: str | None = Field(description=\"The user's location\", default=None)\n",
    "    job: str | None = Field(description=\"The user's job\", default=None)\n",
    "    connections: list[str] = Field(\n",
    "        description=\"Personal connection of the user, such as family members, friends, or coworkers\",\n",
    "        default_factory=list\n",
    "    )\n",
    "    interests: list[str] = Field(\n",
    "        description=\"Interests that the user has\",\n",
    "        default_factory=list\n",
    "    )\n",
    "\n",
    "# ToDo schema\n",
    "class ToDo(BaseModel):\n",
    "    task: str = Field(description=\"The task to be completed.\")\n",
    "    time_to_complete: int | None = Field(description=\"Estimated time to complete the task (minutes).\")\n",
    "    deadline: datetime | None = Field(\n",
    "        description=\"When the task needs to be completed by (if applicable)\",\n",
    "        default=None\n",
    "    )\n",
    "    solutions: list[str] = Field(\n",
    "        description=\"List of specific, actionable solutions (e.g., specific ideas, service providers, or concrete options relevant to completing the task)\",\n",
    "        min_items=1,\n",
    "        default_factory=list\n",
    "    )\n",
    "    status: Literal[\"not started\", \"in progress\", \"done\", \"archived\"] = Field(\n",
    "        description=\"Current status of the task\",\n",
    "        default=\"not started\"\n",
    "    )\n",
    "\n",
    "## Initialize the model and tools\n",
    "\n",
    "# Update memory tool\n",
    "class UpdateMemory(TypedDict):\n",
    "    \"\"\"Decision on what memory type to update.\n",
    "\n",
    "    Attributes:\n",
    "        update_type (Literal['user', 'todo', 'instructions']): The type of memory to update\n",
    "    \"\"\"\n",
    "    update_type: Literal['user', 'todo', 'instructions']\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "model: BaseChatModel = init_chat_model(\"gpt-4o\", model_provider=aiosettings.llm_provider, temperature=0.0) # pyright: ignore[reportUndefinedVariable]\n",
    "# TODO: Use this to get embeddings\n",
    "# tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "## Create the Trustcall extractors for updating the user profile and ToDo list\n",
    "profile_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Profile],\n",
    "    tool_choice=\"Profile\",\n",
    ")\n",
    "\n",
    "## Prompts\n",
    "\n",
    "# Chatbot instruction for choosing what to update and what tools to call\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful chatbot.\n",
    "\n",
    "You are designed to be a companion to a user, helping them keep track of their ToDo list.\n",
    "\n",
    "You have a long term memory which keeps track of three things:\n",
    "1. The user's profile (general information about them)\n",
    "2. The user's ToDo list\n",
    "3. General instructions for updating the ToDo list\n",
    "\n",
    "Here is the current User Profile (may be empty if no information has been collected yet):\n",
    "<user_profile>\n",
    "{user_profile}\n",
    "</user_profile>\n",
    "\n",
    "Here is the current ToDo List (may be empty if no tasks have been added yet):\n",
    "<todo>\n",
    "{todo}\n",
    "</todo>\n",
    "\n",
    "Here are the current user-specified preferences for updating the ToDo list (may be empty if no preferences have been specified yet):\n",
    "<instructions>\n",
    "{instructions}\n",
    "</instructions>\n",
    "\n",
    "Here are your instructions for reasoning about the user's messages:\n",
    "\n",
    "1. Reason carefully about the user's messages as presented below.\n",
    "\n",
    "2. Decide whether any of the your long-term memory should be updated:\n",
    "- If personal information was provided about the user, update the user's profile by calling UpdateMemory tool with type `user`\n",
    "- If tasks are mentioned, update the ToDo list by calling UpdateMemory tool with type `todo`\n",
    "- If the user has specified preferences for how to update the ToDo list, update the instructions by calling UpdateMemory tool with type `instructions`\n",
    "\n",
    "3. Tell the user that you have updated your memory, if appropriate:\n",
    "- Do not tell the user you have updated the user's profile\n",
    "- Tell the user them when you update the todo list\n",
    "- Do not tell the user that you have updated instructions\n",
    "\n",
    "4. Err on the side of updating the todo list. No need to ask for explicit permission.\n",
    "\n",
    "5. Respond naturally to user user after a tool call was made to save memories, or if no tool call was made.\"\"\"\n",
    "\n",
    "# Trustcall instruction\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"Reflect on following interaction.\n",
    "\n",
    "Use the provided tools to retain any necessary memories about the user.\n",
    "\n",
    "Use parallel tool calling to handle updates and insertions simultaneously.\n",
    "\n",
    "System Time: {time}\"\"\"\n",
    "\n",
    "# Instructions for updating the ToDo list\n",
    "CREATE_INSTRUCTIONS = \"\"\"Reflect on the following interaction.\n",
    "\n",
    "Based on this interaction, update your instructions for how to update ToDo list items. Use any feedback from the user to update how they like to have items added, etc.\n",
    "\n",
    "Your current instructions are:\n",
    "\n",
    "<current_instructions>\n",
    "{current_instructions}\n",
    "</current_instructions>\"\"\"\n",
    "\n",
    "## Node definitions\n",
    "\n",
    "\n",
    "async def call_model(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> dict[str, list[BaseMessage]]:\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\n",
    "\n",
    "    This function retrieves user profile, todo list, and custom instructions from the store\n",
    "    and uses them to generate a personalized chatbot response.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Dict containing the list of messages with the chatbot's response\n",
    "        Format: {\"messages\": [response]}\n",
    "    \"\"\"\n",
    "    # Get the user ID from the config\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    user_id = configurable.user_id\n",
    "\n",
    "    # Retrieve profile memory from the store\n",
    "    namespace = (\"profile\", user_id)\n",
    "    # DISABLED: # memories = store.search(namespace)\n",
    "\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        user_profile = memories[0].value\n",
    "    else:\n",
    "        user_profile = None\n",
    "\n",
    "    # Retrieve people memory from the store\n",
    "    namespace = (\"todo\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    todo = \"\\n\".join(f\"{mem.value}\" for mem in memories)\n",
    "\n",
    "    # Retrieve custom instructions\n",
    "    namespace = (\"instructions\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        instructions = memories[0].value\n",
    "    else:\n",
    "        instructions = \"\"\n",
    "\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(user_profile=user_profile, todo=todo, instructions=instructions)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = await model.bind_tools([UpdateMemory], parallel_tool_calls=False).ainvoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "async def tasks_democracy_ai(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> dict[str, list[BaseMessage]]:\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\n",
    "\n",
    "    This function retrieves user profile, todo list, and custom instructions from the store\n",
    "    and uses them to generate a personalized chatbot response.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Dict containing the list of messages with the chatbot's response\n",
    "        Format: {\"messages\": [response]}\n",
    "    \"\"\"\n",
    "    # Get the user ID from the config\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    user_id = configurable.user_id\n",
    "\n",
    "    # Retrieve profile memory from the store\n",
    "    namespace = (\"profile\", user_id)\n",
    "    # DISABLED: # memories = store.search(namespace)\n",
    "\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        user_profile = memories[0].value\n",
    "    else:\n",
    "        user_profile = None\n",
    "\n",
    "    # Retrieve people memory from the store\n",
    "    namespace = (\"todo\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    todo = \"\\n\".join(f\"{mem.value}\" for mem in memories)\n",
    "\n",
    "    # Retrieve custom instructions\n",
    "    namespace = (\"instructions\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        instructions = memories[0].value\n",
    "    else:\n",
    "        instructions = \"\"\n",
    "\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(user_profile=user_profile, todo=todo, instructions=instructions)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = await model.bind_tools([UpdateMemory], parallel_tool_calls=False).ainvoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def update_profile(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> dict[str, list[dict[str, str]]]:\n",
    "    \"\"\"Reflect on the chat history and update the user profile in memory.\n",
    "\n",
    "    This function processes the chat history to extract and update user profile information\n",
    "    in the store using the Trustcall extractor.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Dict containing a tool message confirming the profile update\n",
    "        Format: {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"updated profile\",\n",
    "                \"tool_call_id\": str\n",
    "            }]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Get the user ID from the config\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    user_id = configurable.user_id\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace: tuple[str, str] = (\"profile\", user_id)\n",
    "\n",
    "    # Retrieve the most recent memories for context\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # Format the existing memories for the Trustcall extractor\n",
    "    tool_name: str = \"Profile\"\n",
    "    existing_memories: list[tuple[str, str, Any]] | None = (\n",
    "        [(existing_item.key, tool_name, existing_item.value)\n",
    "         for existing_item in existing_items]\n",
    "        if existing_items\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Merge the chat history and the instruction\n",
    "    trustcall_instruction_formatted: str = TRUSTCALL_INSTRUCTION.format(\n",
    "        time=datetime.now().isoformat()\n",
    "    )\n",
    "    updated_messages: list[BaseMessage] = list(\n",
    "        merge_message_runs(\n",
    "            messages=[SystemMessage(content=trustcall_instruction_formatted)] + state[\"messages\"][:-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Invoke the extractor\n",
    "    result = profile_extractor.invoke({\n",
    "        \"messages\": updated_messages,\n",
    "        \"existing\": existing_memories\n",
    "    })\n",
    "\n",
    "    # Save the memories from Trustcall to the store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"], strict=False):\n",
    "        store.put(\n",
    "            namespace,\n",
    "            rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "            r.model_dump(mode=\"json\"),\n",
    "        )\n",
    "\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "    # Return tool message with update verification\n",
    "    return {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"updated profile\",\n",
    "            \"tool_call_id\": tool_calls[0]['id']\n",
    "        }]\n",
    "    }\n",
    "\n",
    "def update_todos(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> dict[str, list[dict[str, str]]]:\n",
    "    \"\"\"Reflect on the chat history and update the todo list in memory.\n",
    "\n",
    "    This function processes the chat history to extract and update todo items\n",
    "    in the store using the Trustcall extractor. It also tracks changes made\n",
    "    using a Spy instance.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Dict containing a tool message with update details\n",
    "        Format: {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": str,  # Contains details of updates made\n",
    "                \"tool_call_id\": str\n",
    "            }]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Get the user ID from the config\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    user_id = configurable.user_id\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace: tuple[str, str] = (\"todo\", user_id)\n",
    "\n",
    "    # Retrieve the most recent memories for context\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # Format the existing memories for the Trustcall extractor\n",
    "    tool_name: str = \"ToDo\"\n",
    "    existing_memories: list[tuple[str, str, Any]] | None = (\n",
    "        [(existing_item.key, tool_name, existing_item.value)\n",
    "         for existing_item in existing_items]\n",
    "        if existing_items\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Merge the chat history and the instruction\n",
    "    trustcall_instruction_formatted: str = TRUSTCALL_INSTRUCTION.format(\n",
    "        time=datetime.now().isoformat()\n",
    "    )\n",
    "    updated_messages: list[BaseMessage] = list(\n",
    "        merge_message_runs(\n",
    "            messages=[SystemMessage(content=trustcall_instruction_formatted)] + state[\"messages\"][:-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Initialize the spy for visibility into the tool calls made by Trustcall\n",
    "    spy: Spy = Spy()\n",
    "\n",
    "    # Create the Trustcall extractor for updating the ToDo list\n",
    "    todo_extractor: Runnable[InputsLike, ExtractionOutputs] = create_extractor(\n",
    "        model,\n",
    "        tools=[ToDo],\n",
    "        tool_choice=tool_name,\n",
    "        enable_inserts=True\n",
    "    ).with_listeners(on_end=spy)\n",
    "\n",
    "    # Invoke the extractor\n",
    "    result = todo_extractor.invoke({\n",
    "        \"messages\": updated_messages,\n",
    "        \"existing\": existing_memories\n",
    "    })\n",
    "\n",
    "    # Save the memories from Trustcall to the store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"], strict=False):\n",
    "        store.put(\n",
    "            namespace,\n",
    "            rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "            r.model_dump(mode=\"json\"),\n",
    "        )\n",
    "\n",
    "    # Respond to the tool call made in tasks_democracy_ai, confirming the update\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "\n",
    "    # Extract the changes made by Trustcall and add to the ToolMessage returned to tasks_democracy_ai\n",
    "    todo_update_msg: str = extract_tool_info(spy.called_tools, tool_name)\n",
    "    return {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": todo_update_msg,\n",
    "            \"tool_call_id\": tool_calls[0]['id']\n",
    "        }]\n",
    "    }\n",
    "\n",
    "def update_instructions(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> dict[str, list[dict[str, str]]]:\n",
    "    \"\"\"Reflect on the chat history and update the instructions in memory.\n",
    "\n",
    "    This function processes the chat history to extract and update user-specified\n",
    "    preferences for managing the todo list. It stores these instructions for future\n",
    "    reference.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Dict containing a tool message confirming the instructions update\n",
    "        Format: {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"updated instructions\",\n",
    "                \"tool_call_id\": str\n",
    "            }]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Get the user ID from the config\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    user_id = configurable.user_id\n",
    "\n",
    "    namespace: tuple[str, str] = (\"instructions\", user_id)\n",
    "\n",
    "    existing_memory = store.get(namespace, \"user_instructions\")\n",
    "\n",
    "    # Format the memory in the system prompt\n",
    "    system_msg: str = CREATE_INSTRUCTIONS.format(\n",
    "        current_instructions=existing_memory.value if existing_memory else None\n",
    "    )\n",
    "    new_memory: BaseMessage = model.invoke(\n",
    "        [SystemMessage(content=system_msg)] +\n",
    "        state['messages'][:-1] +\n",
    "        [HumanMessage(content=\"Please update the instructions based on the conversation\")]\n",
    "    )\n",
    "\n",
    "    # Overwrite the existing memory in the store\n",
    "    key: str = \"user_instructions\"\n",
    "    store.put(namespace, key, {\"memory\": new_memory.content})\n",
    "\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "\n",
    "    # Return tool message with update verification\n",
    "    return {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"updated instructions\",\n",
    "            \"tool_call_id\": tool_calls[0]['id']\n",
    "        }]\n",
    "    }\n",
    "\n",
    "# Conditional edge\n",
    "def route_message(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore\n",
    ") -> Literal[END, \"update_todos\", \"update_instructions\", \"update_profile\"]:\n",
    "    \"\"\"Route messages to appropriate memory update functions based on tool call type.\n",
    "\n",
    "    This function examines the latest message in the state and determines which memory\n",
    "    update function should handle it based on the tool call's update_type.\n",
    "\n",
    "    Args:\n",
    "        state: Current message state containing chat history\n",
    "        config: Configuration object containing user settings and preferences\n",
    "        store: Storage interface for accessing and managing memories\n",
    "\n",
    "    Returns:\n",
    "        Literal indicating which node should process the message next:\n",
    "        - END: No tool calls present\n",
    "        - \"update_todos\": Route to todo list update\n",
    "        - \"update_instructions\": Route to instructions update\n",
    "        - \"update_profile\": Route to profile update\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the tool call's update_type is not recognized\n",
    "    \"\"\"\n",
    "    message = state['messages'][-1]\n",
    "    if len(message.tool_calls) == 0:\n",
    "        return END\n",
    "    else:\n",
    "        tool_call = message.tool_calls[0]\n",
    "        if tool_call['args']['update_type'] == \"user\":\n",
    "            return \"update_profile\"\n",
    "        elif tool_call['args']['update_type'] == \"todo\":\n",
    "            return \"update_todos\"\n",
    "        elif tool_call['args']['update_type'] == \"instructions\":\n",
    "            return \"update_instructions\"\n",
    "        else:\n",
    "            raise ValueError(\"Unknown update_type in tool call\")\n",
    "\n",
    "# SOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/create-react-agent-memory.ipynb\n",
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "\n",
    "# Create the graph + all nodes\n",
    "builder = StateGraph(MessagesState, config_schema=configuration.Configuration)\n",
    "\n",
    "# Define the flow of the memory extraction process\n",
    "builder.add_node(tasks_democracy_ai)\n",
    "builder.add_node(update_todos)\n",
    "builder.add_node(update_profile)\n",
    "builder.add_node(update_instructions)\n",
    "\n",
    "# Define the flow\n",
    "builder.add_edge(START, \"tasks_democracy_ai\")\n",
    "builder.add_conditional_edges(\"tasks_democracy_ai\", route_message)\n",
    "builder.add_edge(\"update_todos\", \"tasks_democracy_ai\")\n",
    "builder.add_edge(\"update_profile\", \"tasks_democracy_ai\")\n",
    "builder.add_edge(\"update_instructions\", \"tasks_democracy_ai\")\n",
    "\n",
    "# Compile the graph\n",
    "graph: CompiledStateGraph = builder.compile()\n",
    "\n",
    "print(graph.get_graph().print_ascii())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected dict, got [HumanMessage(content='I have 30 minutes, what tasks can I get done?', additional_kwargs={}, response_metadata={})]\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Chat with the chatbot\u001b[39;00m\n\u001b[1;32m      6\u001b[0m input_messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI have 30 minutes, what tasks can I get done?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m----> 8\u001b[0m \u001b[43mprint_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Run the graph\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     chunk[\"messages\"][-1].pretty_print()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# inputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print_stream(graph.stream(inputs, stream_mode=\"values\"))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 638\u001b[0m, in \u001b[0;36mprint_stream\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_stream\u001b[39m(stream):\n\u001b[0;32m--> 638\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1573\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1568\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1569\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1571\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1573\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1580\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:104\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    102\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, writer)\u001b[0m\n\u001b[1;32m     38\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:176\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m    175\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m--> 176\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    178\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/pregel/write.py:85\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     writes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     80\u001b[0m         ChannelWriteEntry(write\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;28minput\u001b[39m, write\u001b[38;5;241m.\u001b[39mskip_none, write\u001b[38;5;241m.\u001b[39mmapper)\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m write\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[1;32m     84\u001b[0m     ]\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_at_least_one_of\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/pregel/write.py:130\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[0;34m(config, writes, require_at_least_one_of)\u001b[0m\n\u001b[1;32m    127\u001b[0m entries \u001b[38;5;241m=\u001b[39m [write \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m writes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry)]\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# process entries into values\u001b[39;00m\n\u001b[1;32m    129\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 130\u001b[0m     \u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m write\u001b[38;5;241m.\u001b[39mmapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m entries\n\u001b[1;32m    132\u001b[0m ]\n\u001b[1;32m    133\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    134\u001b[0m     (write\u001b[38;5;241m.\u001b[39mchannel, val)\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val, write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, entries)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m write\u001b[38;5;241m.\u001b[39mskip_none \u001b[38;5;129;01mor\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m ]\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# filter out SKIP_WRITE values\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/bossjones/democracy-exe/.venv/lib/python3.12/site-packages/langgraph/graph/state.py:649\u001b[0m, in \u001b[0;36mCompiledStateGraph.attach_node.<locals>._get_state_key\u001b[0;34m(input, key)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    645\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[1;32m    646\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    647\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mINVALID_GRAPH_NODE_RETURN_VALUE,\n\u001b[1;32m    648\u001b[0m     )\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(msg)\n",
      "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got [HumanMessage(content='I have 30 minutes, what tasks can I get done?', additional_kwargs={}, response_metadata={})]\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"Lance\"}}\n",
    "\n",
    "# Chat with the chatbot\n",
    "input_messages = [HumanMessage(content=\"I have 30 minutes, what tasks can I get done?\")]\n",
    "\n",
    "print_stream(graph.stream(input_messages, stream_mode=\"values\"))\n",
    "# Run the graph\n",
    "# for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "#     chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "\n",
    "# inputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\n",
    "# print_stream(graph.stream(inputs, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

</document_content>
</document>
<document index="107">
<source>/Users/malcolm/dev/bossjones/democracy-exe/democracy_exe/agentic/workflows/react/requirements.txt</source>
<document_content>
langgraph
langchain-core
langchain-community
langchain-openai
trustcall
langchain-fireworks
pinecone-client
tiktoken
loguru
typing-extensions
python-dotenv
uuid
aiohttp
discord.py
rich
pydantic_settings
yarl
pinecone-text
grandalf
nest_asyncio

</document_content>
</document>
</documents>
